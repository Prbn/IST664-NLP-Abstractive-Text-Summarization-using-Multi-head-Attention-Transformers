# IST664-NLP-Abstractive-Text-Summarization-using-Multi-head-Attention-Transformers

# Abstractive Text Summarization using Multi-head Attention Transformers
#[Syracuse University: IST 664 Natural Language Processing]

# Abstract:
This project, conducted as part of the IST 664 Natural Language Processing course at Syracuse University, aimed to push the boundaries of abstractive text summarization by leveraging the power of Multi-head Attention Transformers. The objective was to develop a state-of-the-art model capable of generating concise and coherent summaries of text or articles. Through extensive research, dataset preparation, model design, and optimization, a high-performing summarization model was achieved, surpassing the baseline ROUGE-0.35 score. This project contributes to the field of natural language processing and paves the way for improved information extraction and comprehension.

# Summary:
The project focused on advancing the field of abstractive text summarization through the utilization of Multi-head Attention Transformers. The goal was to develop a model that could generate accurate and informative summaries of given text or articles, improving information extraction and comprehension.

Extensive research was conducted to understand Attention models and their relevance in natural language processing, specifically in the context of text summarization. Various techniques and approaches were explored to enhance the abstractive summarization process, aiming to improve the model's ability to generate meaningful and concise summaries.

A diverse dataset of Daily News articles was collected, covering a wide range of topics and writing styles. This dataset served as the foundation for training and fine-tuning the multi-head attention model. Data preprocessing techniques such as tokenization, text normalization, and sentence splitting were applied to prepare the dataset for model training.

The model was designed and implemented using the Transformer architecture, incorporating self-attention mechanisms to capture semantic relationships between words and phrases. Through an iterative process of training and optimization, the model was fine-tuned to achieve a ROUGE-0.35 score, surpassing the baseline and demonstrating its effectiveness in generating accurate and relevant summaries.

Continuous evaluations and analyses were conducted to assess the model's performance and identify areas for improvement. The model architecture was refined, and various techniques such as beam search and length normalization were explored to enhance the quality of the generated summaries.

Through this project, a highly advanced abstractive text summarization model using Multi-head Attention Transformers was successfully developed. The project not only deepened the understanding of Attention models and their applications but also enhanced skills in natural language processing, deep learning, and model evaluation. This research contributes to the field by pushing the boundaries of abstractive text summarization and facilitating improved information extraction and comprehension in various domains.
