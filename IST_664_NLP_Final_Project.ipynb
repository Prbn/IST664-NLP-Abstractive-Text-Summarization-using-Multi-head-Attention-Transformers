{"cells":[{"cell_type":"markdown","source":["# Natural Language Processing\n","\n","## Final Project\n","\n","Team:\\\n","Prabin Raj Shrestha\\\n","Neha Reddy Gaddam\\\n","Shivani Agarwal\n",""],"metadata":{"id":"US-nWeIn_ena"}},{"cell_type":"markdown","source":["---\n","---"],"metadata":{"id":"uFpMOEUdAThM"}},{"cell_type":"markdown","metadata":{"id":"7yuytuIllsv1"},"source":["\n","# Transformer Summarizer\n","\n","Explore summarization using the transformer model.\n","We will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed.\n","\n","<img src = 'https://docs.google.com/uc?export=download&id=1H60sdmeFM0HHBREeBmMZ4ieVGcr4q5hi'>\n","<!-- <img src = \"files/transformerNews.png\"> -->\n"]},{"cell_type":"markdown","metadata":{"id":"KGsEcCtIgGTe"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"H4NlfEQhRWPy"},"source":["<a name='0'></a>\n","### Introduction\n","\n","Text Summarization is the process of creating a summary of a certain document that contains the most important information of the original one, the purpose of it is to get a summary of the main points of the document. Text summarization has become increasingly important due to the vast amount of data available today. Extractive and abstractive are the main approaches used to create summaries. Extractive summarization has matured, and research has shifted towards abstractive and real-time summarization\n","\n","Abstractive summarization creates a shorter summary of text by generating new sentences that convey the same meaning as the original text. It uses natural language generation techniques to produce high-quality summaries that retain essential information while maintaining readability. It is particularly useful for large amounts of data, but more challenging than extractive summarization. Abstractive summarization of multi-documents aims to distill the most important ideas while keeping the summary concise and coherent.\n","\n","We would be exploring the below\n","\n","- Use built-in functions to preprocess your data\n","- Implement DotProductAttention\n","- Implement Causal Attention\n","- Know how attention functions\n","- Build transformer model\n","- Evaluate your model\n","- Summarize an article\n","\n","Our model will be heavily based on attention and does not rely on sequences, which allows for parallel computing."]},{"cell_type":"markdown","metadata":{"id":"xHkz7mHQgDab"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"9Jq1YK3WEfEq"},"source":["# Reference Papers & Codes:\n","\n","Paperswithcode: https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily\n","\n","## Papers:\n","\n","\n","* Abstractive Text Summarization using Sequence-to-sequence\n","RNNs and Beyond\n","https://arxiv.org/pdf/1602.06023v5.pdf\n","\n","* Attention is all you need:\n","https://arxiv.org/abs/1706.03762\n","\n","* Transformer Based Implementation for Automatic Book Summarization:\n","https://ijisae.org/index.php/IJISAE/article/view/2421\n","\n","## Articles:\n","\n","* Abstractive Text Summarization Using Transformers: https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453\n","\n","* Get started with Google Trax for NLP: https://towardsdatascience.com/get-started-with-google-trax-for-nlp-ff8dcd3119cf\n","\n","## Online Courses:\n","\n","* Coursera Deep Learning Specialization by Andrew Ng https://www.coursera.org/specializations/deep-learning\n"]},{"cell_type":"markdown","metadata":{"id":"Hnrr0WUgEes9"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"rwfPLBePgNgw"},"source":["# Installing and Importing Packages"]},{"cell_type":"markdown","metadata":{"id":"wDCL7rqwhHCY"},"source":["Packages used:\n","\n","sys: https://docs.python.org/3/library/sys.html \\\n","os: https://docs.python.org/3/library/os.html \\\n","numpy: https://numpy.org/doc \\\n","textwrap: https://docs.python.org/3/library/textwrap.html \\\n","trax: https://trax-ml.readthedocs.io/en/latest/ \\\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"5L9GJtpPguDo","executionInfo":{"status":"ok","timestamp":1686686259734,"user_tz":240,"elapsed":383,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[],"source":["from google.colab import output as colab_output"]},{"cell_type":"markdown","metadata":{"id":"TT-YBZKbgWSd"},"source":["## Installing Packages"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IwpnEVMdgY6R","executionInfo":{"status":"ok","timestamp":1686686268668,"user_tz":240,"elapsed":8578,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[],"source":["# trax\n","!pip install trax\n","\n","colab_output.clear()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CChWzW-rEHVb","outputId":"04f59b2c-8e14-4669-e211-0a101aceea6f","executionInfo":{"status":"ok","timestamp":1686686314731,"user_tz":240,"elapsed":46064,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import sys #system\n","import os #os to run os commands\n","import shutil # shutil\n","import zipfile # to zip file\n","\n","import numpy as np #numpy\n","import pandas as pd #pd\n","\n","import textwrap\n","wrapper = textwrap.TextWrapper(width=70)\n","\n","import trax\n","from trax.fastmath import numpy as jnp\n","from trax import layers as tl\n","\n","import re\n","\n","# nltk\n","import nltk\n","\n","# Downloading nltk resources\n","nltk.download('punkt')\n","nltk.download('wordnet') # for lemma\n","nltk.download('omw-1.4') # for lemma\n","nltk.download('stopwords') # stopwords\n","\n","# Importing Regex Tokenizer from nltk\n","from nltk.tokenize import RegexpTokenizer\n","\n","# Importing Tree bank Word Tokenizer from nltk\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","# Downloading Model\n","from google.colab import files\n","\n","# requests\n","import requests\n","\n","# to print the entire np array\n","np.set_printoptions(threshold=sys.maxsize)\n","\n","# To diplay Youtube videos\n","from IPython.display import YouTubeVideo as YT_V\n"]},{"cell_type":"markdown","source":["## Custom functions"],"metadata":{"id":"Ups-646EKKpP"}},{"cell_type":"code","source":["# Custom functions\n","\n","# Function to create a directory and its parent directories if they do not exist.\n","def create_directory(path):\n","  \"\"\"\n","  Recursively create a directory and its parent directories if they do not exist.\n","  \"\"\"\n","  if not os.path.exists(path):\n","    # Recursively create the parent directory if it does not exist\n","    if not os.path.exists(os.path.dirname(path)):\n","        create_directory(os.path.dirname(path))\n","    # Create the directory\n","    os.mkdir(path)\n","\n","\n","def zip_folder(folder_path, output_path):\n","  \"\"\"Create a zip archive of the given folder.\"\"\"\n","  with zipfile.ZipFile(output_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n","    for root, dirs, files in os.walk(folder_path):\n","      for file in files:\n","        file_path = os.path.join(root, file)\n","        zipf.write(file_path, os.path.relpath(file_path, folder_path))\n","\n","\n","# Zip and Download\n","def zip_n_dl(folder_dir):\n","  zip_folder(folder_dir, f'{folder_dir}.zip')\n","  files.download(f'{folder_dir}.zip')\n","\n","# Unzip a file\n","def unzip_file(zip_file_path, extract_to_path = None):\n","  \"\"\"Extract the contents of the given zip file to the specified directory.\"\"\"\n","  extract_to_path = str(zip_file_path).split('.zip')[0] if extract_to_path is None else extract_to_path\n","  with zipfile.ZipFile(zip_file_path, \"r\") as zipf:\n","    zipf.extractall(extract_to_path)\n","\n","\n","def df_pkl_n_dn(df, pkl_file_dir):\n","  df.to_pickle(f'{pkl_file_dir}.pkl')\n","  files.download(f'{pkl_file_dir}.pkl')\n","\n"],"metadata":{"id":"DAXGp1WCxfmH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEL2rvaHRWP4"},"source":["<a name='1'></a>\n","# Part 1: Importing the dataset"]},{"cell_type":"markdown","metadata":{"id":"mQr9ksCNoD1c"},"source":["Trax makes it easy to work with Tensorflow's datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4SW6kImwOU3"},"outputs":[],"source":["# !rm -rf data\n","# !rm -rf sample_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK_noUxS4XBm"},"outputs":[],"source":["!mkdir 'data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJBjvRX0pdby","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2bd369f7-ec21-4e0f-cdc5-d37ee0a70687","executionInfo":{"status":"ok","timestamp":1683173878923,"user_tz":240,"elapsed":21248,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Load successful!\n"]}],"source":["# Downliading File\n","!wget 'https://docs.google.com/uc?export=download&id=1FSyO2pB2Aw7rqvogIYZQPBOy6gvQYzdf&confirm=t' -O 'cnn_dailymail.zip'\n","# Unzipping File\n","!unzip -o 'cnn_dailymail.zip' -d 'data'\n","# Removing after unzipping\n","!rm cnn_dailymail.zip\n","\n","colab_output.clear()\n","print('Data Load successful!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkBbwz2gYvkN","outputId":"ee7add4c-a16c-4d90-8e47-52a4b71f1b04","executionInfo":{"status":"ok","timestamp":1683173881056,"user_tz":240,"elapsed":2151,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Load successful!\n"]}],"source":["# Vocab\n","# Downliading File\n","!wget 'https://docs.google.com/uc?export=download&id=1FWJuKOMjiGHJhwjcpSoarnZ3l7jpBOKh&confirm=t' -O 'vocabs.zip'\n","# Unzipping File\n","!unzip -o 'vocabs.zip' -d 'data'\n","# Removing after unzipping\n","!rm vocabs.zip\n","\n","colab_output.clear()\n","print('Data Load successful!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"EEgPLfalQXMz","outputId":"b643dd2b-2b53-424b-ee6a-6844d571ae07","executionInfo":{"status":"ok","timestamp":1683173898735,"user_tz":240,"elapsed":17681,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                         id  \\\n","0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n","1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n","2  00027e965c8264c35cc1bc55556db388da82b07f   \n","3  0002c17436637c4fe1837c935c04de47adb18e9a   \n","4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n","\n","                                             article  \\\n","0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n","1  (CNN) -- Ralph Mata was an internal affairs li...   \n","2  A drunk driver who killed a young woman in a h...   \n","3  (CNN) -- With a breezy sweep of his pen Presid...   \n","4  Fleetwood are the only team still to have a 10...   \n","\n","                                          highlights  \n","0  Bishop John Folda, of North Dakota, is taking ...  \n","1  Criminal complaint: Cop used his role to help ...  \n","2  Craig Eccleston-Todd, 27, had drunk at least t...  \n","3  Nina dos Santos says Europe must be ready to a...  \n","4  Fleetwood top of League One after 2-0 win at S...  "],"text/html":["\n","  <div id=\"df-3883d20a-31f7-4df2-9bca-baafee0db48c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>article</th>\n","      <th>highlights</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n","      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n","      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n","      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n","      <td>Criminal complaint: Cop used his role to help ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n","      <td>A drunk driver who killed a young woman in a h...</td>\n","      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n","      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n","      <td>Nina dos Santos says Europe must be ready to a...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n","      <td>Fleetwood are the only team still to have a 10...</td>\n","      <td>Fleetwood top of League One after 2-0 win at S...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3883d20a-31f7-4df2-9bca-baafee0db48c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3883d20a-31f7-4df2-9bca-baafee0db48c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3883d20a-31f7-4df2-9bca-baafee0db48c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["# Loading data\n","\n","# Test and train\n","\n","train_data_df = pd.read_csv('data/cnn_dailymail/train.csv')\n","test_data_df = pd.read_csv('data/cnn_dailymail/test.csv')\n","\n","train_data_df.head()"]},{"cell_type":"markdown","metadata":{"id":"Iv1ZsyWwQOxR"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"FTsV_z2MsiZx"},"source":["<a name='1.1'></a>\n","## 1.1 Preprocessing\n","\n","Applying preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-gIHiVtsiCd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"afb10048-6223-4842-b499-0af188552c39","executionInfo":{"status":"ok","timestamp":1683173898736,"user_tz":240,"elapsed":19,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:5: DeprecationWarning: invalid escape sequence '\\w'\n"]}],"source":["# Custom Tree bank Word Tokenizer\n","# This is used to tokenize the document\n","class custom_tbw:\n","  def __init__(self):\n","     self.re_period_pattern = re.compile('\\w+\\.$')\n","     treebankword_tokenize = TreebankWordTokenizer()\n","     self.treebankword_tokenizer = treebankword_tokenize.tokenize\n","\n","  def tokenizer(self, input_text):\n","    pre_token = self.treebankword_tokenizer(input_text)\n","    out_token = []\n","\n","    for token in pre_token:\n","      if self.re_period_pattern.match(token): # Checking if the word ends with .\n","        out_token.append(token[:-1]) # Appending the word\n","        out_token.append(token[-1]) # Adding the period at the end\n","      else:\n","        out_token.append(token)\n","    # Returning output\n","    return out_token\n","\n","# Preprocessing steps\n","# This is used to clean the text\n","class custom_text_processing(custom_tbw):\n","  def __init__(self):\n","    super().__init__()\n","    self._def_init__()\n","\n","  def _def_init__(self):\n","    # Negationwords\n","    self.negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n","    # Stopwords\n","    self.sw_l = nltk.corpus.stopwords.words('english')\n","    self.sw_l = [sw for sw in self.sw_l if sw not in self.negationwords]\n","    # Unwanted punctuation\n","    self.punctuation_l = ['.', ',', '--', '\"', \"'\"]\n","    # making a stop word dictionary\n","    self.sw_dict = {sw.lower():False for sw in self.sw_l}\n","    # making a stop word dictionary\n","    self.punt_dict = {sw.lower():False for sw in self.punctuation_l}\n","    # Wordnet lemmantizer:\n","    self.wnl = nltk.WordNetLemmatizer()\n","    # stop_wl_pattern\n","    self.stop_pat = r'\\b(?:{})\\b'.format('|'.join(self.sw_l))\n","\n","  def ReturnCleanText(self, text):\n","    # Lower casing the text\n","    text = text.lower()\n","\n","    # Substituting anything between words using reguler expression and substituting it with ' '\n","    text = re.sub(r\"\\W+|_\", ' ', text)\n","\n","    # removing stop words\n","    return re.sub(self.stop_pat, '', text)\n","\n","\n","  # Function to tokenize\n","  def text_tokenization(self, in_text):\n","    return self.tokenizer(in_text)\n","\n","  # Function to lower case the tokens:\n","  def lower_case_tokens(self, in_token):\n","    return [t.lower() for t in in_token]\n","\n","  # Lemmantize tokens\n","  def wnl_lemmatize(self, in_token):\n","    return [self.wnl.lemmatize(token) for token in in_token]\n","\n","  # function to check stop words\n","  def is_not_sw(self, in_token):\n","    return self.sw_dict.get(in_token.lower(),True)\n","\n","  # function to check punctuation\n","  def is_not_punct(self, in_token):\n","    return self.punt_dict.get(in_token.lower(),True)\n","\n","  # making a function that removes stop word\n","  def remove_stopwords(self, in_token):\n","    return [token for token in in_token if self.is_not_sw(token.lower())]\n","\n","  def remove_punctuation(self, in_token):\n","    return [token for token in in_token if self.is_not_punct(token.lower())]\n","\n","  def clean_text_processing(self, in_text):\n","    text = self.ReturnCleanText(in_text)\n","    # Tokenize the text\n","    tokens = self.text_tokenization(text)\n","    # Lower case\n","    tokens = self.lower_case_tokens(tokens)\n","    # lemmantize\n","    tokens = self.wnl_lemmatize(tokens)\n","    # # Remove stop words\n","    # tokens = self.remove_stopwords(tokens)\n","    # # Remove punctuation\n","    # tokens = self.remove_punctuation(tokens)\n","\n","    # Output\n","    out_text = ' '.join(tokens)\n","    return out_text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLFpeRFktZ7U"},"outputs":[],"source":["# Pulling the resource to process the text\n","\n","# Text Processor\n","text_processer = custom_text_processing()\n","# clean text processor\n","clean_text_processor = text_processer.clean_text_processing\n","\n","def bytfy(x:str):\n","  return bytes(x, 'utf-8')\n","\n","# process text:\n","def process_text(x:str):\n","  t = clean_text_processor(x)\n","  return bytfy(t)"]},{"cell_type":"code","source":["# Downloading porcessed data\n","!wget 'https://docs.google.com/uc?export=download&id=1HI920sXIQ6aVlhiE14RDqtbpU_ZB5usI&confirm=t' -O 'processed_train_data_df.pkl'\n","!wget 'https://docs.google.com/uc?export=download&id=1HE9lojnHfpTiTJrT9xj4VtpSLG3tD6Rm&confirm=t' -O 'processed_test_data_df.pkl'\n","\n","\n","colab_output.clear()"],"metadata":{"id":"NFeDob3jM5iC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":262},"id":"pvsX58fWTQKx","outputId":"aa9b216a-bc69-41d3-d969-10375a47f669","executionInfo":{"status":"ok","timestamp":1683177932997,"user_tz":240,"elapsed":2229,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                         id  \\\n","0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n","1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n","2  00027e965c8264c35cc1bc55556db388da82b07f   \n","3  0002c17436637c4fe1837c935c04de47adb18e9a   \n","4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n","\n","                                                text  \\\n","0  b'associated press published 14 11 est 25 octo...   \n","1  b'cnn ralph mata internal affair lieutenant mi...   \n","2  b'drunk driver killed young woman head crash c...   \n","3  b'cnn breezy sweep pen president vladimir puti...   \n","4  b'fleetwood team still 100 record sky bet leag...   \n","\n","                                             summary  \n","0  b'Bishop John Folda, of North Dakota, is takin...  \n","1  b'Criminal complaint: Cop used his role to hel...  \n","2  b\"Craig Eccleston-Todd, 27, had drunk at least...  \n","3  b\"Nina dos Santos says Europe must be ready to...  \n","4  b'Fleetwood top of League One after 2-0 win at...  "],"text/html":["\n","  <div id=\"df-f66a7850-3672-4d18-8308-2a1d086b0531\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n","      <td>b'associated press published 14 11 est 25 octo...</td>\n","      <td>b'Bishop John Folda, of North Dakota, is takin...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n","      <td>b'cnn ralph mata internal affair lieutenant mi...</td>\n","      <td>b'Criminal complaint: Cop used his role to hel...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n","      <td>b'drunk driver killed young woman head crash c...</td>\n","      <td>b\"Craig Eccleston-Todd, 27, had drunk at least...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n","      <td>b'cnn breezy sweep pen president vladimir puti...</td>\n","      <td>b\"Nina dos Santos says Europe must be ready to...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n","      <td>b'fleetwood team still 100 record sky bet leag...</td>\n","      <td>b'Fleetwood top of League One after 2-0 win at...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f66a7850-3672-4d18-8308-2a1d086b0531')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f66a7850-3672-4d18-8308-2a1d086b0531 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f66a7850-3672-4d18-8308-2a1d086b0531');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":30}],"source":["# Preporcessing 1\n","\n","process_data_f = False\n","\n","if process_data_f:\n","  '''If we want to process the data'''\n","\n","  # Rename columns\n","  rename_col_dict = {'article': 'text', 'highlights': 'summary'}\n","\n","  processed_train_data_df = train_data_df.rename(columns=rename_col_dict)\n","  processed_test_data_df = test_data_df.rename(columns=rename_col_dict)\n","\n","  # applying bytes on training data\n","  processed_train_data_df['text'] = processed_train_data_df['text'].apply(lambda x: process_text(str(x)))\n","  processed_train_data_df['summary'] = processed_train_data_df['summary'].apply(lambda x: bytfy(str(x)))\n","\n","  # applying bytes on test data\n","  processed_test_data_df['text'] = processed_test_data_df['text'].apply(lambda x: process_text(str(x)))\n","  processed_test_data_df['summary'] = processed_test_data_df['summary'].apply(lambda x: bytfy(str(x)))\n","\n","else:\n","  processed_train_data_df = pd.read_pickle('processed_train_data_df.pkl')\n","  processed_test_data_df = pd.read_pickle('processed_test_data_df.pkl')\n","\n","# Preview\n","processed_train_data_df.head()"]},{"cell_type":"code","source":["if process_data_f:\n","  df_pkl_n_dn(processed_train_data_df, 'processed_train_data_df')\n","  df_pkl_n_dn(processed_test_data_df, 'processed_test_data_df')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"6g7Juo8EsGzP","outputId":"1841656c-9435-4d3e-928e-748d305e7862","executionInfo":{"status":"ok","timestamp":1683175372573,"user_tz":240,"elapsed":4006,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_7cd53a1a-aa04-4e6a-8deb-a0bd0c693d6e\", \"processed_train_data_df.pkl\", 886925528)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0a738301-c208-453d-a014-14d1674d19a0\", \"processed_test_data_df.pkl\", 35045956)"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"sGwEwY1qsvGp"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"aEmblgeNQPnk"},"source":["Making Text stream"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiGzs-JJRsbK"},"outputs":[],"source":["def data_stream(data_df, n:int = 0, col_n:dict={'text': 'text', 'summary':'summary'}):\n","  c = 0\n","  for index, row in data_df.iterrows():\n","    c += 1\n","    if n != 0 and c>n:\n","        break\n","    yield row[col_n['text']], row[col_n['summary']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YN7_6WfVFKb"},"outputs":[],"source":["# making a text stream for training\n","train_stream_fn = data_stream(processed_train_data_df)\n","\n","test_stream_fn = data_stream(processed_test_data_df)"]},{"cell_type":"code","source":["# Test\n","\n","test_text, test_summary = next(test_stream_fn)\n","\n","print(test_text)\n","print('')\n","print(test_summary)"],"metadata":{"id":"YSRdMZbWuR8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xf98b0n0UL0F"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"5mlMyZJwoD1c"},"source":["<a name='1.1'></a>\n","## 1.2 Tokenize & Detokenize helper functions\n","\n","Just like in the previous assignment, the cell above loads in the encoder for you. To work with any dataset, it's important to have the ability to link words to their numerical indices, and vice versa. Trax models typically use numeric tensors as input and output, where each numeric value corresponds to a particular word.\n","\n","In order to translate text into numerical values that can be processed by a computer, we can utilize the following tools:\n","\n","* word2Ind: a dictionary that links each word to its corresponding index number.\n","* ind2Word: a dictionary that links each index number to its corresponding word.\n","* word2Count: a dictionary that links each word to the number of times it appears in the text.\n","* num_words: the total number of words that appear in the text.\n","\n","These step have been implemented several times in previous assignments and labs, we will use helper functions that will do this for us. Below are the functions:\n","\n","* Tokenize: Tokenize refers to the process of transforming a textual sentence into a token list, which is essentially a list of indices. This conversion also involves breaking down words into smaller subwords.\n","\n","* Detokenize: On the other hand, detokenize involves the reverse process of converting a token list back into its corresponding sentence, essentially creating a string of text from the list of indices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djTiSLcaNFGa"},"outputs":[],"source":["def tokenizer(input_str, EOS=1):\n","    # Tokenizing Input strings in text to features dict,\n","    # To tokenize data in Trax, we can use the trax.data.tokenize method. This method works with streams as input and output.\n","    # However, to bypass this limitation, we can create a unigram stream using the iter function.\n","    # The summarize32k.subword.subwords is a large list of words\n","    inputs =  next(trax.data.tokenize(iter([input_str]),vocab_dir='data/vocabs/',vocab_file='summarize32k.subword.subwords'))\n","\n","    # Add the EOS symbol to signify the conclusion of the sentence.\n","    return list(inputs) + [EOS]\n","\n","def detokenizer(integers):\n","    # List of ints (tokenized sentence) to string\n","    s = trax.data.detokenize(integers,vocab_dir='data/vocabs/',vocab_file='summarize32k.subword.subwords')\n","    return wrapper.fill(s)\n","\n","\n","test_s = 'We will be using Transformer, encoder, and Decoder to make a text summarization'\n","\n","\n","tokens=tokenizer(test_s)\n","print(tokens)\n","print(test_s,'\\n')\n","print(detokenizer(tokens))"]},{"cell_type":"markdown","metadata":{"id":"7WvhaFbCRWQS"},"source":["<a name='1.2'></a>\n","\n","## 1.3 Preprocessing for Language Models: Concatenate It!\n","\n","As we know language models predict the next word, they have no notion of inputs. To create a single input suitable for a language model, we concatenate inputs with targets putting a separator in between.\n","\n","We also need to create a mask -- with 0s at inputs and 1s at targets -- to avoid panalizing the model for mis predicting. This way the model is not penalized for mis-predicting the article and only focuses on the summary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4rgPxYSRWQS"},"outputs":[],"source":["# Special tokens\n","sep = 0 # Separator token or padding\n","eos = 1 # End of sentence token\n","\n","# Combining tokenized inputs with targets by using separator as 0 .\n","def preprocess(stream):\n","  for (text, summary) in stream:\n","    joint = np.array(list(text) + [eos, sep] + list(summary) + [sep])\n","    mask = [0] * (len(list(text)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for sep and sep\n","    yield joint, joint, np.array(mask)\n","\n","# combining data preprocessing steps into a pipeline\n","input_pipeline = trax.data.Serial(\n","  # Pipenline Step 1: Tokenizes\n","  trax.data.Tokenize(vocab_dir='data/vocabs/', vocab_file='summarize32k.subword.subwords')\n","  # Pipenline Step 2: Using function defined above\n","  , preprocess\n","  # Pipenline Step 3: Filters out examples longer than 2048\n","  , trax.data.FilterByLength(2048)\n",")\n","\n","# Applying preprocessing to data streams.\n","processed_train_stream = input_pipeline(train_stream_fn)\n","processed_test_stream = input_pipeline(test_stream_fn)\n","\n","train_input, train_target, train_mask = next(processed_train_stream)\n","\n","# Checking\n","assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKFoGsUKSa_I","outputId":"7006d272-3b27-479c-dd2c-53bb1627e1c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Single example mask:\n","\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] \n","\n"," ------------------- \n","\n","\n","Single example train input:\n","\n"," [  567   379 13550   574   379  7226  5182  3047  6611   136  4601     3\n","   373   180   253 16958     4     2   393   560   429 11969 28081   379\n","  9720 22449  3590  4601     3   271   180  1513 16958     4     2   393\n","   560   429   379     9  8762   527   213  4035   273  1448 21905   132\n","   438  8597    23  4537  4840  3808   527   687   339   132  4035   273\n","     2  1711  4629  1283   186  6331 13708  3313   320   213 17758  6067\n","    27  6879   132   532   493   186   263   560     3     9   205  1404\n","   676    23  1956   163 14443   527  4713  1019  1815  1779  2561   409\n","  2758   186   436 18808     3  3963   240  7773  1000     8 12370    21\n","    12   527   213  4035   273  1448 21905   132   438  8597    23  4537\n","  4840  3808   527   687   339   132  4035   273     2  1711  4629  1283\n","   186  6331 13708  3313   320   213 17758  6067    27   379   303 13347\n","  3951  1700  2122  3860 11984    20 12685   371   465   213   993   229\n","   529     2    35  1631  1006   103     7     5   294   320 18340   101\n","   320   213   498  4713     3     9 20596  1595    78  2613   285  3963\n","   240  7773  1000   229   892    55   236   102   144 17935  1248 17758\n","  6067    27     3     9 20596   465    22 23062   213  5890   123 25724\n","    17   837   192  7449    28  1900  1019  3713 20483 14089   132  2060\n","   220   984     3  7099  6334  1387   527 17758  6067    27   377 19215\n","     2 11721 16532     2  1201   527  7219 20266     2  8225  1304   186\n"," 16891 12016  7564 26835    26     3  4035   273  1448 21905   132   438\n","  8597     8 12370    21    12   229  4872   213  8762   229   698  2104\n","     1     0  3963   240  7773  1000     2   527   438  8597     2   229\n","   892    55   236   102   144 17935 16346 27439  6774  1628    69 23062\n","   213  5890   123 25724    17   837   132  2060 16346 27439  6774  1628\n","   526   339   132  4035   273     2  1711  4629  1283   186  6331 13708\n","  3313   143    18    46  4537  2104     0] \n","\n"," ------------------- \n","\n","\n","Single example:\n","\n"," By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | .\n","UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo\n","Catholic Diocese in North Dakota has exposed potentially hundreds of\n","church members in Fargo, Grand Forks and Jamestown to the hepatitis A\n","virus in late September and early October. The state Health Department\n","has issued an advisory of exposure for anyone who attended five\n","churches and took communion. Bishop John Folda (pictured) of the Fargo\n","Catholic Diocese in North Dakota has exposed potentially hundreds of\n","church members in Fargo, Grand Forks and Jamestown to the hepatitis A\n",". State Immunization Program Manager Molly Howell says the risk is\n","low, but officials feel it's important to alert people to the possible\n","exposure. The diocese announced on Monday that Bishop John Folda is\n","taking time off after being diagnosed with hepatitis A. The diocese\n","says he contracted the infection through contaminated food while\n","attending a conference for newly ordained bishops in Italy last month.\n","Symptoms of hepatitis A include fever, tiredness, loss of appetite,\n","nausea and abdominal discomfort. Fargo Catholic Diocese in North\n","Dakota (pictured) is where the bishop is located .<EOS><pad>BishopJohn\n","Folda, of North Dakota, is taking time off after being diagnosed . He\n","contracted the infection through contaminated food in Italy . Church\n","members in Fargo, Grand Forks and Jamestown could have been exposed\n",".<pad> \n","\n"," ------------------- \n","\n","\n","Single example train target:\n","\n"," [  567   379 13550   574   379  7226  5182  3047  6611   136  4601     3\n","   373   180   253 16958     4     2   393   560   429 11969 28081   379\n","  9720 22449  3590  4601     3   271   180  1513 16958     4     2   393\n","   560   429   379     9  8762   527   213  4035   273  1448 21905   132\n","   438  8597    23  4537  4840  3808   527   687   339   132  4035   273\n","     2  1711  4629  1283   186  6331 13708  3313   320   213 17758  6067\n","    27  6879   132   532   493   186   263   560     3     9   205  1404\n","   676    23  1956   163 14443   527  4713  1019  1815  1779  2561   409\n","  2758   186   436 18808     3  3963   240  7773  1000     8 12370    21\n","    12   527   213  4035   273  1448 21905   132   438  8597    23  4537\n","  4840  3808   527   687   339   132  4035   273     2  1711  4629  1283\n","   186  6331 13708  3313   320   213 17758  6067    27   379   303 13347\n","  3951  1700  2122  3860 11984    20 12685   371   465   213   993   229\n","   529     2    35  1631  1006   103     7     5   294   320 18340   101\n","   320   213   498  4713     3     9 20596  1595    78  2613   285  3963\n","   240  7773  1000   229   892    55   236   102   144 17935  1248 17758\n","  6067    27     3     9 20596   465    22 23062   213  5890   123 25724\n","    17   837   192  7449    28  1900  1019  3713 20483 14089   132  2060\n","   220   984     3  7099  6334  1387   527 17758  6067    27   377 19215\n","     2 11721 16532     2  1201   527  7219 20266     2  8225  1304   186\n"," 16891 12016  7564 26835    26     3  4035   273  1448 21905   132   438\n","  8597     8 12370    21    12   229  4872   213  8762   229   698  2104\n","     1     0  3963   240  7773  1000     2   527   438  8597     2   229\n","   892    55   236   102   144 17935 16346 27439  6774  1628    69 23062\n","   213  5890   123 25724    17   837   132  2060 16346 27439  6774  1628\n","   526   339   132  4035   273     2  1711  4629  1283   186  6331 13708\n","  3313   143    18    46  4537  2104     0] \n","\n"," ------------------- \n","\n","\n"]}],"source":["# Testing\n","\n","# printing Single example train mask\n","print(f'Single example mask:\\n\\n {train_mask} \\n\\n ------------------- \\n\\n')\n","\n","# printing Single example train input\n","print(f'Single example train input:\\n\\n {train_input} \\n\\n ------------------- \\n\\n')\n","\n","# printing the detokenized input\n","print(f'Single example:\\n\\n {detokenizer(train_input)} \\n\\n ------------------- \\n\\n')\n","\n","# printing tokenized input\n","print(f'Single example train target:\\n\\n {train_target} \\n\\n ------------------- \\n\\n')"]},{"cell_type":"markdown","metadata":{"id":"F-wb38tvXbu4"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"T4sDS1WIVaYG"},"source":["<a name='1.3'></a>\n","\n","## 1.4 Batching with bucketing\n","\n","using bucketing to create batches of data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oqj1NsbERWQX"},"outputs":[],"source":["# Bucketing to create batched generators.\n","\n","# In trax Buckets are defined in terms of boundaries and batch sizes.\n","# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n","# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n","# 4 of length < 512. And so on.\n","boundaries =  [128, 256, 512, 1024, 2048]\n","batch_sizes = [16, 8, 4, 2, 1]\n","\n","# Creating the data streams.\n","# Training\n","train_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(processed_train_stream)\n","# Test\n","test_batch_stream = trax.data.BucketByLength(boundaries, batch_sizes)(processed_test_stream)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6M5OA8QRWQb","outputId":"a5f5a48b-7e86-4f2e-dbbd-2487dc6aa00c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 2048)"]},"metadata":{},"execution_count":20}],"source":["# Every execution will result in generation of a different article\n","# Try running this cell multiple times to see how the length of the examples affects the batch size\n","input_batch, _, mask_batch = next(train_batch_stream)\n","\n","# Shape of the input_batch\n","input_batch.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SjNOlljxTGuQ","outputId":"ab2050d3-e2d9-441d-ed7c-07c38f7c6d2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[   27 23176  4694  1779  1343    28   506  1091   132    28   570     6\n","    78  7124   192 14454    15  3570  2067    23    46 26133    17  1019\n","   635    91     3  5349 23421   494     6 10487     2   728     2  1353\n","  3156   278  1838    28   736   809    28 13481  7511    22   625    28\n","  1311  2396     3   187    22  1353  1510   181 16146  1049   320   103\n","     2    22 26563   651   467   213   826   192  3156  1262    28 13131\n","     4   186 16949    17    71 12319  6604   828 29725     4     5  1081\n","  1083   213    54   138     3  5349 23421   494     6 10487     2   728\n","     8   346    12  1353   354    15  3570  2067  7511    22 24497   570\n","     6    78    71   213  1081   144  3360   691 12319  6604   828     2\n","   705     8   231    24   305   710   272  1838    68  6341   379     9\n","   570     6    78  7124   436   219   132   560   429     3   368 23421\n","   494     6 10487     7     5  1081  1353 10874 20919   217     8 12370\n","    21    12  2713   127 23421   494     6 10487    40 23176   809   518\n","   150   181   290  3892   275   527  8947   171  1269   936   213  9025\n","     3    69  1353   233  8272   527  6056   583   691  4398  3156   809\n"," 14507  5429   812  7356     3  3622  6604   828     2    28   705     6\n","   104     6   292 15004   181 29725     4     5 21961  1838 10687    45\n","     2 11985   527 11907  5364     2    40    43  1383   213  2801  1248\n","  1078   809    28 13481    35    40    19 23176   116  4016     2   864\n","   127     3   305  1353  3156 17775 12979  3095   186    77  1353   669\n"," 27439  6050 13459  1628  1290   131   143    18   757   320  2501   213\n"," 25725 29725     2    41   969     3 16978  1822  9855  1962     2 17347\n","    16     2   127  4601 27439  6050 13459  1628  5349 23421   494     6\n"," 10487 29725     4     5  3156  2868   132   213 15191   583   527    28\n","   506  1091     2 12319  6604   828     2    28   583   285   143    18\n","    46 13488 23707  6050 13459  1628   368 23421   494     6 10487   436\n","   213   884   320  3429    61    15  3570  2067  6715  3156   186     2\n","   673  1510   181 16146  1049   320   824  1311  2396     2  1353    90\n"," 15438    17   285    22  2214   320 17950    28   346     6   650 13131\n","     4     2  7228   213  1052   763   314    71   213  2358   527  3622\n","  6604   828 29725     4     5 18352  2398  1081     3  3622  6604   828\n","  1353  7214   213 19839   277   527    68 27439  9275  1628 12320  5403\n","  9242  5590  2385    35   710   272  1838    68  6341   132  2642 11969\n"," 27439  6050 13459  1628  3622  6604   828   669 27884     4    40 27872\n","   391    28  5302   531  2504   527    68     3   305  1353    43  4925\n","   278   523  1383   163 20812  2801  1248  1078   186  1353  3156 17775\n"," 12979  3095 23707  6050 13459  1628   305    40  5945   320  1242    68\n","  1078  7511   131   540   278   320  8916   285   131    40  2362 15627\n","     3  1561  1078  8075   114   369  1613  1838    68   102    41  7584\n","    17   458 23707  6050 13459  1628  3622  6604   828 29725     4     5\n","   583   132    97  2861  6107 17946     5   213  6349   527   354    28\n","   650     6   475  3570  2067  6715  3156  4172 29725   391  2713    25\n","  3630   320   245 17388   181  1884  4140  1838 23421   494     6 10487\n","  1820     2    35   132  4140   329   926   102   213  5556    22  1353\n","    86 25070   918   155   213  6700     6  2057  3602     3     9  4038\n","  2256  1248   864   285    22    62    18    46    95   213  3602   809\n","   213    55    15   651  6866  4604   279  1205  3622  6604   828 29725\n","     4     5  2498 12320  5403  9242  5590  2385    78    28   826   542\n"," 15902  3569     2 11985   527 11907  5364     2    78   560   253     2\n","   429     3   405  2067   992  1606    22  1353    43 17997   595   239\n","   213    55   527   213  7124     3  6753  1565  8120   479     2  1838\n"," 12887 26509 21380   328 29725     4     5  1839 25725  2694  1676     2\n","   127  3611   871  5784  1435  1248 12319     7     5   228   809   824\n","    55     3   305    40    46    64  1248  1078   809    28 13481   132\n"," 15010  7301   285  2801     2    35    40    19    40   116  4016  1782\n","   871  2694  1606   285    77  1353  1290   131   143    18   757   320\n","  2501   213 25725   186  8075   114   103   919    68    68   177  1782\n","   368 23421   494     6 10487    40   346   126   132 15902  3569   186\n","  1326  1248  1078   809    28 13481  4872    22  6005  6929   809   518\n","   150   320   290  3892   275   527  7468    81     3    69 12402     7\n","    26   209   346   213 13481   320   955   278  7511   213 25725  1841\n","   809   239   128    10  3229  2535  1782   129  8198     7    26   217\n","   320   245 17388   181  1884  4140  1838   134  1820   186   849  1884\n","   576   329   926   102   213 25725  1606    22  1353 25070   918   155\n","   213  3602     2    51  2253    22    62    18    46    95   213  3602\n","   809   213    55   527   213 25725   186   132 13040  2398    61   592\n","     2   213  4038  2256  1782     9   641   527    15  2067   992  1606\n","   285    22  1353 17997   595    78    15  2067   239   213    55   527\n","   213 25725    90   103     7     5  1232   761   824    62    43    18\n","  3625   320    15  4398  3156   186  1201   527   490  2002 23421   494\n","     6 10487  1353   233  8272   527  6056   583   691  4398  3156   355\n","    28  2145   809 14507  5429   812     8 12370    21    12    69   969\n","  3611   368 23421   494     6 10487    39   169  3263   635    91   936\n","  5892     2    35 12319     7     5   228    18   913    68  8232  1782\n","    13  1525   824    39   191   101   362  3060   171  6642   116  4016\n","   186  1269   936   213  9025     2   181   354    28  2067   640    41\n","     7   165    78   213   826  1782     9 26024   527  6700  3156   186\n","  3156  6715   354    28  3570  2067  1435  3787     3  2994  1779   952\n","   320   124    90   993  3736    28  3537    55   132  2173     3    56\n","   347  6335   141  7270 15191   213  4472   527 16972   595    97 23891\n","  6412    49  1151 20327 27439  6050 13459  1628   368 23421   494     6\n"," 10487    39   169  3263   635    91   936  5892     2    35 12319 29725\n","     4     5   228    18   913    68  1019   545     3    13  1525   824\n","    39   191   101   362  3060   171  6642   116  4016   186  1269   936\n","   213  9025     2   181   354    28  2067   640    41 29725     4   165\n","    78   213   826     3    56   347  6335   141  7270 15191   213  4472\n","   527 16972   595    97 23891  6412    49  1151  4172 29725   391 23421\n","   494     6 10487     2   527 14735     2 11985   527 11907  5364     2\n","  1353    43 24306  5831  4461  1838  3156  1019  1223    91 27439  9275\n","  1628   102  1480    22    39    18   320   976   163  2008   165     6\n","  1166    10     1     0  5349 23421   494     6 10487     2   728     2\n","    40 23176   809   518   150  3892   275   171  3156  1081 16346 27439\n","  6774  1628  5670   354  2067  7511    22 26563   651   467   826   132\n"," 15902  3569     2 11985   527 11907  5364 16346 27439  6774  1628  3481\n","  3094   570     6    78    71   705     6   104     6   292 12319  6604\n","   828     7     5  1081     2  1779   710   132  2642 16346 27439  6774\n","  1628  2713   476    22    62    18    46    95   904  6700     6  2057\n","  3602   809    55   527  7124 16346 27439  6774  1628    69  1353   233\n","  8272   809 14507  5429   812   527  6056   583   691  4398  3156  2104\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n","\n","------------------------------------------------------------------------\n","\n","A drunk driver who killed a young woman in a head-on crash while\n","checking his mobile phone has been jailed for six years. Craig\n","Eccleston-Todd, 27, was driving home from a night at a pub when he\n","received a text message. As he was reading or replying to it, he\n","veered across the road while driving round a bend and smashed into\n","Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27\n","(left) was using his mobile phone when he crashed head-on into the car\n","being driven by Rachel Titley, 28 (right). She died later from her\n","injuries . The head-on crash took place in October 2013. Mr Eccleston-\n","Todd's car was barely recognisable (pictured) Police said Eccleston-\n","Todd had drunk at least three or four pints of beer before getting\n","behind the wheel. He was found guilty of causing death by dangerous\n","driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-\n","old solicitor’s clerk from Cowes, Isle of Wight, had also spent the\n","evening with friends at a pub but had not drunk any alcohol, police\n","said. She was driving responsibly and there was ‘nothing she could\n","have done to avoid the collision’, they added. Lindsay Pennell,\n","prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the\n","tragic death of a young woman, Rachel Titley, a death that could have\n","been avoided. ‘Mr Eccleston-Todd took the decision to pick up his\n","mobile phone whilst driving and, either reading or replying to this\n","text message, was so distracted that he failed to negotiate a left-\n","hand bend, crossing the central white line into the path of Miss\n","Titley’s oncoming car. Miss Titley was pulled the wreckage of\n","her Daihatsu Cuore but died later from her injuries in hospital .\n","‘Miss Titley [had] a bright future ahead of her. She was also\n","returning home having spent an enjoyable evening with friends and was\n","driving responsibly. ‘She had arranged to contact her friends when she\n","got home to confirm that she had arrived safely. Her friends sadly\n","never heard from her after they parted company. ‘Miss Titley’s death\n","in these circumstances reiterates the danger of using a hand-held\n","mobile phone whilst driving.’ Police were unable to take breath or\n","blood tests from Eccleston-Todd immediately, but in tests several\n","hours after the accident he was only marginally under the drink-drive\n","limit. The judge agreed with police that he would have been over the\n","limit at the time his red Citroen hit Miss Titley’s blue Daihatsu\n","Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His\n","phone records showed he was also texting around the time of the crash.\n","PC Mark Furse, from Hampshire constabulary’s serious collision\n","investigation unit, said: 'Our thoughts are with Rachel's family at\n","this time. She had been out with friends at a pub in Shalfleet that\n","evening, but had not had any alcohol. 'Our investigation showed that\n","there was nothing she could have done to avoid the collision and sadly\n","it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and\n","met with friends at a pub where he drank at least three to four pints\n","of lager. He hadn't long left the pub to return home when the\n","collision occurred at around 9.30pm. 'We weren't able to take breath\n","or blood tests from him immediately and although blood taken several\n","hours after the collision showed he was marginally under the limit, we\n","maintain he would have been over the limit at the time of the\n","collision and in summing up today, the judge agreed. 'The analysis of\n","his phone records showed that he was texting on his phone around the\n","time of the collision so it's highly likely this would also have\n","contributed to his dangerous driving and loss of control.' Eccleston-\n","Todd was found guilty of causing death by dangerous driving following\n","a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-\n","Todd will now spend six years behind bars, but Rachel's family have\n","lost her forever. 'I hope this will make people think twice before\n","drinking any alcohol and getting behind the wheel, or using a phone\n","once they're on the road. 'The dangers of drink driving and driving\n","whilst using a mobile phone are obvious. Those who continue to do so\n","risk spending a substantial time in prison. This case highlights just\n","how tragic the consequences of committing these offences can be.' ‘Mr\n","Eccleston-Todd will now spend six years behind bars, but Rachel’s\n","family have lost her for ever. I hope this will make people think\n","twice before drinking any alcohol and getting behind the wheel, or\n","using a phone once they’re on the road. This case highlights just how\n","tragic the consequences of committing these offences can be.’\n","Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from\n","driving for eight years after which he will have to complete an\n","extended re-test.<EOS><pad>CraigEccleston-Todd, 27, had drunk at least\n","three pints before driving car . Was using phone when he veered across\n","road in Yarmouth, Isle of Wight . Crashed head-on into 28-year-old\n","Rachel Titley's car, who died in hospital . Police say he would have\n","been over legal drink-drive limit at time of crash . He was found\n","guilty at Portsmouth Crown Court of causing death by dangerous driving\n",".<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad>\n"]}],"source":["# print corresponding integer values\n","print(input_batch[0])\n","print('\\n------------------------------------------------------------------------\\n')\n","print(detokenizer(input_batch[0]))"]},{"cell_type":"markdown","metadata":{"id":"GD-72TENV2Jk"},"source":["NOTE:\n"," - First is the corresponding values of the words.\n"," - The first 1, represents the `<EOS>` tag of the article.\n"," - Followed by a 0, which represents a `<pad>` tag.\n"," - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n"," - The second 1 represents the `<EOS>` tag for the summary.\n"," - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bu05ZwbWTE6P","outputId":"54c931c1-b55d-449a-f2f5-9312f50bc0d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Article:\n","\n"," A drunk driver who killed a young woman in a head-on crash while\n","checking his mobile phone has been jailed for six years. Craig\n","Eccleston-Todd, 27, was driving home from a night at a pub when he\n","received a text message. As he was reading or replying to it, he\n","veered across the road while driving round a bend and smashed into\n","Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27\n","(left) was using his mobile phone when he crashed head-on into the car\n","being driven by Rachel Titley, 28 (right). She died later from her\n","injuries . The head-on crash took place in October 2013. Mr Eccleston-\n","Todd's car was barely recognisable (pictured) Police said Eccleston-\n","Todd had drunk at least three or four pints of beer before getting\n","behind the wheel. He was found guilty of causing death by dangerous\n","driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-\n","old solicitor’s clerk from Cowes, Isle of Wight, had also spent the\n","evening with friends at a pub but had not drunk any alcohol, police\n","said. She was driving responsibly and there was ‘nothing she could\n","have done to avoid the collision’, they added. Lindsay Pennell,\n","prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the\n","tragic death of a young woman, Rachel Titley, a death that could have\n","been avoided. ‘Mr Eccleston-Todd took the decision to pick up his\n","mobile phone whilst driving and, either reading or replying to this\n","text message, was so distracted that he failed to negotiate a left-\n","hand bend, crossing the central white line into the path of Miss\n","Titley’s oncoming car. Miss Titley was pulled the wreckage of\n","her Daihatsu Cuore but died later from her injuries in hospital .\n","‘Miss Titley [had] a bright future ahead of her. She was also\n","returning home having spent an enjoyable evening with friends and was\n","driving responsibly. ‘She had arranged to contact her friends when she\n","got home to confirm that she had arrived safely. Her friends sadly\n","never heard from her after they parted company. ‘Miss Titley’s death\n","in these circumstances reiterates the danger of using a hand-held\n","mobile phone whilst driving.’ Police were unable to take breath or\n","blood tests from Eccleston-Todd immediately, but in tests several\n","hours after the accident he was only marginally under the drink-drive\n","limit. The judge agreed with police that he would have been over the\n","limit at the time his red Citroen hit Miss Titley’s blue Daihatsu\n","Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His\n","phone records showed he was also texting around the time of the crash.\n","PC Mark Furse, from Hampshire constabulary’s serious collision\n","investigation unit, said: 'Our thoughts are with Rachel's family at\n","this time. She had been out with friends at a pub in Shalfleet that\n","evening, but had not had any alcohol. 'Our investigation showed that\n","there was nothing she could have done to avoid the collision and sadly\n","it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and\n","met with friends at a pub where he drank at least three to four pints\n","of lager. He hadn't long left the pub to return home when the\n","collision occurred at around 9.30pm. 'We weren't able to take breath\n","or blood tests from him immediately and although blood taken several\n","hours after the collision showed he was marginally under the limit, we\n","maintain he would have been over the limit at the time of the\n","collision and in summing up today, the judge agreed. 'The analysis of\n","his phone records showed that he was texting on his phone around the\n","time of the collision so it's highly likely this would also have\n","contributed to his dangerous driving and loss of control.' Eccleston-\n","Todd was found guilty of causing death by dangerous driving following\n","a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-\n","Todd will now spend six years behind bars, but Rachel's family have\n","lost her forever. 'I hope this will make people think twice before\n","drinking any alcohol and getting behind the wheel, or using a phone\n","once they're on the road. 'The dangers of drink driving and driving\n","whilst using a mobile phone are obvious. Those who continue to do so\n","risk spending a substantial time in prison. This case highlights just\n","how tragic the consequences of committing these offences can be.' ‘Mr\n","Eccleston-Todd will now spend six years behind bars, but Rachel’s\n","family have lost her for ever. I hope this will make people think\n","twice before drinking any alcohol and getting behind the wheel, or\n","using a phone once they’re on the road. This case highlights just how\n","tragic the consequences of committing these offences can be.’\n","Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from\n","driving for eight years after which he will have to complete an\n","extended re-test.\n","\n","------------------------------------------------------------------------\n","\n","Summary:\n","\n"," <pad>CraigEccleston-Todd, 27, had drunk at least\n","three pints before driving car . Was using phone when he veered across\n","road in Yarmouth, Isle of Wight . Crashed head-on into 28-year-old\n","Rachel Titley's car, who died in hospital . Police say he would have\n","been over legal drink-drive limit at time of crash . He was found\n","guilty at Portsmouth Crown Court of causing death by dangerous driving\n",".<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad\n","><pad><pad><pad><pad><pad><pad><pad><pad>\n"]}],"source":["# print the article and its summary\n","print('Article:\\n\\n', detokenizer(input_batch[0]).split('<EOS>')[0])\n","\n","\n","print('\\n------------------------------------------------------------------------\\n')\n","\n","print('Summary:\\n\\n', detokenizer(input_batch[0]).split('<EOS>')[1])"]},{"cell_type":"markdown","metadata":{"id":"aNFVhgHoncGm"},"source":["You can see that the data has the following structure:\n","- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n","\n","The loss is taken only on the summary using cross_entropy as loss function."]},{"cell_type":"markdown","metadata":{"id":"Un8NHIRoj-1W"},"source":["<a name='2'></a>\n","# Part 2: Summarization with transformer\n","\n","We now have a data generator and have handled done the preprocessing.\n","Now we will build our model.\n","\n","We will be implementing the attention and then use it in your transformer model.\n","\n","Image:\n","<img src = 'https://docs.google.com/uc?export=download&id=1FLTIDih1WbBpvTehrULB1AEBmWH2vxpd'>\n","<a name='2.1'></a>\n"]},{"cell_type":"markdown","metadata":{"id":"7vxfvAMboD1f"},"source":["The formula for attention is this one:\n","\n","$$\n","\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right)\\\n","$$\n","\n","$d_{k}$ stands for the dimension of queries and keys.\\\n","$Q$ stands for Query.\\\n","$K$ stands for Key.\\\n","$V$ stands for Value.\\\n","$M$ stands for Mask.\n","\n","Masking is done using very negative values that will yield a similar effect to using $-\\infty $.\n","\n","\n","> Query, Key, and Value are three vectors used in attention mechanisms to compute the attention weights, which are used to determine which parts of the input to focus on.\n","\n","> Query:\\\n","The Query vector represents the current state of the model, or the \"query\" that the attention mechanism is trying to answer. In the case of natural language processing, the query vector might represent the current word being processed, or the current hidden state of the decoder in a machine translation system.\n","\n","> Key:\\\n","The Key vector represents the input being attended to. For example, the key vector might represent the hidden states of the encoder, which encode the source language input sequence.\n","\n","> Value:\\\n","The Value vector represents the corresponding output for each key vector. In the language translation example, the value vector might represent the hidden states of the decoder, which generate the target language output sequence.\n","\n","The attention mechanism works by computing a score between the query and key vectors, which determines how much attention should be paid to each key vector. The scores are computed using a dot product between the query and key vectors, and then normalized using a softmax function. The resulting attention weights are then used to compute a weighted sum of the value vectors, which produces the context vector representing the most relevant parts of the input.\n","\n","Overall, the Query, Key, and Value vectors are important components of attention mechanisms, as they allow the model to selectively focus on different parts of the input at different times. By computing attention weights based on the similarity between the query and key vectors, the model can learn to attend to the most relevant parts of the input, leading to better performance on a wide range of tasks.\n","\n","Reference:\n","[Attention is all you need](https://arxiv.org/abs/1706.03762): https://arxiv.org/abs/1706.03762\n","\n","Youtube Video that explains it really well:\n","https://www.youtube.com/watch?v=7wMQgveLiQ4&ab_channel=MatKelcey\n"]},{"cell_type":"code","source":["YT_V('7wMQgveLiQ4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"EO7bT9YnVimc","executionInfo":{"status":"ok","timestamp":1686686323939,"user_tz":240,"elapsed":301,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}},"outputId":"443cdce8-fd9b-49e0-b49e-244dd150c5c0"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.lib.display.YouTubeVideo at 0x7fadc70caaa0>"],"text/html":["\n","        <iframe\n","            width=\"400\"\n","            height=\"300\"\n","            src=\"https://www.youtube.com/embed/7wMQgveLiQ4\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","            \n","        ></iframe>\n","        "],"image/jpeg":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDBoYFhsaGRodHRsfIy8lIiIiIy8uLyktLjA1MDEvMTI0PFBCNThLOS0vRWFFS1NWW1xbMkFlbWRYbFBZXVcBERISGRYZLxsbMFdCN0JXV1dXV1dYV1dXV11XV1dXV1dXV1dXWVdXV1dXV1dXV1dXXVxXV1dXV1dXXVdXV1ddV//AABEIAWgB4AMBIgACEQEDEQH/xAAcAAEAAgMBAQEAAAAAAAAAAAAABAUBBgcDAgj/xABLEAACAQIBBwYKBwYGAAcBAAAAAQIDEQQFEiExQVHRBhMVVGGTFBYiU3GBkaGisQcjMjVSc8EzNEJicvAkgpLC0uFDRISj0+LxY//EABgBAQEBAQEAAAAAAAAAAAAAAAABAwIE/8QAHhEBAQACAgIDAAAAAAAAAAAAAAECEQMhMUESIjL/2gAMAwEAAhEDEQA/AOfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA6/VyXgYJOWGw6vvpR4H08kYNW/wANh9P/APKHAmV8IqiSlFux9uhe3kvQS/L06+uld0Vgs7N8Gw+du5qHA945BwebnPDYdL8qHAkeCrPz81528lqm3TtqY71XE37VtPIOBl9mhh3bdShwPvxcwfVqHdQ4E1YdtSUmnnK2hWPmOT4K6vKzSVr7iY22duqieLuD6tQ7qHAeLmD6tQ7qHAlQwEVK+fPbob3qwWToZyd5aNl9fpOkRfFzB9Wod1DgPF3B9Ww/dQ4Eno2NknKbt2/oI5Niv4p9unXsAjeLuD6th+6hwHi5g+rUO6hwJvgUc5SvK6Vlp1aLfJnzSyfGDunK+jW9dlbSBE8XcH1ah3UOB5VshYOP/lcPb8qHAta+HU1FO/ku6tvR54ym5LRuApo5NwTdvBaHdQ4Hr0LhOq4fuocDNHJzjOUrS8rXdt+xE/Me5lFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHuYzHuYFf0LhOq4fuocB0LhOq4fuocCwzHufsGY9zAr+hcJ1XD91DgOhcJ1XD91DgWGY9zGY9zAr+hcJ1XD91DgOhcJ1XD91DgWGY9zGY9zAr+hcJ1XD91DgOhcJ1XD91DgWGY9zGY9zAr+hcJ1XD91DgY6Dwjf7rQ7qPAscx7mfLTvs9Zhz/AId8flCWQcJ1Wh3UeBl5CwnVaHdR4FhB31mWeTdaKmeQ8J1ah3ceBQ5cWEwzUVhKTk1f9nG3yNjxtRyS5upmpadCvcoMdGGIq3lpa7Nhrxy2umvRylRT8rCUGuyCv8i8ydh8JiKanGhRvtWZHR6dB4TyXRley02a1bSswWNnhY1YU1Fycr3fYrG2eGp058eWzRyVhur0e7jwPGrkvDqT+opdn1cfTu9JSUOUVenG0ubm7t3lLT6NGg2VVM+NOpa2dFO399jMM5lHeNlRZ5Nw9pWoUtDUl9XHU/UVHKvBUoUFOnSpwu1pjFL5GwRelJ9sH6tRS8p3fA2euNRL5k47flFzk+NdIqNpaFc+kfFatGEXKbUYrW2YliIJKTkrSdk99z3PI+oybbutB4YirNStG2yyaflXenStVj08Jp/jj7UPCqf44+1bQPSeoh4fEynOpFwlFQdk3/F2olqaaummt6PKliKc3aElJ2vZPYDSFHKFTTehP3+/R/fvFTKTio3pTvKTWbpvo3aNO/1E2piIRlmu/b2ekxi8TTowz6jaje2hN/ITvqJuIM8pyVk6Mk27K99Lfq9fo3aj0jjp5t3Rmndq2nZ6tv6ErCYiFaGfC7jdrSmtRmjiKc3aLu0rtbVs0l1TaJHHTeujJK2u7+Vuw+vDJ2vzUtu3d6iVVqwg0pPTJpL1ma9RQV2m7uyS7SeVRVi5u/1UvW/+jCxc/NSt6+BJw1aNRNpNWtr7Umvczyo4+lOo6cXpV12NrXZi2TyslvcecsXNf+G9l9fDcfXhE1ocNO9Xt8iRVrQg4qTs5al6D0kkk3uG4iCsVUtd03f8Om/t1BYmra7h6Vp7OL9hKw9WFWOdDShGtBzzFe+n0aNa/vcwI6xMrSvFp7E09+n02WnQeax8rfs5P2r3WLCVlr0B2SvsGxBljJ2ebTfve2z1I+6mImtEacm2tr0eslRcXq0kbw6nn5lpfazb6LX9tyzvwl6eSxlTTenJJI+liajaWZb2u/Cy0kmdanG95RVtenV/d0fUJRkrxd1p9xFQXi6iveGq2m0t1/noPvwqdr5jWu97t6PQfeHx0KklFKWlXV7bPWe0a1NuynFvdctlnlJdoscXN/wP36XuWjR6Q8VNfwuWi+hSW3tJXPU7faj7TMKkJO0ZJu19DuRURYmp+D3PTr2+z2mY4md3nRe21k9x94jGwpycWpOyzna2rT29h7SrU1a8krq6uy2VNo3hc9tOWrT2PafKxdXR9U+3S93oJUq9NWu9eq132bDCxVL8S9/t9HaRUd4ufmpe/UfU8TNWtTlp/vYSa1SMIOb1LcedDFQnnaHHNte9tvrLq62m0eeNlnOMIN2enX/aMrFz0rm2u3S1oXoJPhNL8cfbuPqtVjCDm9S3BUWOJqa8x6Nlnq9O/wBRmniZuTzo5sd7v6j1oYqE87Q45tr3tt9Z9PEUkr58fU9+gXpI8PCJ5l1G8rvRpWjYz6oVpyflRcVbXvZ7qtTf8Ufb6j5eIpafLjo7SK+hcznwuldXepXPtRQHncXPt2R51K9OLzZSSe7aBm54YjESjKKUbp+nelu0a76T3p1YSdotN7j0zUBhPQyHh8Y5VXC0rrXdO3qZOSMKKJZVj5qSacbbXZ+xnhiHaXqJMpJNJtJvV2lflCdpW3pGXP8Al1xz7PSlLQUvKHFVFKnTpTcLpyk1bUtS09pZqZq2X6854icYRzs2KT8qz2s8uM7b6V1Gpiq10sRZZyhG+jObV2lbctb9BKVJTaknma7Zrt2EKFOs4xUaVs1NLy1t1vVrZIVKUIwvZaLPse3SevBz77TOYea7Ocb6fRv1lLlmmqNKKgtM5Nyl+heRU3CPla05N22LUimxNJuUs5pTjos9RtlNucrpT06NR5raWa9ug3qh5WGp2/CvlY1urWxDzKd0lNqNloWn1GwZJd8Mo7VePrTa+djzZyrjYy5aG1r0TXyZT8rX9Q7apSi/mWyelbruPqelFPyld8Gt8ZpfMywn2jvL81ulblLk2pFxniaUovWnfT7j5lyjya4qLxFJxjay07PUceMHteV2BZfyXr5+jf18B0/kvT9fR02voezStm84+AOyQ5T5OjHNWJppbtO31HzT5R5Ni7xxFJNK23V/aRx0Adgq8oMmzlnPFQv6XZ+nQYxnKDJtaGZPFQte+hvgcgAnV3E1HYMJyhybRhmQxULXb0t7dO4+qXKPJsHJxxFJOTu3p0+446C7pqOw1uUWTKjTniKUrar34GcRykyfUjZ4uCs0003dNeo46CK67gcu5NoRcYYuLTd/Kbb1W3dhijlrJcKjqLEwvptpdlfXZWORgXu7qy3Gajr2Ky7kyq05YqN0mk1KS0PZo9B7T5T5OlFxeKp2atrfA42AjsWG5R5NpQUY4qnZengFyjyapZ3hNO921r0N67aDjoA7K+VOTnp8Jp+/gPGjJ1reE0rbtPA40AOyw5T5OjqxNNX9PAiPLGTOc5zwuN87OtfRf2HJQWX4+Czfl2GfKDJcm3KvRbeu9+HYekeVGTkrLE00vXwONAg61h8s5MpzU44uN0rK70W9h7x5QZLTuq9FPfp2eo48C22+Uk07BHL+S1b6+lo9J9U+UeTIyco4ikm9bV+Bx0EV1vFZZyZVbcsXFXSTs7aF6u09pcosmSs5Yik2rWvfZq2HHgW23pNOxPlHkyyXhFKy0LXo9x8+MGS/P0vecfBFdirco8nTg4PFU1Hsvs9R5YfLmTKefbFQefa929m7RoORAu7rSa9uwPL+S278/Svq28D6q8o8nShmPFU1G1tF9nqOOgiuvYbLuTaedbFQec03d7vUZWXcleeo+/gcgBbd90k07Asv5LTb8Ip6VbW/T+i9gjygyWtVekvb6Tj4IOw+MOTM7O8IpZ2/SekeVGTlqxNNe3gcaAHZXyoyc9eJpafTwPmfKTJsneWIpN6r6e3s7WccAHY6fKTJsW3HEUk3rengenjXk/rVP38DjAA7P415P61T9/AeNeT+tU/fwOMADsz5U5ObTeJpNrVr0e48KuUaWImp0ainBaLrfuOQG68kK8I4V50op571tLcZcs+rTj/TbOcNUqY6HP4i8lnSlmpX7UvkXc8dTSb5yGhfiRrdCvDwdNuGdOo3s9/rZjhG1y0nSxCStqS3GcNarTcZyu1O6a3FXPERzvtR9qPbJlSKnN58UvStdz1SaY45W5dthctCKLGVoPEKmktC0vfJ6dJMx2UadOF8+LexJrWa9SqqVTOclfS3pNF5b1pdJRvFvTmtNelErIlTyai3TbXrtIpnXitClH2om5IxMFOac4pNLau1GXLOtuOO+k/EaJTW53XqfAp+UsvqHulKL+ZZ43Ewzoyz4u6V/KXofyKTLtWMsPZSTaktTMpjrJpb01sAG7EAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASujMR5it3cuA6MxHmK3dy4HdQBwrozEeYrd3LgOjMR5it3cuB3UAcK6MxHmK3dy4DozEeYrd3Lgd1AHCujMR5it3cuA6MxHmK3dy4HdQBwrozEeYrd3LgOjMR5it3cuB3UAcK6MxHmK3dy4DozEeYrd3Lgd1AHCujMR5it3cuA6MxHmK3dy4HdQBwrozEeYrd3LgOjMR5it3cuB3UAcK6MxHmK3dy4DozEeYrd3Lgd1AHCujMR5it3cuA6MxHmK3dy4HdQBwrozEeYrd3LgOjMR5it3cuB3UAcK6MxHmK3dy4DozEeYrd3Lgd1AHCujMR5it3cuA6MxHmK3dy4HdTIHCejMR5it3cuA6MxHmK3dy4HdgBwnozEeYrd3LgOjMR5it3cuB3YAcJ6MxHmK3dy4DozEeYrd3Lgd2AHCejMR5it3cuA6MxHmK3dy4HdgBwnozEeYrd3LgOjMR5it3cuB3YAcJ6MxHmK3dy4DozEeYrd3Lgd2AHCejMR5it3cuA6MxHmK3dy4HdgBwnozEeYrd3LgOjMR5it3cuB3YAcJ6MxHmK3dy4DozEeYrd3Lgd2AHCejMR5it3cuA6MxHmK3dy4HdgBwnozEeYrd3LgOjMR5it3cuB3YAcJ6MxHmK3dy4HxWwdWmrzpVILfKLS953g1T6SPu9fmx+TA5WAAAAAAAAAAAAA7+ZMGQAAAAAAAAAAAAAAAAAAAAAAAYAyDAAyYMgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAan9JH3evzY/Jm2Gp/SR93r82PyYHKwAAAAAAAAAAAAHfzIAAAAAAAAAAAAAAAAAAAAADAAoaeKxONlJ4apGhhoycVUzVKdRp2binoUb3V9N7F7NXTWq6KXknWSwscO7RrYf6upDamnofoa03A+amIxWClGVeosRhpSUZTzVGdO7snK2iUb69qL25S8qq8fBZ0Fpq4j6unBa25bbblrb7D3hg0kl4VXTSt9qP6xAtAVywMtmMr/+2/8AYffgVZasVU9cKb/2gTgQPBcRsxPtpR/SwdDFbK9L10X+kwJ4K/m8Z53D9zP/AOQzm4v8WHf+Wa/3MCeCA5Yv8OHf+ea/2swqmM81h/VWn/8AGBYAg8/ittCm/RWf6wMeFYjbhvZVj+tgJ4IDxtVa8LV9Uqb/ANxjpGp1TEf+1/zAsAQFlJ7cNiF/li/lJh5UitdLEdzJ/JATwV/S9PzeIX/p6v8AxNeyryvngq6U6fO0Z3cXmypzjbWmpKz161YDcTDNfyfyyweIjJqUoSim3GcbPR2q695nAZM8MhHE4tzm6qzoUs5qFOD0xWatcrPS2BfmTXsTh+jpQrUZy8Gc4wq0pSclHOeapwv9mzautVjYAMgAAAAAAAGp/SR93r82PyZthqf0kfd6/Nj8mBysAAAAAAAAAAAAB+gAAAAAAAAAAAAAAAAAAAAMACuys5ZrzK06ckr2ik/buJDqyqO1PRHbP9EfawsM1xaun9q+30gavkrA4nFNTq1aipf1O8vQXmKyDhqua3BxnFWU4ScZW3Zyd36yyirKy1IAalyZq4eGNxOHlG+Kpzko1JycpSp3uknLVa+rabPXw6np1M53lrJVeeWas6EubkpQam9SlJeSvW1Y6Bk6rVnSi69Pm6q0SjdNXW1NbGB9YeGatMdO0kJ3B8uO1awPoyfMZX9JkDIMADIAAGDIAwDIAwDIAwVNXk5halV1q0HWm9tR3S7FHUl6i3MARpYClzM6UIRhCUXG0YpaGrbCqyXlaGHpxw2MkqNWklBOeiNSMdCnGWp3S1a0XxUcq8asPgK1RpOSVoXV/KehARsdjIZQccNh3zlPPjKtVX2Ixi1LNT2ybS0I2Aj5Orxq4elUgkozgpJJW1q5JAwRcBjlXU2oyjmycfKWslALuaZAAQAAA1P6SPu9fmx+TNsNT+kj7vX5sfkwOVgAAAAAAAAAAAAP0AAAAAAAAAAAAAAAAAAYAEWTdVuK/Zr7T/E9y7D7xMm2qcdDlre5bWesIKKSWhIDMYpKy0JGTEppHi8Qv4U36APcNke9R7l7zPg9/tSb9fACLlWhCVGtm2VSSzk/5o6Y+9I9cJj41aUJq7z4p6FvR7xoRWxEPIitRlDzdScPUpO3uaAlc/LZB+4c5P8AD7z3sAI0uc1pRv6f+jKlU3R95IPnU/SB5Z1Tcvb/ANDnZr+D2M9wB4eEb4yXq4H1HERe1HrY+JUk9aQH2mDw8Ht9ltGM+cdaut64ASQedOqpamfYGQYMgAABg1Pl3hauJhChT0RgnWqSepJJqK7W9OjsNsIWWv3Sv+XL5AQOSNKrRwzw1ZWnQk4p7JRemMl2aWvUXphGQAAAAAAAABqf0kfd6/Nj8mbYan9JH3evzY/JgcrAAAAAAAAAAAAAfoAAAAAAAAAAAAAAAAAww3YhYzEN05KG3Rf06NG8D6oVF5VR/wAT0ehav77T7c5y+yrLe+BmhhlFLa0rHvYDxjh1rk2/SeqikZMgYMgwAK/JP/mHs5+f6IYzK9KndK9SaV3GOm3pepaiBkjD4qdCMpTVGNRuo1FXl5bctLeha0BeVa0YK8pKK7XYhSy1R1Qzqj/ki5e/UZpZHop3knUl+Kbzn7ydGCWpJegCu8Pry+xhpJb5yS+Vz5m8bJaqEP8AVLgWgYFb4Pi3rrwXop8WZ8CxPWn6oInRkrIzd7F7QIHgWJ60/wDQh4NilqxEH6af/ZPs94zO1gV6jjVtoT9Uo/qw8Tio/aw8ZL+SfFFhmen2jM7WBQ4vKNWdSFChRdPETWdnVPswgtDk7PytyR5wjiPIlTx1SUptqnztKHNTau7LNSklZOzvs2kjFTWHyjCrUdqVakqOe9UZxk5JPdnKT9aPuOCdHmYTcVhcP5UJXSasmkp32JPWtfZtCZknKHhFNuUMypCThUh+GS1+lbV2MnFLycfOeE4laKeIq51PtjGKhnevNbLkDIAAEHLX7pX/AC5fInEHLX7pX/ofyAmoyYMgAAAAAAAADU/pI+71+bH5M2w1P6SPu9fmx+TA5WAAAAAAAAAAAAA7+ZMGQAAAAAAAAAAAw2earRcc5NWte584yebTk9yZrGRZVq1qd7Uou7e17bAbIk6mvRHdvGJjbm1vmvdp/QkRVkeGK10/6/0YEgyYMgDFwaHyh5U+D5Vja8oUYOLSeuUv/wAQG64zGQoxzpv0JaW3uS2kFUa+J01G6NL8EX5Uv6ns9CI/J7OxEfCa1KcZv7OfZWX8sdi9Oll5JpJt6EgKnH4eEYU8LSio887StsprTN+zR6y3irJJakVuTE6s54qS+35NJPZTWp/5np9hZAZMAjTxKz7X0L5gSW7HzpfYIWem9z6eoD4pRskfZiC0IyBkAAAAB5V6EKkHCpFThJWcZK6ZWQ5MYOLX1TaWlRlUnKK9EW7e4uABo/KDKtXJeUVUgs7DV1nTp/zLQ3Hc7WNsyZlOji6Sq0ZqUXr3p7mtjIeX8ixxng+ck1TqqTT2x/iXy9h9ZKw8KNSdJwiqsVomkk6lO+hu2trU/btAtTJgyAIOW/3Sv/QycQct/ulb+hgTTJgyAAAAAAAAANT+kj7vX5sfkzbDU/pI+71+bH5MDlYAAAAAAAAAAAADv5kAAAAAAAAADB44zF06FN1KslGC2v3Jb32HsU2UIqplHCU5q8IwqVIp6nUjmpetKUmB4YzLUqlKX+DxcabVs901q35qlnW9RZZHjR5iEqElOm1oktu/13IWFx0niZ06k3GHOyVN3VptJeR2W121v1GckJQx2OpQ/Zp055q1RnOLzkvTaL9faBa4rFQo05VKklGEVdtlPVyrXqRU6eBrumpKSbcIyaW1Rbvq32Z6ZdSlXwMJ/s5Vm2tjlGDcE/Xp9R6SVRYmVK9Rwq2qKV35ObZTjfYn5Nl/NLcBLyflCniYZ9NvQ7SjJWlGS1xknpTPbE4iFKEqlSShCKu5N6EVVBKOVKqhqnQjKol+JSai322v7BlqKnisDSn+zlUnJp6nKEbxT37X/lA+anKCThKdPBYuULO0sxK/aouWdb1FRyW5OUajeNryjWq1JOVtkHe7TT05y7dRbVcbKOLqRnOUaClBZyl9mTimovdFvbv0H1g0oZTxMIaITowqTS1Z7co39LSXsAt6tWFODnOSjCKu29CSRr+NylWxVO1HB13Qk1nSbjBzhtUYyd9PbbQSuUqUo4aE/wBlPEQVS+prS0n2OSie2MrVKddQUpZteObTsl5E1r2bY6dP4HvA98mZQp4iD5tSi4PNlTks2UHua2fIk1qsYRlOclGMVdtuySW1lRNKOVqWZrnhp872qMo5jfbdyVzPKKKm8JSl+yqV0qi2NKMpJPsckgPmXKBzi5UcJiqlPZUUEk1vipSTa9RLyXWoV4Z9OWfZ2kmrOL3Si9KfpIGMxcoYyopSmsOoUs9xk1mNynZpLUm1FSa1K2xtr1ilDKq5vVVw7lUS2uMkoSfbZtX7ALmc1GLlJpRSu29CSRTLLdSqnLD4StVp7KjcYKXbHOd2u3QfXKl/4VRf2JVacan9DmlL1WPXHzqUqlNwu4Ti6SitUZ/wS7FrT9QHtk3KlPEZySnCpDROnUVpR3XW57GtBNKTGQzMo4LNd5yhUjUe1wUU03/mt7WXYGQAAAAAAAYImUMNKcYzp6KtN50HqT3xf8r1ex7CWAPHCYmNWmpxur609cWtDi+1PQe5W1/8PW51fsqjSqL8MtUZ+h6E/U95YgZIOW/3St/QycQct/ulb+hgTgYMgAAAAAAAADU/pI+71+bH5M2w1P6SPu9fmx+TA5WAAAAAAAAAAAAA/QAAAAAAAAAAAwQcq5NWIjG03Tq05Z9OotcZatW1NOzRPMAa/VnjoyhSksHGU3aNTytMkr3ULa7JvXsLTJeT1h6bjnOc5ycqlSWucntfyS2JFD9IblDBU6kG4yp1ouLWtOzLDktl6OPw6loVWGipHc9j9D4gWOUcDDEUnTndJ2aa0OMlpUk96ZVYSeOnGcKWIw9RQk6bqypyzrx13Sdm18zw5a8ofA6HN0n9fUTtb+COpy4f9H3yBg1kyk3rlKb9smBaZLyasOptzlUq1HnVKktcnqWhaEktSPvKeT44inmtuMotShOOuElqkiWAKGtWx9PNhPwR58lBVXnK7adrwtr0bywyXk7mFOUpupWqPOqVGrXexJbIrYiq5fJ9GzlG6lCcJJrY1JafefXI7lCsdh7Tf19Oymvxbpev5gXWOwcMRSlSqK8JLTs7U09jT0lPhvDb1KVLEUK3NSzXOpCWdF2vZ5rtJpNbjPK3L6wOH8izrzuqa3b5PsRE+jpN4CU5NuU6sm29urSBdZMyZzLnUqTdWvUtn1GraFqjFfwxW49so4GGJpSpTuk7NNaHGS0qSexpkkAUNWtj8PGKnLC1I3UVWm5Qd27K8Umr3ttROyXk10XOrVqc7XqWz52srLVGK2RRD5bU87JeJW5J+ySf6EPkRyj8Mo81Vl/iKa0/zx2S9OxgbHisPCrTlTqRUoTTUk9qZS4aGLpznh6OIpVo0lH9tGWfBO9k5RdpOy9OokcpsuRwOHc/tVZeTTjvlv8AQil+jecqlDE1ZycpzreU3tean+oGwZOyY6c5Vq1TnsRNWcrWUYr+GEdi97LEGQAAAAAAAAMGTBkD4qU1OLjJJxkrNPamQsBUcJPD1G3KCvTk/wCOHFan6ntJ5FyhhnUipQaVWm86m+3an2NaH6QJRCy3+6Vv6T3weJVWmppNbHF64taHF9qZHy3+6Vv6QJxkwZAAAAAAAAAGp/SR93r82PyZthqf0kfd6/Nj8mBysAAAAAAAAAAAAB+gAAAAAAAAADAGQYPOlXhO6jOMmnZ2adnuYETLOTI4ulGlL7KqRlLtUXdr16iPkrBUqdaolBRqUnZSWhypz8qKdtaWmKv+EtzX/CcRicQ6mCVKNOKdOVWrdqpZ/wAEYtN5rzldvawJeLwlOeIhBQjeX1lV20uMdEYt/wBTWj+VkrJWAWGoRox+zFu3obbXuZV4bE1sPiX4bGn9e4whWpXzFa+bCSemLbbetq7L8DIB4UsZSnKUI1IucXaUbq69K1geGWcnrFYeVFuym439Ckm/cRMn5Po08ROKpxjOn5dOSVnmT1xvtSkpaPQXBQYjE18Rib4KNP6nOp1K1W+Y27XhFR0yaaWnQgJeUcNTlWpwUFn1XectvN09LXobzVb+ZkjJWTo4WnKnD7LnKaW7Od7eoq44nEYbEc5jY05QqKNONWldRp6X9uMndZzaV77EbABkA8I4ym6jpqpHPjrhdZy9WsDyytg/CMNVo3tzkHG+6+0r8HkuhRxPNqnFZsVUoyWhr+GautmhNr+Yuyix+KrV8SoYOMHOg/rKtS+Ys5aYaNMnqdtGpaQJOVsPCUoRzU6lVqnnbVT+1O25WTXpaPfJmTYYbnlCyjUqOpZbLpJr2oqqmIxWGrRr4yNKdFRzOcoqS5vOavKUZN6NC0p6LGxJ3AyAYAyAAAMSkkrt2QTuBkHxUmoxcpNKKV23qSRTU8o4zErnMLRpQov7MsRKSc1vUYrQn2+wC7MlXgMqTlW8HxNLmq+bnRs86FRbXCXZtTV9JZgZMA86NeFRXhOMlvi0wIWJ/wAPV55fsptKqvwvVGp+j7LPYfeW/wB0rf0kyqouMs+2bZ519Vtt+w1mlWxWIw86WHpRnQ0xp1q03FygtVlZt7rvXa4G0GSswGVXOo6Fak6NdLOUW7xmtrhLbb2lmAAAAAAAAANT+kj7vX5sfkzbDU/pI+71+bH5MDlYAAAAAAAAAAAAD9AAAAAAAAA+ZXs7a9lyFzWL89R7qX/MngCBzOK8/S7l/wDM0zlBkrGYnFqWFi3OKtOvGLopvddy8q29HQQBrOSslZTp05KvjYyvFpRzc53to8t2ZY8l6kXgKCirZkFCUdsZR0ST7bplqUeWcBCjCti6dWph5qLlN02rTstF4yTV+21wPTlVJPCOn/4lScIUltz85NNLstf1EpYbE9Zj3P8A9iNknJtN83ipVKlerKF4zqNeSpK9opWS9hbgQfBsT1ld0uJqPKXImLxmJhzEW5U9Eq8oqn6Ene8kt/sN9MAa1kPIePoJc9j5SX4LZy/1S0k3ktOPgih/HTnOFRbVNSd7+m9/WXBU5TyXTvPExqVMPUUbznTa8pR0+VFpp+wBypqRWArqSvnxzIx2uctEUu27R7UsLilGKeJjdJX+q/8AsQsh4GFaFDGVKtSvNxUoc41aDa02jFJX2XL0CD4NiOsx7pcTVeVWSMVi6lOFKLqTpvTWcVTUf5U73ZvAA1bIOQMfQs6uPlmrXTSz16Lz1eosOTDSp1qb/awr1Oc33lJyi/XFouSuyhkqFSfPKc6NWKtzlN2bitkk7qS9KA+su1oQwdd1LZvNyVt91ZJdrbseGTaGMhh6MZVKN404p51OTd0lredrImRMFTxVOjiqtapiGneCqNZsWm1dRikr9rNhAg5mL85h+7l/zGZi/OYfu5/8yeAIGZi/x4fu5/8AMZmL/Hh/9E/+RPAFdUWKUXnTw2bbTeErW7fKNDhhsovEzeTlVhRvoavGn2uMZv7J0xoAarjqWUOjsQsVKjLyNKpp51la+nVqvsNmw84ypxlCzg4pxtqtbQejRrOVcH4BTz8PiqmHpyqRjzebGcU5ySbipLydbdlo0AS8veViMDCFue55yWi9oKEs9vs0pelotMyr+OH+h9n83pI2TskwoTlUlOdWvNWlVqPTZakktEV2IsAPDMq/jj/o/wCzRMs5KxeJxrqYGnOnsnWV6Sm1tSvd7dO33voQA0/E5PyjSyfiPCcWqq5vyoKF3m/xeXoeq+w2vCuDpw5u2ZmrNtqtbRb1Hq1dWZrmMwk8Eo+D4vmaU6kYRpTpqpGMpu3k6U0uy9gJWXLeEYFL9rz94783Mln+q36FyV2T8lc1UdarUlXrtZvOSSVl+GMVoiiyAAAAAAAAAGp/SR93r82PyZthqf0kfd6/Nj8mBysAAAAAAAAAAAAB+gAYAGQAAAAAAAAABg1X6Q8W44NUIJudeVrLXmx8p/JG1FdlrDw5ivVcU5xozinuTWm269l7AK/kNinPJ8ITup0ZOnJPQ1bSrr0NGxHhh8NCLc4xSlNLOa22WhvtPcAAABrvLnFyp4CUIJupWapxS0t30v3JmxEbF4eElntXlCMs17rrS12ga79HmKcsE6M01OjNqz12l5S+bNqIOS8NBU6VRRSnKlCMmtqS0X32u/aTwAAAFLyuxjo4Cs43z5rm4pa256NHbrLo8a2HhOUJSipODzo32O1r+mzftA1L6N68lh6uHqJqVOamk1byZr5XT9puRW5Fw0OZoVc1c5zSjnb1rs9+n5ssgMgAAAAAAAwaH9JVedR0MNTjKVr1JqKvujH5v3G+FblTDQjHnFFZ86tFSltaVSNl6NftYH3kHG+EYOhWvdygr+laH70ywPHD4aFJNQiopycmlqu9L9+k9gAAAwaD9JWUGp4ehDXH659myP6m/FVlDA04zVXNvOpVpqTenyU/srs7O0CZk3Fqvh6VVapwUvaiUR8Fg4UKfN01aCbcVuu72XZpJAAAAAAAAAA1P6SPu9fmx+TNsNT+kj7vX5sfkwOVgAAAAAAAAAAAAO/mQAAAAAAAAAAAAEPK8HLC14xTcnTkkltbTJhgDEVoXoPowZAAAAeWJTdOdtLzXb2HqYA8cDFxoUk1ZqEU16ke5gyAAAAw9RkwBEyTBxwtGMk1JQV09jsTDBkAAAAAAAAAQsp03KEFFN/W027blNNsmmABkwZAAAAQcpq/M/nQ/UnGABkwZAAAAAAAAAGp/SR93r82PyZthqf0kfd6/Nj8mBysAAAAAAAAAAAAB+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADU/pI+71+bH5M2w1P6SPu9fmx+TA5WAAAAAAAAAAAAA7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuA8dMndY+CXA4+AOweOmTusfBLgPHTJ3WPglwOPgDsHjpk7rHwS4Dx0yd1j4JcDj4A7B46ZO6x8EuBr3LblFhMVg1ToVc+fOJ2zWtCvvRoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMyVm1o0bjAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMqOhvRoMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//Z\n"},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"5AL6eP5u8ptN"},"source":["<a name='2.1'></a>\n","## 2.1 Dot product attention\n","\n","We implement dot product attention which takes in a query, key, value, and a mask. It returns the output.\n","\n","In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below.\n","\n","Functions that will be used to create tensors and display useful information:\n","   - `create_tensor`  creates a `jax numpy array` from a list of lists. [more on jaz numpy array](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.array.html)\n","   - `display_tensor` prints out the shape and the actual tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PW6fjyV0oD1f"},"outputs":[],"source":["def create_tensor(t):\n","    \"\"\"Create tensor from list of lists\"\"\"\n","    return jnp.array(t)\n","\n","\n","def display_tensor(t, name):\n","    \"\"\"Display shape and tensor\"\"\"\n","    print(f'{name} shape: {t.shape}\\n')\n","    print(f'{t}\\n')"]},{"cell_type":"code","source":["# trax.fastmath.set_backend('jax') #or tensorflow-numpy\n","\n","# # And this is how you set a TPU backend in Jax using Colab:\n","# # Add TPU Address to Globals"],"metadata":{"id":"mi_J824Baba4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSauPt0NUl_o"},"outputs":[],"source":["# DotProductAttention\n","def DotProductAttention(query, key, value, mask):\n","    \"\"\"Dot product self-attention.\n","    Args:\n","        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n","        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n","        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n","        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n","\n","    Returns:\n","        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n","    \"\"\"\n","\n","    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n","\n","    # depth/dimension of the query embedding for scaling down the dot product\n","    depth = query.shape[-1]\n","\n","    # Calculating scaled query key dot product according to formula above\n","    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n","\n","    # Apply the mask\n","    if mask is not None: # The 'None' in this line does not need to be replaced\n","        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n","    #print(dots)\n","    # Softmax formula implementation\n","    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n","    # Hint: Last axis should be used and keepdims should be True\n","    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n","    logsumexp = trax.fastmath.logsumexp(dots,axis=-1,keepdims=True)\n","\n","    # Take exponential of dots minus logsumexp to get softmax\n","    # Useing jnp.exp()\n","    dots = jnp.exp(dots-logsumexp)\n","\n","    # Multiply dots by value to get self-attention\n","    # Useing jnp.matmul()\n","    attention = jnp.matmul(dots,value)\n","\n","    return attention"]},{"cell_type":"markdown","source":["> The function first checks that the embedding dimensions of query, key, and value are all the same by using an assertion statement. The variable depth is then assigned the value of the last dimension of the query array. This value is used to scale down the dot product to avoid numerical instability issues.\n","\n","> The dot product of the query and the transposed key arrays is then calculated and divided by the square root of the depth variable. If a mask is provided, it is applied to the dots array. The dots array is then passed through the softmax function to get the attention scores. The softmax function is implemented by first using the trax.fastmath.logsumexp() function to avoid underflow by division by large numbers. Finally, the attention scores are computed by taking the dot product of the attention scores and the value array."],"metadata":{"id":"sffidGdCpIWw"}},{"cell_type":"markdown","metadata":{"id":"2y2PSiLVRWQ2"},"source":["<a name='2.2'></a>\n","\n","## 2.2 Causal Attention\n","\n","Implementing causal attention: multi-headed attention with a mask to attend only to words that occurred before.\n"]},{"cell_type":"markdown","metadata":{"id":"aZkwcMw7oD1h"},"source":["The following 3 functions would normally be defined within the `CausalAttention` function further below.\n","\n","* compute_attention_heads_closure\n","* dot_product_self_attention\n","* compute_attention_output\n","\n","However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`.\n","\n","### Support Functions\n","\n","<span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (batch_size, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (batch_size $\\times$ n_heads, seqlen, d_head).\n","\n","**For the closures you only have to fill the inner function.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_60sgx8oD1h"},"outputs":[],"source":["# compute_attention_heads:\n","\n","# Compute_attention_heads_closure\n","# > It is a higher-order function that returns a compute_attention_heads specifed in the argument\n","def compute_attention_heads_closure(n_heads, dim_head):\n","    \"\"\" Function that simulates environment inside CausalAttention function.\n","    Args:\n","        n_heads (int): number of attention heads.\n","        dim_head (int):  dimensionality of heads.\n","    Returns:\n","        function: compute_attention_heads function\n","    \"\"\"\n","\n","    def compute_attention_heads(x):\n","        \"\"\" Compute the attention heads.\n","        Args:\n","            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n","        Returns:\n","            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n","        \"\"\"\n","        # Size of the x's batch dimension\n","        batch_size = x.shape[0]\n","        # Length of the sequence\n","        len_seq = x.shape[1]\n","        # Reshape x using jnp.reshape()\n","        x = jnp.reshape(x,(batch_size, len_seq, n_heads, dim_head))\n","        # Transpose x using jnp.transpose\n","        x = jnp.reshape(x,(-1, len_seq, dim_head))\n","\n","        return x\n","\n","    return compute_attention_heads"]},{"cell_type":"markdown","source":["> The purpose of this function is to reshape the input tensor x into a 3D tensor where the batch dimension is merged with the attention head dimension, so that attention can be computed across all the heads in parallel."],"metadata":{"id":"nt3qpSgXqmwr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6S2llCGkoD1h"},"outputs":[],"source":["# dot_product_self_attention :\n","# Creates a mask matrix with False values above the diagonal and True values below\n","# and calls DotProductAttention which implements dot product self attention.\n","def dot_product_self_attention(q, k, v):\n","    \"\"\" Masked dot product self attention.\n","    Args:\n","        q (jax.interpreters.xla.DeviceArray): queries.\n","        k (jax.interpreters.xla.DeviceArray): keys.\n","        v (jax.interpreters.xla.DeviceArray): values.\n","    Returns:\n","        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n","    \"\"\"\n","    # mask size should be equal to L_q\n","    mask_size = q.shape[-2]\n","\n","    # matrix with ones below the diagonal and 0s above.\n","    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n","\n","    return DotProductAttention(q, k, v, mask)"]},{"cell_type":"markdown","source":["> The function creates a mask matrix of size mask_size with all elements initialized to 1 below the diagonal and 0 above the diagonal using jnp.tril function. This is because self-attention should only attend to tokens that come before the current token in the input sequence.\n","\n","> The DotProductAttention function is then called with q, k, v, and the mask as input arguments to compute the masked dot product self-attention tensor which is then returned."],"metadata":{"id":"iWPOm1nmq4kc"}},{"cell_type":"markdown","metadata":{"id":"62wvzrRjoD1i"},"source":["<span style='color:blue'> compute_attention_output </span>:\n","\n","Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BB2sV6droD1i"},"outputs":[],"source":["# compute_attention_output :\n","# Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (batch_size, seqlen, n_heads  ×  d_head). These operations concatenate (stack/merge) the heads.\n","def compute_attention_output_closure(n_heads, d_head):\n","    \"\"\" Function that simulates environment inside CausalAttention function.\n","    Args:\n","        d_head (int):  dimensionality of heads.\n","        n_heads (int): number of attention heads.\n","    Returns:\n","        function: compute_attention_output function\n","    \"\"\"\n","\n","    def compute_attention_output(x):\n","        \"\"\" Compute the attention output.\n","        Args:\n","            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n","        Returns:\n","            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n","        \"\"\"\n","        # Length of the sequence\n","        seqlen = x.shape[1]\n","\n","        # Reshaping x using jnp.reshape() to shape\n","        x = jnp.reshape(x,(-1, n_heads, seqlen, d_head))\n","        # Transposing x using jnp.transpose() to shape\n","        x = jnp.transpose(x,(0,2,1,3))\n","\n","        # Reshape to allow to concatenate the heads\n","        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n","\n","    return compute_attention_output"]},{"cell_type":"markdown","source":["> The function first reshapes the tensor x using jnp.reshape() to shape (-1, n_heads, seqlen, d_head), then transposes it using jnp.transpose() to shape (0,2,1,3). Finally, it reshapes the tensor again to allow concatenation of the heads to shape (-1, seqlen, n_heads * d_head) and returns it.\n","\n","> Overall, this function is used to reverse the operation performed by the compute_attention_heads_closure() function, which concatenated the heads together before sending them through the attention mechanism."],"metadata":{"id":"kdiABAr6sD5A"}},{"cell_type":"markdown","metadata":{"id":"nwJj21hfoD1i"},"source":["### Causal Attention Function\n","\n","Now it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:"]},{"cell_type":"markdown","metadata":{"id":"EKDiwU3UoD1i"},"source":["**Instructions:** Implement the causal attention.\n","Your model returns the causal attention through a $tl.Serial$ with the following:\n","\n","- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n","- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n","- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in compute_attention_output_closure to allow for parallel computing.\n","- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n","\n","Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9Adn6DtRWRG"},"outputs":[],"source":["# CausalAttention\n","def CausalAttention(d_feature,\n","                    n_heads,\n","                    compute_attention_heads_closure=compute_attention_heads_closure,\n","                    dot_product_self_attention=dot_product_self_attention,\n","                    compute_attention_output_closure=compute_attention_output_closure,\n","                    mode='train'):\n","\n","    \"\"\"Transformer-style multi-headed causal attention.\n","\n","    Args:\n","        d_feature (int):  dimensionality of feature embedding.\n","        n_heads (int): number of attention heads.\n","        compute_attention_heads_closure (function): Closure around compute_attention heads.\n","        dot_product_self_attention (function): dot_product_self_attention function.\n","        compute_attention_output_closure (function): Closure around compute_attention_output.\n","        mode (str): 'train' or 'eval'.\n","\n","    Returns:\n","        trax.layers.combinators.Serial: Multi-headed self-attention model.\n","    \"\"\"\n","\n","    assert d_feature % n_heads == 0\n","    dim_head = d_feature // n_heads\n","    # Computing Functional Attention heads\n","    # function with the correct parameters to get the actual uncalled function.\n","    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, dim_head), n_out=1)\n","\n","\n","    return tl.Serial(\n","        tl.Branch( # creating three towers for one input, takes activations and creates queries keys and values\n","            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # queries\n","            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # keys\n","            [tl.Dense(n_units=d_feature), ComputeAttentionHeads], # values\n","        ),\n","\n","        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n","        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, dim_head), n_out=1), # to allow for parallel\n","        tl.Dense(d_feature) # Final dense layer\n","    )"]},{"cell_type":"markdown","source":["> The function returns a multi-headed self-attention model implemented using the Trax deep learning library.\n","\n","> Inside this function, the code first checks then calculates dim_head which is the dimensionality of a single head. It then creates a ComputeAttentionHeads function which is a closure around the compute_attention_heads function and takes n_heads and dim_head as input.\n","\n","> The attention model is implemented using the Trax Serial combinator. Inside this combinator, there are three branches. Each branch contains a Dense layer and a call to the ComputeAttentionHeads function. These branches are used to create queries, keys, and values, respectively.\n","\n","> The output of each branch is then passed to the dot_product_self_attention function, which implements the dot product self-attention mechanism using the queries, keys, and values.\n","\n","> The output of the self-attention mechanism is then passed to the compute_attention_output_closure function, which reshapes and concatenates the attention heads.\n","\n","> Finally, the concatenated attention output is passed through a Dense layer with d_feature units to produce the final output of the model."],"metadata":{"id":"8aWdwO93tQvq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVNAAuWcoD1i","outputId":"0b539926-82c5-4014-9acd-07f3b1070997"},"outputs":[{"output_type":"stream","name":"stdout","text":["Serial[\n","  Branch_out3[\n","    [Dense_512, AttnHeads]\n","    [Dense_512, AttnHeads]\n","    [Dense_512, AttnHeads]\n","  ]\n","  DotProductAttn_in3\n","  AttnOutput\n","  Dense_512\n","]\n"]}],"source":["# Preview\n","print(CausalAttention(d_feature=512, n_heads=8))"]},{"cell_type":"markdown","metadata":{"id":"8D9j27PbmTi0"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"W6zwtPjqRWRJ"},"source":["<a name='2.3'></a>\n","\n","## 2.3 Transformer decoder block\n","\n","Implementing the causal part of the transformer\n","\n","Image:\n","<img src = 'https://docs.google.com/uc?export=download&id=1GtTPbuNW3Qhqa0PJb9VfX1JUbxv1mb7t'>\n","\n","We will call the `CausalAttention` or Masked multi-head attention function you implemented the transformer decoder block.\n","\n","Adding a feedforward which consists of:\n","\n","- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n","- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n","- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n","- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n","- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n","- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n","\n","After this we can go ahead and implement the entire block using:\n","\n","- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout.\n","\n","- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKOxnRbp1K5U"},"outputs":[],"source":["# DecoderBlock\n","def DecoderBlock(d_model, d_ff_n, n_heads,\n","                 dropout_rate, mode, ff_activation):\n","    \"\"\"The function will return a list of layers that implements a Transformer decoder block.\n","\n","    Activation tensor is input\n","\n","    Args:\n","        d_model (int):  depth of embedding.\n","        d_ff (int): depth of feed-forward layer.\n","        n_heads (int): number of attention heads.\n","        dropout (float): dropout rate (how much to drop out).\n","        mode (str): 'train' or 'eval'.\n","        ff_activation (function): the non-linearity in feed-forward layer.\n","\n","    Returns:\n","        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n","    \"\"\"\n","\n","    # A masked multi-head attention block using CausalAttention function\n","    causal_attention = CausalAttention(d_model, n_heads=n_heads, mode=mode)\n","\n","    # Create feed-forward block\n","    # It is a list with individual steps\n","    # It consist of two dense layers with dropout and input normalized\n","    feed_forward = [\n","        # Normalizing layer inputs\n","        tl.LayerNorm(),\n","        # Adding first dense layer\n","        tl.Dense(d_ff_n),\n","        # Activation (Generally ReLU)\n","        ff_activation(),\n","        # Adding dropout\n","        tl.Dropout(rate=dropout_rate,mode=mode),\n","        # Adding second feed forward layer\n","        tl.Dense(d_model),\n","        # Adding dropout\n","        tl.Dropout(rate=dropout_rate,mode=mode)\n","    ]\n","\n","    # Contains two layes one with attention with normalization and feed-forward blocks\n","    return [\n","      tl.Residual(\n","          # Normalizing layer input\n","          tl.LayerNorm(),\n","          # Adding causal attention block previously defined\n","          causal_attention,\n","          # Adding dropout with rate and mode specified\n","          tl.Dropout(rate=dropout_rate,mode=mode)\n","        ),\n","      tl.Residual(\n","          # Adding the feed forward block\n","          feed_forward\n","        ),\n","      ]"]},{"cell_type":"markdown","source":["> The DecoderBlock function takes the hyperparameters required to build a decoder block for a Transformer model and returns a list of layers that implement the decoder block.\n","\n","> The decoder block is composed of a masked multi-head attention block that uses the CausalAttention function and a feed-forward block. The feed-forward block is a list of two dense layers with ReLU activation, dropout, and input normalization.\n","\n","> The masked multi-head attention block and feed-forward block are then wrapped inside two Residual blocks with layer normalization and dropout.\n","\n","> The function takes the following arguments:\n","\n","> * d_model: an integer representing the depth of the embedding.\n","> * d_ff_n: an integer representing the depth of the feed-forward layer.\n","> * n_heads: an integer representing the number of attention heads.\n","> * dropout_rate: a float representing the dropout rate.\n","> * mode: a string representing the mode of the decoder block, either \"train\" or \"eval\".\n","> * ff_activation: a function representing the non-linearity in the feed-forward layer.\n","\n","> The function then returns a list of layers that implement the decoder block."],"metadata":{"id":"YLsgrHM9uuy6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7mCsIL3oD1j","outputId":"bcd5b063-93a7-45f9-bd50-8b40daa3f887"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Serial[\n","  Branch_out2[\n","    None\n","    Serial[\n","      LayerNorm\n","      Serial[\n","        Branch_out3[\n","          [Dense_512, AttnHeads]\n","          [Dense_512, AttnHeads]\n","          [Dense_512, AttnHeads]\n","        ]\n","        DotProductAttn_in3\n","        AttnOutput\n","        Dense_512\n","      ]\n","      Dropout\n","    ]\n","  ]\n","  Add_in2\n","], Serial[\n","  Branch_out2[\n","    None\n","    Serial[\n","      LayerNorm\n","      Dense_2048\n","      Serial[\n","        Relu\n","      ]\n","      Dropout\n","      Dense_512\n","      Dropout\n","    ]\n","  ]\n","  Add_in2\n","]]\n"]}],"source":["# Testing\n","print(DecoderBlock(d_model=512, d_ff_n=2048, n_heads=8, dropout_rate=0.1, mode='train', ff_activation=tl.Relu))"]},{"cell_type":"markdown","metadata":{"id":"SoFv-nfLRWRN"},"source":["<a name='2.4'></a>\n","## 2.4 Transformer Language Model\n","\n","Building the tranformer language Model. We will use all fuctions/components we built above\n","\n","**Building the model:**\n","\n","We will be using the following:\n","\n","- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n","    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n","    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n","    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n","\n","- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n","- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n","    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n","    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n","    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n","    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n","    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n","    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n","\n","\n"]},{"cell_type":"code","source":["# # import requests\n","# trax.fastmath.set_backend('jax') #or tensorflow-numpy\n","\n","# # And this is how you set a TPU backend in Jax using Colab:\n","# # Add TPU Address to Globals"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"id":"dao5mUaXY7BB","outputId":"a37abfc4-45c4-4cc1-af92-de2f5471e950"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-f82f87ac1733>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Add TPU Address to Globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'TPU_DRIVER_MODE'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':8475/requestversion/tpu_driver0.1-dev20191206'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mTPU_DRIVER_MODE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yi4LJO1RWRS"},"outputs":[],"source":["# TransformerLM\n","def TransformerLM(vocab_size=33300,\n","                  d_model=512,\n","                  d_ff=2048,\n","                  n_layers=6,\n","                  n_heads=8,\n","                  dropout=0.1,\n","                  max_len=4096,\n","                  mode='train',\n","                  ff_activation=tl.Relu):\n","    \"\"\"Returns a Transformer language model.\n","\n","    The input to the model is a tensor of tokens. (This model uses only the\n","    decoder part of the overall Transformer.)\n","\n","    Args:\n","        vocab_size (int): vocab size.\n","        d_model (int):  depth of embedding.\n","        d_ff (int): depth of feed-forward layer.\n","        n_heads (int): the no. of attention heads.\n","        n_layers (int): the no. of decoder layers.\n","        dropout (float): dropout rate (how much to drop out).\n","        max_len (int): maximum symbol length for positional encoding.\n","        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n","        ff_activation (function): the non-linearity in feed-forward layer.\n","\n","    Returns:\n","        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n","        to activations over a vocab set.\n","    \"\"\"\n","\n","    # Embedding inputs and positional encoder\n","    positional_encoder = [\n","        # Adding embedding layer\n","        tl.Embedding(vocab_size,d_model),\n","        # Using dropout with rate and mode specified\n","        tl.Dropout(rate=dropout,mode=mode),\n","        # Adding positional encoding layer\n","        tl.PositionalEncoding(max_len=max_len,mode=mode)]\n","\n","    # A stack list of decoder blocks\n","    decoder_blocks = [DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n","\n","    # Final complete model\n","    return tl.Serial(\n","        # feed output of previous step to current step\n","        tl.ShiftRight(n_positions=1,mode=mode), # Specify the mode!\n","        # Adding positional encoder\n","        positional_encoder,\n","        # Adding decoder blocks\n","        decoder_blocks,\n","        # Normalizing the layer\n","        tl.LayerNorm(),\n","\n","        # Adding a dense layer\n","        tl.Dense(vocab_size),\n","        # Geting probabilities with Logsoftmax\n","        tl.LogSoftmax()\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Uug2hN1oD1j","outputId":"56000da3-23f1-426d-e402-ea66089bd040"},"outputs":[{"output_type":"stream","name":"stdout","text":["Serial[\n","  Serial[\n","    ShiftRight(1)\n","  ]\n","  Embedding_33300_512\n","  Dropout\n","  PositionalEncoding\n","  Serial[\n","    Branch_out2[\n","      None\n","      Serial[\n","        LayerNorm\n","        Serial[\n","          Branch_out3[\n","            [Dense_512, AttnHeads]\n","            [Dense_512, AttnHeads]\n","            [Dense_512, AttnHeads]\n","          ]\n","          DotProductAttn_in3\n","          AttnOutput\n","          Dense_512\n","        ]\n","        Dropout\n","      ]\n","    ]\n","    Add_in2\n","  ]\n","  Serial[\n","    Branch_out2[\n","      None\n","      Serial[\n","        LayerNorm\n","        Dense_2048\n","        Serial[\n","          Relu\n","        ]\n","        Dropout\n","        Dense_512\n","        Dropout\n","      ]\n","    ]\n","    Add_in2\n","  ]\n","  LayerNorm\n","  Dense_33300\n","  LogSoftmax\n","]\n"]}],"source":["# Testing\n","print(TransformerLM(n_layers=1))  ## input [batch,T] , output [b,T,vocab_size]"]},{"cell_type":"markdown","metadata":{"id":"37mx3zq9bOyR"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"dRRKnoAdvmJ7"},"source":["<a name='3'></a>\n","# Part 3: Training\n","\n","Training the model.\n","Defining the cost function, the optimizer. In this case, we will train your model on a cpu for a few steps."]},{"cell_type":"markdown","metadata":{"id":"l1lkVebQRWRV"},"source":["<a name='3.1'></a>\n","### 3.1 Training Setup\n","\n","Function that takes in the model and trains it.\n","\n","<a name='ex05'></a>\n","\n","**Steps:** Implementing the `train_model` to train the neural network above.\n","\n","- Train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following:\n","    - <span style='color:blue'> labeled_data </span> = train_gen\n","    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n","    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n","    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n","\n","\n","- Evaluate task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following:\n","    - <span style='color:blue'> labeled_data </span> = eval_gen\n","    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n","\n","\n","- Training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following:\n","    - <span style='color:blue'> TransformerLM </span>\n","    - <span style='color:blue'> train_task </span>\n","    - <span style='color:blue'> eval_task </span> = [eval_task]\n","    - <span style='color:blue'> output_dir</span> = output_dir\n","\n","You will be using a cross entropy loss, with Adam optimizer. More information on can be found on [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding.\n","\n","The training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gM2gpu4xvjtX"},"outputs":[],"source":["from trax.supervised import training\n","\n","# train_model\n","def training_loop(Transformer_model, data_stream, test_gen, n_steps_per_checkpoint: int = 10, model_dir: str = \"model/summary_transformer_model\"):\n","  '''\n","  Input:\n","      TransformerLM (trax.layers.combinators.Serial): The model you are building.\n","      data_stream (generator): Training stream of data.\n","      test_gen (generator): Evaluation stream of data.\n","      n_steps_per_checkpoint (int): Number of steps per checkpoint\n","      model_dir (str): folder to save your file.\n","\n","  Returns:\n","      trax.supervised.training.Loop: Training loop.\n","  '''\n","  model_dir = os.path.expanduser(model_dir)  # trainer is an object\n","  lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n","\n","  # Train Task\n","  # labeled data from training generator\n","  # Loss function as CrossEntropyLoss\n","  # Optimizer as Adam with learning rate 0.01\n","  train_task = training.TrainTask(\n","    labeled_data=data_stream, # data from training generator\n","    loss_layer=tl.CrossEntropyLoss(), # Loss function\n","    optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n","    lr_schedule=lr_schedule, # Something you need\n","    n_steps_per_checkpoint= n_steps_per_checkpoint\n","  )\n","\n","  # Evaluate Task\n","  # The evaluation generator\n","  # CrossEntropyLoss and Accuracy\n","  test_task = training.EvalTask(labeled_data=test_gen, metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])\n","\n","  loop = training.Loop(Transformer_model, train_task, eval_tasks=[test_task], output_dir=model_dir)\n","\n","  return loop"]},{"cell_type":"markdown","metadata":{"id":"owVLjnwKoD1j"},"source":["The model will be trained for only 10 steps.\n","\n","Even with this constraint the model with the original default arguments took a very long time to finish. Due to this some parameters are changed when defining the model that is fed into the training loop in the function above."]},{"cell_type":"code","source":["# Create DIR\n","create_directory('/content/model')"],"metadata":{"id":"9fsnn2nH78NB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Downloading model\n","!wget 'https://docs.google.com/uc?export=download&id=1H8lMO5bd__2OIa3sEHXKxqJEi265B3J4&confirm=t' -O '/content/model/summary_transformer_model_v1.zip'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TF7c1ZAq7l91","executionInfo":{"status":"ok","timestamp":1683173563859,"user_tz":240,"elapsed":2419,"user":{"displayName":"Prabin Raj Shrestha","userId":"08128782292317104630"}},"outputId":"76c78ec7-3a9d-4b7f-c368-07a5b2d679d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-05-04 04:12:41--  https://docs.google.com/uc?export=download&id=1H8lMO5bd__2OIa3sEHXKxqJEi265B3J4&confirm=t\n","Resolving docs.google.com (docs.google.com)... 74.125.137.138, 74.125.137.139, 74.125.137.113, ...\n","Connecting to docs.google.com (docs.google.com)|74.125.137.138|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://doc-00-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8c3h5cctlngfkm957p2uugf7810re8vc/1683173550000/08128782292317104630/*/1H8lMO5bd__2OIa3sEHXKxqJEi265B3J4?e=download&uuid=14353ea1-8a65-43a9-ab50-aa701b78d721 [following]\n","Warning: wildcards not supported in HTTP.\n","--2023-05-04 04:12:43--  https://doc-00-40-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8c3h5cctlngfkm957p2uugf7810re8vc/1683173550000/08128782292317104630/*/1H8lMO5bd__2OIa3sEHXKxqJEi265B3J4?e=download&uuid=14353ea1-8a65-43a9-ab50-aa701b78d721\n","Resolving doc-00-40-docs.googleusercontent.com (doc-00-40-docs.googleusercontent.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n","Connecting to doc-00-40-docs.googleusercontent.com (doc-00-40-docs.googleusercontent.com)|142.250.101.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3128605 (3.0M) [application/zip]\n","Saving to: ‘/content/model/summary_transformer_model_v1.zip’\n","\n","/content/model/summ 100%[===================>]   2.98M  --.-KB/s    in 0.02s   \n","\n","2023-05-04 04:12:43 (159 MB/s) - ‘/content/model/summary_transformer_model_v1.zip’ saved [3128605/3128605]\n","\n"]}]},{"cell_type":"code","source":["# Model\n","# Model Training\n","\n","# Control\n","model_version_nm = 'summary_transformer_model_v1'\n","steps = 1000\n","create = False\n","n_steps_per_checkpoint = 100\n","\n","# Version Control\n","model_path = '/content/model'\n","\n","model_dir = os.path.join(model_path, model_version_nm)\n","\n","\n","\n","# create_directory(model_dir)\n","create_directory(model_dir)\n","\n","# Creating a fresh model\n","New_transformer = TransformerLM(vocab_size=2000, d_model=32, d_ff=256, n_layers=3, n_heads=8, dropout=0.1, max_len=2048, ff_activation = tl.Relu)\n","\n","# Training\n","loop = training_loop(New_transformer, train_batch_stream, test_batch_stream, n_steps_per_checkpoint = n_steps_per_checkpoint, model_dir = model_dir )\n","\n","if not(create):\n","  # unzip\n","  unzip_file(f'{model_dir}.zip')\n","  # Load\n","  loop.model.init_from_file(f'{model_dir}/model.pkl.gz', weights_only=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6Vgq6jNJ2O_","outputId":"4f3c8bf8-4d44-40ba-b5a3-5cfe901f7d1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py:658: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Loading trained model\n","\n","# Create or load\n","\n","\n","# unzip\n","unzip_file(f'{model_dir}.zip')\n","\n","# Load the pre-trained weights\n","New_transformer.init_from_file(f'{model_dir}/model.pkl.gz', weights_only=True)\n","model"],"metadata":{"id":"xryGXr9jCehC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a name='3.2'></a>\n","### 3.2 Training the Model"],"metadata":{"id":"e-7FlD7my3nr"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BAECQiTvR2Tk","outputId":"d9154524-7e0b-4afe-c880-4530b241f10e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Step   6020: Ran 10 train steps in 25.27 secs\n","Step   6020: train CrossEntropyLoss |  2.83537626\n","Step   6020: eval  CrossEntropyLoss |  3.16935253\n","Step   6020: eval          Accuracy |  0.07843138\n","\n","Step   6030: Ran 10 train steps in 23.18 secs\n","Step   6030: train CrossEntropyLoss |  2.94593930\n","Step   6030: eval  CrossEntropyLoss |  2.34274578\n","Step   6030: eval          Accuracy |  0.11111111\n","\n","Step   6040: Ran 10 train steps in 24.83 secs\n","Step   6040: train CrossEntropyLoss |  2.85485029\n","Step   6040: eval  CrossEntropyLoss |  2.66719866\n","Step   6040: eval          Accuracy |  0.13970588\n","\n","Step   6050: Ran 10 train steps in 22.41 secs\n","Step   6050: train CrossEntropyLoss |  3.00162959\n","Step   6050: eval  CrossEntropyLoss |  2.88957810\n","Step   6050: eval          Accuracy |  0.08682635\n","\n","Step   6060: Ran 10 train steps in 21.18 secs\n","Step   6060: train CrossEntropyLoss |  2.97003365\n","Step   6060: eval  CrossEntropyLoss |  2.67711949\n","Step   6060: eval          Accuracy |  0.12280702\n","\n","Step   6070: Ran 10 train steps in 22.52 secs\n","Step   6070: train CrossEntropyLoss |  2.75675797\n","Step   6070: eval  CrossEntropyLoss |  3.23956013\n","Step   6070: eval          Accuracy |  0.06363636\n","\n","Step   6080: Ran 10 train steps in 23.88 secs\n","Step   6080: train CrossEntropyLoss |  2.86102533\n","Step   6080: eval  CrossEntropyLoss |  3.78801107\n","Step   6080: eval          Accuracy |  0.11855670\n","\n","Step   6090: Ran 10 train steps in 21.79 secs\n","Step   6090: train CrossEntropyLoss |  2.68223071\n","Step   6090: eval  CrossEntropyLoss |  3.29314852\n","Step   6090: eval          Accuracy |  0.11111111\n","\n","Step   6100: Ran 10 train steps in 22.42 secs\n","Step   6100: train CrossEntropyLoss |  3.04598379\n","Step   6100: eval  CrossEntropyLoss |  2.75003910\n","Step   6100: eval          Accuracy |  0.13888890\n","\n","Step   6110: Ran 10 train steps in 24.29 secs\n","Step   6110: train CrossEntropyLoss |  2.77283692\n","Step   6110: eval  CrossEntropyLoss |  3.10561943\n","Step   6110: eval          Accuracy |  0.12173913\n","\n","Step   6120: Ran 10 train steps in 24.95 secs\n","Step   6120: train CrossEntropyLoss |  2.82228804\n","Step   6120: eval  CrossEntropyLoss |  2.66331482\n","Step   6120: eval          Accuracy |  0.10967742\n","\n","Step   6130: Ran 10 train steps in 23.76 secs\n","Step   6130: train CrossEntropyLoss |  2.98091125\n","Step   6130: eval  CrossEntropyLoss |  2.58654881\n","Step   6130: eval          Accuracy |  0.12500000\n","\n","Step   6140: Ran 10 train steps in 22.51 secs\n","Step   6140: train CrossEntropyLoss |  2.88658261\n","Step   6140: eval  CrossEntropyLoss |  2.52901888\n","Step   6140: eval          Accuracy |  0.09790210\n","\n","Step   6150: Ran 10 train steps in 23.69 secs\n","Step   6150: train CrossEntropyLoss |  2.87625360\n","Step   6150: eval  CrossEntropyLoss |  2.33220434\n","Step   6150: eval          Accuracy |  0.13414635\n","\n","Step   6160: Ran 10 train steps in 21.91 secs\n","Step   6160: train CrossEntropyLoss |  2.82961416\n","Step   6160: eval  CrossEntropyLoss |  2.73613858\n","Step   6160: eval          Accuracy |  0.11038961\n","\n","Step   6170: Ran 10 train steps in 23.73 secs\n","Step   6170: train CrossEntropyLoss |  2.85243583\n","Step   6170: eval  CrossEntropyLoss |  2.08563995\n","Step   6170: eval          Accuracy |  0.11827957\n","\n","Step   6180: Ran 10 train steps in 21.84 secs\n","Step   6180: train CrossEntropyLoss |  2.74071789\n","Step   6180: eval  CrossEntropyLoss |  3.37457037\n","Step   6180: eval          Accuracy |  0.10126583\n","\n","Step   6190: Ran 10 train steps in 23.02 secs\n","Step   6190: train CrossEntropyLoss |  2.70423627\n","Step   6190: eval  CrossEntropyLoss |  2.88904357\n","Step   6190: eval          Accuracy |  0.12871288\n","\n","Step   6200: Ran 10 train steps in 24.89 secs\n","Step   6200: train CrossEntropyLoss |  2.90432763\n","Step   6200: eval  CrossEntropyLoss |  3.03745365\n","Step   6200: eval          Accuracy |  0.12290503\n","\n","Step   6210: Ran 10 train steps in 24.15 secs\n","Step   6210: train CrossEntropyLoss |  2.87298274\n","Step   6210: eval  CrossEntropyLoss |  2.70586586\n","Step   6210: eval          Accuracy |  0.11764706\n","\n","Step   6220: Ran 10 train steps in 20.69 secs\n","Step   6220: train CrossEntropyLoss |  2.81054020\n","Step   6220: eval  CrossEntropyLoss |  3.09864998\n","Step   6220: eval          Accuracy |  0.08333334\n","\n","Step   6230: Ran 10 train steps in 24.91 secs\n","Step   6230: train CrossEntropyLoss |  2.90899801\n","Step   6230: eval  CrossEntropyLoss |  2.42061353\n","Step   6230: eval          Accuracy |  0.08783784\n","\n","Step   6240: Ran 10 train steps in 25.99 secs\n","Step   6240: train CrossEntropyLoss |  2.88179541\n","Step   6240: eval  CrossEntropyLoss |  2.85248113\n","Step   6240: eval          Accuracy |  0.09210526\n","\n","Step   6250: Ran 10 train steps in 20.20 secs\n","Step   6250: train CrossEntropyLoss |  2.87047791\n","Step   6250: eval  CrossEntropyLoss |  3.06198525\n","Step   6250: eval          Accuracy |  0.06857143\n","\n","Step   6260: Ran 10 train steps in 20.83 secs\n","Step   6260: train CrossEntropyLoss |  2.85826564\n","Step   6260: eval  CrossEntropyLoss |  3.26322341\n","Step   6260: eval          Accuracy |  0.10059172\n","\n","Step   6270: Ran 10 train steps in 23.96 secs\n","Step   6270: train CrossEntropyLoss |  2.85355139\n","Step   6270: eval  CrossEntropyLoss |  3.42017937\n","Step   6270: eval          Accuracy |  0.11904762\n","\n","Step   6280: Ran 10 train steps in 19.87 secs\n","Step   6280: train CrossEntropyLoss |  2.82908869\n","Step   6280: eval  CrossEntropyLoss |  2.78538609\n","Step   6280: eval          Accuracy |  0.08695652\n","\n","Step   6290: Ran 10 train steps in 25.52 secs\n","Step   6290: train CrossEntropyLoss |  2.86354160\n","Step   6290: eval  CrossEntropyLoss |  2.81179285\n","Step   6290: eval          Accuracy |  0.13043478\n","\n","Step   6300: Ran 10 train steps in 22.55 secs\n","Step   6300: train CrossEntropyLoss |  2.86320162\n","Step   6300: eval  CrossEntropyLoss |  2.54792905\n","Step   6300: eval          Accuracy |  0.08510638\n","\n","Step   6310: Ran 10 train steps in 24.87 secs\n","Step   6310: train CrossEntropyLoss |  2.80569124\n","Step   6310: eval  CrossEntropyLoss |  2.74305415\n","Step   6310: eval          Accuracy |  0.12440191\n","\n","Step   6320: Ran 10 train steps in 26.11 secs\n","Step   6320: train CrossEntropyLoss |  2.71846008\n","Step   6320: eval  CrossEntropyLoss |  2.86378384\n","Step   6320: eval          Accuracy |  0.08208955\n","\n","Step   6330: Ran 10 train steps in 23.74 secs\n","Step   6330: train CrossEntropyLoss |  2.77593946\n","Step   6330: eval  CrossEntropyLoss |  3.04006147\n","Step   6330: eval          Accuracy |  0.06285714\n","\n","Step   6340: Ran 10 train steps in 20.78 secs\n","Step   6340: train CrossEntropyLoss |  2.95902824\n","Step   6340: eval  CrossEntropyLoss |  3.32873940\n","Step   6340: eval          Accuracy |  0.12149533\n","\n","Step   6350: Ran 10 train steps in 23.68 secs\n","Step   6350: train CrossEntropyLoss |  2.71798682\n","Step   6350: eval  CrossEntropyLoss |  2.44186878\n","Step   6350: eval          Accuracy |  0.08620690\n","\n","Step   6360: Ran 10 train steps in 24.89 secs\n","Step   6360: train CrossEntropyLoss |  2.78552556\n","Step   6360: eval  CrossEntropyLoss |  2.86069226\n","Step   6360: eval          Accuracy |  0.07826087\n","\n","Step   6370: Ran 10 train steps in 21.95 secs\n","Step   6370: train CrossEntropyLoss |  3.01716471\n","Step   6370: eval  CrossEntropyLoss |  3.40416074\n","Step   6370: eval          Accuracy |  0.10526316\n","\n","Step   6380: Ran 10 train steps in 22.00 secs\n","Step   6380: train CrossEntropyLoss |  2.84256506\n","Step   6380: eval  CrossEntropyLoss |  2.71514940\n","Step   6380: eval          Accuracy |  0.08064516\n","\n","Step   6390: Ran 10 train steps in 24.93 secs\n","Step   6390: train CrossEntropyLoss |  2.88142920\n","Step   6390: eval  CrossEntropyLoss |  2.66327691\n","Step   6390: eval          Accuracy |  0.08333334\n","\n","Step   6400: Ran 10 train steps in 20.66 secs\n","Step   6400: train CrossEntropyLoss |  2.88517189\n","Step   6400: eval  CrossEntropyLoss |  2.89808345\n","Step   6400: eval          Accuracy |  0.07746479\n","\n","Step   6410: Ran 10 train steps in 21.89 secs\n","Step   6410: train CrossEntropyLoss |  2.67872334\n","Step   6410: eval  CrossEntropyLoss |  2.87345099\n","Step   6410: eval          Accuracy |  0.12000000\n","\n","Step   6420: Ran 10 train steps in 26.77 secs\n","Step   6420: train CrossEntropyLoss |  2.77215528\n","Step   6420: eval  CrossEntropyLoss |  2.63968396\n","Step   6420: eval          Accuracy |  0.09756097\n","\n","Step   6430: Ran 10 train steps in 23.84 secs\n","Step   6430: train CrossEntropyLoss |  2.81957722\n","Step   6430: eval  CrossEntropyLoss |  2.96206570\n","Step   6430: eval          Accuracy |  0.07482993\n","\n","Step   6440: Ran 10 train steps in 26.55 secs\n","Step   6440: train CrossEntropyLoss |  2.64745784\n","Step   6440: eval  CrossEntropyLoss |  2.74802518\n","Step   6440: eval          Accuracy |  0.07951070\n","\n","Step   6450: Ran 10 train steps in 22.67 secs\n","Step   6450: train CrossEntropyLoss |  2.67102718\n","Step   6450: eval  CrossEntropyLoss |  2.71369863\n","Step   6450: eval          Accuracy |  0.07692308\n","\n","Step   6460: Ran 10 train steps in 26.39 secs\n","Step   6460: train CrossEntropyLoss |  2.83324003\n","Step   6460: eval  CrossEntropyLoss |  2.73028231\n","Step   6460: eval          Accuracy |  0.09734514\n","\n","Step   6470: Ran 10 train steps in 23.74 secs\n","Step   6470: train CrossEntropyLoss |  2.75648975\n","Step   6470: eval  CrossEntropyLoss |  2.51805782\n","Step   6470: eval          Accuracy |  0.16883117\n","\n","Step   6480: Ran 10 train steps in 25.01 secs\n","Step   6480: train CrossEntropyLoss |  2.67970753\n","Step   6480: eval  CrossEntropyLoss |  3.13400030\n","Step   6480: eval          Accuracy |  0.07964602\n","\n","Step   6490: Ran 10 train steps in 26.06 secs\n","Step   6490: train CrossEntropyLoss |  2.89113021\n","Step   6490: eval  CrossEntropyLoss |  2.48317528\n","Step   6490: eval          Accuracy |  0.12121212\n","\n","Step   6500: Ran 10 train steps in 21.91 secs\n","Step   6500: train CrossEntropyLoss |  2.90765309\n","Step   6500: eval  CrossEntropyLoss |  2.58115292\n","Step   6500: eval          Accuracy |  0.11111111\n","\n","Step   6510: Ran 10 train steps in 22.68 secs\n","Step   6510: train CrossEntropyLoss |  2.97554827\n","Step   6510: eval  CrossEntropyLoss |  2.97625661\n","Step   6510: eval          Accuracy |  0.14457831\n","\n","Step   6520: Ran 10 train steps in 23.17 secs\n","Step   6520: train CrossEntropyLoss |  2.89123225\n","Step   6520: eval  CrossEntropyLoss |  2.66772866\n","Step   6520: eval          Accuracy |  0.13821138\n","\n","Step   6530: Ran 10 train steps in 23.70 secs\n","Step   6530: train CrossEntropyLoss |  2.67491603\n","Step   6530: eval  CrossEntropyLoss |  2.47878623\n","Step   6530: eval          Accuracy |  0.11764706\n","\n","Step   6540: Ran 10 train steps in 23.01 secs\n","Step   6540: train CrossEntropyLoss |  2.87533236\n","Step   6540: eval  CrossEntropyLoss |  3.02421165\n","Step   6540: eval          Accuracy |  0.11904762\n","\n","Step   6550: Ran 10 train steps in 23.72 secs\n","Step   6550: train CrossEntropyLoss |  2.76240492\n","Step   6550: eval  CrossEntropyLoss |  2.47021151\n","Step   6550: eval          Accuracy |  0.09600000\n","\n","Step   6560: Ran 10 train steps in 26.34 secs\n","Step   6560: train CrossEntropyLoss |  2.79590249\n","Step   6560: eval  CrossEntropyLoss |  3.05586123\n","Step   6560: eval          Accuracy |  0.11111111\n","\n","Step   6570: Ran 10 train steps in 26.34 secs\n","Step   6570: train CrossEntropyLoss |  2.80801773\n","Step   6570: eval  CrossEntropyLoss |  3.11752129\n","Step   6570: eval          Accuracy |  0.06896552\n","\n","Step   6580: Ran 10 train steps in 23.87 secs\n","Step   6580: train CrossEntropyLoss |  2.82186770\n","Step   6580: eval  CrossEntropyLoss |  3.20041513\n","Step   6580: eval          Accuracy |  0.06451613\n","\n","Step   6590: Ran 10 train steps in 22.57 secs\n","Step   6590: train CrossEntropyLoss |  2.78216600\n","Step   6590: eval  CrossEntropyLoss |  2.43120313\n","Step   6590: eval          Accuracy |  0.08474576\n","\n","Step   6600: Ran 10 train steps in 23.19 secs\n","Step   6600: train CrossEntropyLoss |  2.66459417\n","Step   6600: eval  CrossEntropyLoss |  3.52420402\n","Step   6600: eval          Accuracy |  0.13461539\n","\n","Step   6610: Ran 10 train steps in 17.67 secs\n","Step   6610: train CrossEntropyLoss |  2.76498866\n","Step   6610: eval  CrossEntropyLoss |  2.49792695\n","Step   6610: eval          Accuracy |  0.10362694\n","\n","Step   6620: Ran 10 train steps in 22.74 secs\n","Step   6620: train CrossEntropyLoss |  3.06799269\n","Step   6620: eval  CrossEntropyLoss |  2.78032088\n","Step   6620: eval          Accuracy |  0.05714286\n","\n","Step   6630: Ran 10 train steps in 23.97 secs\n","Step   6630: train CrossEntropyLoss |  2.87654161\n","Step   6630: eval  CrossEntropyLoss |  2.77502441\n","Step   6630: eval          Accuracy |  0.07432432\n","\n","Step   6640: Ran 10 train steps in 25.98 secs\n","Step   6640: train CrossEntropyLoss |  2.90226126\n","Step   6640: eval  CrossEntropyLoss |  2.25263309\n","Step   6640: eval          Accuracy |  0.05769231\n","\n","Step   6650: Ran 10 train steps in 23.14 secs\n","Step   6650: train CrossEntropyLoss |  2.74594164\n","Step   6650: eval  CrossEntropyLoss |  2.40579176\n","Step   6650: eval          Accuracy |  0.19047619\n","\n","Step   6660: Ran 10 train steps in 22.72 secs\n","Step   6660: train CrossEntropyLoss |  2.86136150\n","Step   6660: eval  CrossEntropyLoss |  2.70669889\n","Step   6660: eval          Accuracy |  0.07228915\n","\n","Step   6670: Ran 10 train steps in 26.23 secs\n","Step   6670: train CrossEntropyLoss |  2.77907348\n","Step   6670: eval  CrossEntropyLoss |  2.59166384\n","Step   6670: eval          Accuracy |  0.11428571\n","\n","Step   6680: Ran 10 train steps in 21.93 secs\n","Step   6680: train CrossEntropyLoss |  2.87720919\n","Step   6680: eval  CrossEntropyLoss |  2.47393084\n","Step   6680: eval          Accuracy |  0.13333334\n","\n","Step   6690: Ran 10 train steps in 22.61 secs\n","Step   6690: train CrossEntropyLoss |  2.74201155\n","Step   6690: eval  CrossEntropyLoss |  2.58386087\n","Step   6690: eval          Accuracy |  0.11585366\n","\n","Step   6700: Ran 10 train steps in 23.76 secs\n","Step   6700: train CrossEntropyLoss |  2.95089102\n","Step   6700: eval  CrossEntropyLoss |  2.78939772\n","Step   6700: eval          Accuracy |  0.12048193\n","\n","Step   6710: Ran 10 train steps in 23.27 secs\n","Step   6710: train CrossEntropyLoss |  2.70216489\n","Step   6710: eval  CrossEntropyLoss |  2.79532933\n","Step   6710: eval          Accuracy |  0.11392405\n","\n","Step   6720: Ran 10 train steps in 23.89 secs\n","Step   6720: train CrossEntropyLoss |  2.79913139\n","Step   6720: eval  CrossEntropyLoss |  2.32888317\n","Step   6720: eval          Accuracy |  0.08750000\n","\n","Step   6730: Ran 10 train steps in 23.14 secs\n","Step   6730: train CrossEntropyLoss |  2.93018484\n","Step   6730: eval  CrossEntropyLoss |  2.59512424\n","Step   6730: eval          Accuracy |  0.11842106\n","\n","Step   6740: Ran 10 train steps in 24.97 secs\n","Step   6740: train CrossEntropyLoss |  2.76179719\n","Step   6740: eval  CrossEntropyLoss |  2.78754520\n","Step   6740: eval          Accuracy |  0.12345679\n","\n","Step   6750: Ran 10 train steps in 21.97 secs\n","Step   6750: train CrossEntropyLoss |  2.71934748\n","Step   6750: eval  CrossEntropyLoss |  3.41545963\n","Step   6750: eval          Accuracy |  0.08695652\n","\n","Step   6760: Ran 10 train steps in 27.42 secs\n","Step   6760: train CrossEntropyLoss |  2.84230733\n","Step   6760: eval  CrossEntropyLoss |  3.02735639\n","Step   6760: eval          Accuracy |  0.14935064\n","\n","Step   6770: Ran 10 train steps in 18.64 secs\n","Step   6770: train CrossEntropyLoss |  2.87876272\n","Step   6770: eval  CrossEntropyLoss |  3.15004945\n","Step   6770: eval          Accuracy |  0.04237288\n","\n","Step   6780: Ran 10 train steps in 24.93 secs\n","Step   6780: train CrossEntropyLoss |  2.72805619\n","Step   6780: eval  CrossEntropyLoss |  3.27671337\n","Step   6780: eval          Accuracy |  0.11818182\n","\n","Step   6790: Ran 10 train steps in 21.94 secs\n","Step   6790: train CrossEntropyLoss |  2.69417882\n","Step   6790: eval  CrossEntropyLoss |  2.80591011\n","Step   6790: eval          Accuracy |  0.07936508\n","\n","Step   6800: Ran 10 train steps in 24.95 secs\n","Step   6800: train CrossEntropyLoss |  2.67073226\n","Step   6800: eval  CrossEntropyLoss |  2.61711097\n","Step   6800: eval          Accuracy |  0.06557377\n","\n","Step   6810: Ran 10 train steps in 26.30 secs\n","Step   6810: train CrossEntropyLoss |  2.91187143\n","Step   6810: eval  CrossEntropyLoss |  2.86433363\n","Step   6810: eval          Accuracy |  0.08127209\n","\n","Step   6820: Ran 10 train steps in 25.45 secs\n","Step   6820: train CrossEntropyLoss |  2.82066369\n","Step   6820: eval  CrossEntropyLoss |  2.60399222\n","Step   6820: eval          Accuracy |  0.10000000\n","\n","Step   6830: Ran 10 train steps in 26.20 secs\n","Step   6830: train CrossEntropyLoss |  2.81467462\n","Step   6830: eval  CrossEntropyLoss |  2.82589722\n","Step   6830: eval          Accuracy |  0.09333333\n","\n","Step   6840: Ran 10 train steps in 24.87 secs\n","Step   6840: train CrossEntropyLoss |  2.87409973\n","Step   6840: eval  CrossEntropyLoss |  2.52387810\n","Step   6840: eval          Accuracy |  0.11538462\n","\n","Step   6850: Ran 10 train steps in 28.36 secs\n","Step   6850: train CrossEntropyLoss |  2.62178540\n","Step   6850: eval  CrossEntropyLoss |  3.01013327\n","Step   6850: eval          Accuracy |  0.11764706\n","\n","Step   6860: Ran 10 train steps in 21.66 secs\n","Step   6860: train CrossEntropyLoss |  2.70043755\n","Step   6860: eval  CrossEntropyLoss |  3.00109911\n","Step   6860: eval          Accuracy |  0.10679612\n","\n","Step   6870: Ran 10 train steps in 21.91 secs\n","Step   6870: train CrossEntropyLoss |  2.87945795\n","Step   6870: eval  CrossEntropyLoss |  3.13412333\n","Step   6870: eval          Accuracy |  0.12295082\n","\n","Step   6880: Ran 10 train steps in 21.97 secs\n","Step   6880: train CrossEntropyLoss |  2.82811475\n","Step   6880: eval  CrossEntropyLoss |  2.65631580\n","Step   6880: eval          Accuracy |  0.10382514\n","\n","Step   6890: Ran 10 train steps in 20.44 secs\n","Step   6890: train CrossEntropyLoss |  2.91410184\n","Step   6890: eval  CrossEntropyLoss |  2.62365699\n","Step   6890: eval          Accuracy |  0.10000000\n","\n","Step   6900: Ran 10 train steps in 23.93 secs\n","Step   6900: train CrossEntropyLoss |  2.73327684\n","Step   6900: eval  CrossEntropyLoss |  2.79023838\n","Step   6900: eval          Accuracy |  0.12121212\n","\n","Step   6910: Ran 10 train steps in 22.65 secs\n","Step   6910: train CrossEntropyLoss |  2.84299517\n","Step   6910: eval  CrossEntropyLoss |  2.70600629\n","Step   6910: eval          Accuracy |  0.06896552\n","\n","Step   6920: Ran 10 train steps in 23.24 secs\n","Step   6920: train CrossEntropyLoss |  2.76926756\n","Step   6920: eval  CrossEntropyLoss |  3.18540120\n","Step   6920: eval          Accuracy |  0.10869565\n","\n","Step   6930: Ran 10 train steps in 26.55 secs\n","Step   6930: train CrossEntropyLoss |  3.10071611\n","Step   6930: eval  CrossEntropyLoss |  2.90565920\n","Step   6930: eval          Accuracy |  0.10000000\n","\n","Step   6940: Ran 10 train steps in 20.57 secs\n","Step   6940: train CrossEntropyLoss |  2.87906814\n","Step   6940: eval  CrossEntropyLoss |  3.07273674\n","Step   6940: eval          Accuracy |  0.13559322\n","\n","Step   6950: Ran 10 train steps in 23.85 secs\n","Step   6950: train CrossEntropyLoss |  2.89039087\n","Step   6950: eval  CrossEntropyLoss |  3.08092737\n","Step   6950: eval          Accuracy |  0.10810811\n","\n","Step   6960: Ran 10 train steps in 19.69 secs\n","Step   6960: train CrossEntropyLoss |  2.74489737\n","Step   6960: eval  CrossEntropyLoss |  3.02582788\n","Step   6960: eval          Accuracy |  0.05978261\n","\n","Step   6970: Ran 10 train steps in 22.66 secs\n","Step   6970: train CrossEntropyLoss |  2.84390736\n","Step   6970: eval  CrossEntropyLoss |  2.26365781\n","Step   6970: eval          Accuracy |  0.10344828\n","\n","Step   6980: Ran 10 train steps in 25.03 secs\n","Step   6980: train CrossEntropyLoss |  2.80956435\n","Step   6980: eval  CrossEntropyLoss |  2.57870293\n","Step   6980: eval          Accuracy |  0.07926829\n","\n","Step   6990: Ran 10 train steps in 25.05 secs\n","Step   6990: train CrossEntropyLoss |  2.73278403\n","Step   6990: eval  CrossEntropyLoss |  2.59357858\n","Step   6990: eval          Accuracy |  0.06818182\n","\n","Step   7000: Ran 10 train steps in 22.74 secs\n","Step   7000: train CrossEntropyLoss |  2.91960096\n","Step   7000: eval  CrossEntropyLoss |  2.72460532\n","Step   7000: eval          Accuracy |  0.08333334\n","\n","Step   7010: Ran 10 train steps in 21.08 secs\n","Step   7010: train CrossEntropyLoss |  2.82917356\n","Step   7010: eval  CrossEntropyLoss |  2.90409541\n","Step   7010: eval          Accuracy |  0.07079646\n","\n","Step   7020: Ran 10 train steps in 24.96 secs\n","Step   7020: train CrossEntropyLoss |  2.85363388\n","Step   7020: eval  CrossEntropyLoss |  2.86678481\n","Step   7020: eval          Accuracy |  0.10989011\n","\n","Step   7030: Ran 10 train steps in 26.88 secs\n","Step   7030: train CrossEntropyLoss |  2.83927727\n","Step   7030: eval  CrossEntropyLoss |  2.22893572\n","Step   7030: eval          Accuracy |  0.10000000\n","\n","Step   7040: Ran 10 train steps in 21.43 secs\n","Step   7040: train CrossEntropyLoss |  2.87618852\n","Step   7040: eval  CrossEntropyLoss |  2.57628393\n","Step   7040: eval          Accuracy |  0.09090909\n","\n","Step   7050: Ran 10 train steps in 24.99 secs\n","Step   7050: train CrossEntropyLoss |  2.76592612\n","Step   7050: eval  CrossEntropyLoss |  3.54450202\n","Step   7050: eval          Accuracy |  0.07608695\n","\n","Step   7060: Ran 10 train steps in 22.82 secs\n","Step   7060: train CrossEntropyLoss |  2.79256797\n","Step   7060: eval  CrossEntropyLoss |  2.54297471\n","Step   7060: eval          Accuracy |  0.07339449\n","\n","Step   7070: Ran 10 train steps in 22.62 secs\n","Step   7070: train CrossEntropyLoss |  2.74225283\n","Step   7070: eval  CrossEntropyLoss |  2.81862617\n","Step   7070: eval          Accuracy |  0.08860759\n","\n","Step   7080: Ran 10 train steps in 25.95 secs\n","Step   7080: train CrossEntropyLoss |  2.89085531\n","Step   7080: eval  CrossEntropyLoss |  3.18810129\n","Step   7080: eval          Accuracy |  0.13333334\n","\n","Step   7090: Ran 10 train steps in 20.74 secs\n","Step   7090: train CrossEntropyLoss |  2.67105651\n","Step   7090: eval  CrossEntropyLoss |  2.86547327\n","Step   7090: eval          Accuracy |  0.12571429\n","\n","Step   7100: Ran 10 train steps in 25.67 secs\n","Step   7100: train CrossEntropyLoss |  2.77316093\n","Step   7100: eval  CrossEntropyLoss |  2.68800879\n","Step   7100: eval          Accuracy |  0.06015038\n","\n","Step   7110: Ran 10 train steps in 22.77 secs\n","Step   7110: train CrossEntropyLoss |  2.84877157\n","Step   7110: eval  CrossEntropyLoss |  2.75151896\n","Step   7110: eval          Accuracy |  0.09195402\n","\n","Step   7120: Ran 10 train steps in 25.06 secs\n","Step   7120: train CrossEntropyLoss |  2.61412573\n","Step   7120: eval  CrossEntropyLoss |  2.73202920\n","Step   7120: eval          Accuracy |  0.12352941\n","\n","Step   7130: Ran 10 train steps in 23.82 secs\n","Step   7130: train CrossEntropyLoss |  2.78226972\n","Step   7130: eval  CrossEntropyLoss |  2.88494635\n","Step   7130: eval          Accuracy |  0.08387097\n","\n","Step   7140: Ran 10 train steps in 23.83 secs\n","Step   7140: train CrossEntropyLoss |  2.68409491\n","Step   7140: eval  CrossEntropyLoss |  3.33081841\n","Step   7140: eval          Accuracy |  0.13559322\n","\n","Step   7150: Ran 10 train steps in 24.38 secs\n","Step   7150: train CrossEntropyLoss |  2.90510631\n","Step   7150: eval  CrossEntropyLoss |  3.13600254\n","Step   7150: eval          Accuracy |  0.13157895\n","\n","Step   7160: Ran 10 train steps in 20.80 secs\n","Step   7160: train CrossEntropyLoss |  2.99320841\n","Step   7160: eval  CrossEntropyLoss |  2.93884826\n","Step   7160: eval          Accuracy |  0.11904762\n","\n","Step   7170: Ran 10 train steps in 23.92 secs\n","Step   7170: train CrossEntropyLoss |  2.75705934\n","Step   7170: eval  CrossEntropyLoss |  3.06452155\n","Step   7170: eval          Accuracy |  0.10569106\n","\n","Step   7180: Ran 10 train steps in 26.56 secs\n","Step   7180: train CrossEntropyLoss |  3.01157451\n","Step   7180: eval  CrossEntropyLoss |  3.00695014\n","Step   7180: eval          Accuracy |  0.09782609\n","\n","Step   7190: Ran 10 train steps in 21.19 secs\n","Step   7190: train CrossEntropyLoss |  2.66734529\n","Step   7190: eval  CrossEntropyLoss |  2.73175836\n","Step   7190: eval          Accuracy |  0.07758620\n","\n","Step   7200: Ran 10 train steps in 23.23 secs\n","Step   7200: train CrossEntropyLoss |  2.92318082\n","Step   7200: eval  CrossEntropyLoss |  2.58414483\n","Step   7200: eval          Accuracy |  0.10328639\n","\n","Step   7210: Ran 10 train steps in 22.03 secs\n","Step   7210: train CrossEntropyLoss |  2.97537708\n","Step   7210: eval  CrossEntropyLoss |  2.62540650\n","Step   7210: eval          Accuracy |  0.08080808\n","\n","Step   7220: Ran 10 train steps in 23.94 secs\n","Step   7220: train CrossEntropyLoss |  2.84440470\n","Step   7220: eval  CrossEntropyLoss |  3.05136228\n","Step   7220: eval          Accuracy |  0.11042945\n","\n","Step   7230: Ran 10 train steps in 22.03 secs\n","Step   7230: train CrossEntropyLoss |  2.84661746\n","Step   7230: eval  CrossEntropyLoss |  2.74534702\n","Step   7230: eval          Accuracy |  0.14606741\n","\n","Step   7240: Ran 10 train steps in 18.49 secs\n","Step   7240: train CrossEntropyLoss |  2.99773097\n","Step   7240: eval  CrossEntropyLoss |  3.15282226\n","Step   7240: eval          Accuracy |  0.08823530\n","\n","Step   7250: Ran 10 train steps in 22.71 secs\n","Step   7250: train CrossEntropyLoss |  2.88613939\n","Step   7250: eval  CrossEntropyLoss |  2.79172397\n","Step   7250: eval          Accuracy |  0.10362694\n","\n","Step   7260: Ran 10 train steps in 23.34 secs\n","Step   7260: train CrossEntropyLoss |  2.76691914\n","Step   7260: eval  CrossEntropyLoss |  2.90611243\n","Step   7260: eval          Accuracy |  0.10309278\n","\n","Step   7270: Ran 10 train steps in 24.37 secs\n","Step   7270: train CrossEntropyLoss |  2.86969972\n","Step   7270: eval  CrossEntropyLoss |  2.69601488\n","Step   7270: eval          Accuracy |  0.10909091\n","\n","Step   7280: Ran 10 train steps in 21.59 secs\n","Step   7280: train CrossEntropyLoss |  2.63350153\n","Step   7280: eval  CrossEntropyLoss |  2.27681947\n","Step   7280: eval          Accuracy |  0.12658228\n","\n","Step   7290: Ran 10 train steps in 22.10 secs\n","Step   7290: train CrossEntropyLoss |  2.70749998\n","Step   7290: eval  CrossEntropyLoss |  2.83679152\n","Step   7290: eval          Accuracy |  0.09489051\n","\n","Step   7300: Ran 10 train steps in 25.22 secs\n","Step   7300: train CrossEntropyLoss |  2.85626721\n","Step   7300: eval  CrossEntropyLoss |  2.76585698\n","Step   7300: eval          Accuracy |  0.07547170\n","\n","Step   7310: Ran 10 train steps in 25.91 secs\n","Step   7310: train CrossEntropyLoss |  2.91974568\n","Step   7310: eval  CrossEntropyLoss |  2.55899596\n","Step   7310: eval          Accuracy |  0.14062500\n","\n","Step   7320: Ran 10 train steps in 23.28 secs\n","Step   7320: train CrossEntropyLoss |  2.65062761\n","Step   7320: eval  CrossEntropyLoss |  2.79629469\n","Step   7320: eval          Accuracy |  0.11320755\n","\n","Step   7330: Ran 10 train steps in 25.08 secs\n","Step   7330: train CrossEntropyLoss |  2.86826468\n","Step   7330: eval  CrossEntropyLoss |  2.96670365\n","Step   7330: eval          Accuracy |  0.12053572\n","\n","Step   7340: Ran 10 train steps in 23.94 secs\n","Step   7340: train CrossEntropyLoss |  2.63636184\n","Step   7340: eval  CrossEntropyLoss |  2.87033653\n","Step   7340: eval          Accuracy |  0.15306123\n","\n","Step   7350: Ran 10 train steps in 22.44 secs\n","Step   7350: train CrossEntropyLoss |  2.86435556\n","Step   7350: eval  CrossEntropyLoss |  3.47146916\n","Step   7350: eval          Accuracy |  0.07865169\n","\n","Step   7360: Ran 10 train steps in 22.93 secs\n","Step   7360: train CrossEntropyLoss |  2.92099929\n","Step   7360: eval  CrossEntropyLoss |  2.58764315\n","Step   7360: eval          Accuracy |  0.11764706\n","\n","Step   7370: Ran 10 train steps in 21.44 secs\n","Step   7370: train CrossEntropyLoss |  2.90336370\n","Step   7370: eval  CrossEntropyLoss |  3.33014464\n","Step   7370: eval          Accuracy |  0.10967742\n","\n","Step   7380: Ran 10 train steps in 25.04 secs\n","Step   7380: train CrossEntropyLoss |  2.70230770\n","Step   7380: eval  CrossEntropyLoss |  3.03409076\n","Step   7380: eval          Accuracy |  0.09333333\n","\n","Step   7390: Ran 10 train steps in 19.11 secs\n","Step   7390: train CrossEntropyLoss |  2.81306291\n","Step   7390: eval  CrossEntropyLoss |  2.78738499\n","Step   7390: eval          Accuracy |  0.10000000\n","\n","Step   7400: Ran 10 train steps in 25.10 secs\n","Step   7400: train CrossEntropyLoss |  2.97593808\n","Step   7400: eval  CrossEntropyLoss |  2.67971706\n","Step   7400: eval          Accuracy |  0.08759124\n","\n","Step   7410: Ran 10 train steps in 24.02 secs\n","Step   7410: train CrossEntropyLoss |  2.75829506\n","Step   7410: eval  CrossEntropyLoss |  2.42515039\n","Step   7410: eval          Accuracy |  0.05970149\n","\n","Step   7420: Ran 10 train steps in 23.91 secs\n","Step   7420: train CrossEntropyLoss |  2.93026590\n","Step   7420: eval  CrossEntropyLoss |  2.31948662\n","Step   7420: eval          Accuracy |  0.15000001\n","\n","Step   7430: Ran 10 train steps in 20.22 secs\n","Step   7430: train CrossEntropyLoss |  2.79034853\n","Step   7430: eval  CrossEntropyLoss |  2.45896864\n","Step   7430: eval          Accuracy |  0.04477612\n","\n","Step   7440: Ran 10 train steps in 23.90 secs\n","Step   7440: train CrossEntropyLoss |  2.96607542\n","Step   7440: eval  CrossEntropyLoss |  2.78140712\n","Step   7440: eval          Accuracy |  0.12328767\n","\n","Step   7450: Ran 10 train steps in 22.08 secs\n","Step   7450: train CrossEntropyLoss |  3.01000166\n","Step   7450: eval  CrossEntropyLoss |  3.03393006\n","Step   7450: eval          Accuracy |  0.13934426\n","\n","Step   7460: Ran 10 train steps in 23.98 secs\n","Step   7460: train CrossEntropyLoss |  2.85489798\n","Step   7460: eval  CrossEntropyLoss |  2.60311675\n","Step   7460: eval          Accuracy |  0.08571429\n","\n","Step   7470: Ran 10 train steps in 23.05 secs\n","Step   7470: train CrossEntropyLoss |  2.92574263\n","Step   7470: eval  CrossEntropyLoss |  3.09044886\n","Step   7470: eval          Accuracy |  0.08620690\n","\n","Step   7480: Ran 10 train steps in 20.60 secs\n","Step   7480: train CrossEntropyLoss |  2.86523938\n","Step   7480: eval  CrossEntropyLoss |  2.72307158\n","Step   7480: eval          Accuracy |  0.10638298\n","\n","Step   7490: Ran 10 train steps in 23.25 secs\n","Step   7490: train CrossEntropyLoss |  2.73909068\n","Step   7490: eval  CrossEntropyLoss |  2.75656605\n","Step   7490: eval          Accuracy |  0.11578947\n","\n","Step   7500: Ran 10 train steps in 21.02 secs\n","Step   7500: train CrossEntropyLoss |  2.77277422\n","Step   7500: eval  CrossEntropyLoss |  2.85140967\n","Step   7500: eval          Accuracy |  0.09961686\n","\n","Step   7510: Ran 10 train steps in 24.03 secs\n","Step   7510: train CrossEntropyLoss |  2.87165594\n","Step   7510: eval  CrossEntropyLoss |  2.61261725\n","Step   7510: eval          Accuracy |  0.09316770\n","\n","Step   7520: Ran 10 train steps in 24.58 secs\n","Step   7520: train CrossEntropyLoss |  2.74542594\n","Step   7520: eval  CrossEntropyLoss |  2.60721850\n","Step   7520: eval          Accuracy |  0.08333334\n","\n","Step   7530: Ran 10 train steps in 21.48 secs\n","Step   7530: train CrossEntropyLoss |  3.01155353\n","Step   7530: eval  CrossEntropyLoss |  2.40833044\n","Step   7530: eval          Accuracy |  0.05660377\n","\n","Step   7540: Ran 10 train steps in 24.32 secs\n","Step   7540: train CrossEntropyLoss |  2.71601844\n","Step   7540: eval  CrossEntropyLoss |  2.83530569\n","Step   7540: eval          Accuracy |  0.10489511\n","\n","Step   7550: Ran 10 train steps in 21.03 secs\n","Step   7550: train CrossEntropyLoss |  2.88413668\n","Step   7550: eval  CrossEntropyLoss |  2.99211788\n","Step   7550: eval          Accuracy |  0.11764706\n","\n","Step   7560: Ran 10 train steps in 25.24 secs\n","Step   7560: train CrossEntropyLoss |  2.90882635\n","Step   7560: eval  CrossEntropyLoss |  2.49379992\n","Step   7560: eval          Accuracy |  0.11351351\n","\n","Step   7570: Ran 10 train steps in 20.80 secs\n","Step   7570: train CrossEntropyLoss |  2.92865038\n","Step   7570: eval  CrossEntropyLoss |  1.77671409\n","Step   7570: eval          Accuracy |  0.09375000\n","\n","Step   7580: Ran 10 train steps in 25.05 secs\n","Step   7580: train CrossEntropyLoss |  2.87342668\n","Step   7580: eval  CrossEntropyLoss |  2.79767466\n","Step   7580: eval          Accuracy |  0.11330049\n","\n","Step   7590: Ran 10 train steps in 25.42 secs\n","Step   7590: train CrossEntropyLoss |  2.68752742\n","Step   7590: eval  CrossEntropyLoss |  2.87919283\n","Step   7590: eval          Accuracy |  0.06666667\n","\n","Step   7600: Ran 10 train steps in 25.06 secs\n","Step   7600: train CrossEntropyLoss |  2.69217563\n","Step   7600: eval  CrossEntropyLoss |  2.77477217\n","Step   7600: eval          Accuracy |  0.10218978\n","\n","Step   7610: Ran 10 train steps in 23.31 secs\n","Step   7610: train CrossEntropyLoss |  2.97195196\n","Step   7610: eval  CrossEntropyLoss |  2.93868470\n","Step   7610: eval          Accuracy |  0.05633803\n","\n","Step   7620: Ran 10 train steps in 22.79 secs\n","Step   7620: train CrossEntropyLoss |  2.81083298\n","Step   7620: eval  CrossEntropyLoss |  3.15624928\n","Step   7620: eval          Accuracy |  0.04054054\n","\n","Step   7630: Ran 10 train steps in 22.08 secs\n","Step   7630: train CrossEntropyLoss |  2.85957861\n","Step   7630: eval  CrossEntropyLoss |  2.96210265\n","Step   7630: eval          Accuracy |  0.08035714\n","\n","Step   7640: Ran 10 train steps in 23.95 secs\n","Step   7640: train CrossEntropyLoss |  2.78088903\n","Step   7640: eval  CrossEntropyLoss |  2.84527421\n","Step   7640: eval          Accuracy |  0.13253012\n","\n","Step   7650: Ran 10 train steps in 24.04 secs\n","Step   7650: train CrossEntropyLoss |  3.06382036\n","Step   7650: eval  CrossEntropyLoss |  2.80667448\n","Step   7650: eval          Accuracy |  0.16666667\n","\n","Step   7660: Ran 10 train steps in 23.32 secs\n","Step   7660: train CrossEntropyLoss |  2.81202054\n","Step   7660: eval  CrossEntropyLoss |  2.45962691\n","Step   7660: eval          Accuracy |  0.12820514\n","\n","Step   7670: Ran 10 train steps in 24.02 secs\n","Step   7670: train CrossEntropyLoss |  2.80259037\n","Step   7670: eval  CrossEntropyLoss |  1.87128401\n","Step   7670: eval          Accuracy |  0.10389610\n","\n","Step   7680: Ran 10 train steps in 24.29 secs\n","Step   7680: train CrossEntropyLoss |  2.83516002\n","Step   7680: eval  CrossEntropyLoss |  3.20416641\n","Step   7680: eval          Accuracy |  0.10975610\n","\n","Step   7690: Ran 10 train steps in 25.09 secs\n","Step   7690: train CrossEntropyLoss |  3.12190580\n","Step   7690: eval  CrossEntropyLoss |  3.03087759\n","Step   7690: eval          Accuracy |  0.09574468\n","\n","Step   7700: Ran 10 train steps in 24.67 secs\n","Step   7700: train CrossEntropyLoss |  2.88053942\n","Step   7700: eval  CrossEntropyLoss |  3.24962330\n","Step   7700: eval          Accuracy |  0.11940298\n","\n","Step   7710: Ran 10 train steps in 25.81 secs\n","Step   7710: train CrossEntropyLoss |  2.86533737\n","Step   7710: eval  CrossEntropyLoss |  2.82008171\n","Step   7710: eval          Accuracy |  0.10377359\n","\n","Step   7720: Ran 10 train steps in 22.11 secs\n","Step   7720: train CrossEntropyLoss |  2.60394621\n","Step   7720: eval  CrossEntropyLoss |  3.06115031\n","Step   7720: eval          Accuracy |  0.12686567\n","\n","Step   7730: Ran 10 train steps in 24.97 secs\n","Step   7730: train CrossEntropyLoss |  2.86991930\n","Step   7730: eval  CrossEntropyLoss |  3.10288715\n","Step   7730: eval          Accuracy |  0.08196721\n","\n","Step   7740: Ran 10 train steps in 21.95 secs\n","Step   7740: train CrossEntropyLoss |  2.92715621\n","Step   7740: eval  CrossEntropyLoss |  3.24202824\n","Step   7740: eval          Accuracy |  0.07936508\n","\n","Step   7750: Ran 10 train steps in 23.87 secs\n","Step   7750: train CrossEntropyLoss |  2.78785849\n","Step   7750: eval  CrossEntropyLoss |  2.83868408\n","Step   7750: eval          Accuracy |  0.10569106\n","\n","Step   7760: Ran 10 train steps in 22.80 secs\n","Step   7760: train CrossEntropyLoss |  2.87217593\n","Step   7760: eval  CrossEntropyLoss |  3.03767300\n","Step   7760: eval          Accuracy |  0.17931035\n","\n","Step   7770: Ran 10 train steps in 22.06 secs\n","Step   7770: train CrossEntropyLoss |  2.87841296\n","Step   7770: eval  CrossEntropyLoss |  3.05771923\n","Step   7770: eval          Accuracy |  0.07954545\n","\n","Step   7780: Ran 10 train steps in 26.14 secs\n","Step   7780: train CrossEntropyLoss |  2.97698855\n","Step   7780: eval  CrossEntropyLoss |  3.27193952\n","Step   7780: eval          Accuracy |  0.12598425\n","\n","Step   7790: Ran 10 train steps in 21.89 secs\n","Step   7790: train CrossEntropyLoss |  2.79109859\n","Step   7790: eval  CrossEntropyLoss |  3.20396709\n","Step   7790: eval          Accuracy |  0.10294118\n","\n","Step   7800: Ran 10 train steps in 25.07 secs\n","Step   7800: train CrossEntropyLoss |  2.98123550\n","Step   7800: eval  CrossEntropyLoss |  2.34250116\n","Step   7800: eval          Accuracy |  0.09677419\n","\n","Step   7810: Ran 10 train steps in 23.03 secs\n","Step   7810: train CrossEntropyLoss |  2.78972435\n","Step   7810: eval  CrossEntropyLoss |  2.88453722\n","Step   7810: eval          Accuracy |  0.05000000\n","\n","Step   7820: Ran 10 train steps in 20.31 secs\n","Step   7820: train CrossEntropyLoss |  2.81118226\n","Step   7820: eval  CrossEntropyLoss |  2.83064675\n","Step   7820: eval          Accuracy |  0.11111111\n","\n","Step   7830: Ran 10 train steps in 26.18 secs\n","Step   7830: train CrossEntropyLoss |  2.76130271\n","Step   7830: eval  CrossEntropyLoss |  3.61791682\n","Step   7830: eval          Accuracy |  0.07936508\n","\n","Step   7840: Ran 10 train steps in 23.09 secs\n","Step   7840: train CrossEntropyLoss |  2.82646561\n","Step   7840: eval  CrossEntropyLoss |  3.01376057\n","Step   7840: eval          Accuracy |  0.10059172\n","\n","Step   7850: Ran 10 train steps in 25.92 secs\n","Step   7850: train CrossEntropyLoss |  2.84133816\n","Step   7850: eval  CrossEntropyLoss |  3.05230856\n","Step   7850: eval          Accuracy |  0.09756097\n","\n","Step   7860: Ran 10 train steps in 25.00 secs\n","Step   7860: train CrossEntropyLoss |  2.74475098\n","Step   7860: eval  CrossEntropyLoss |  3.38017893\n","Step   7860: eval          Accuracy |  0.12903225\n","\n","Step   7870: Ran 10 train steps in 23.09 secs\n","Step   7870: train CrossEntropyLoss |  2.83257508\n","Step   7870: eval  CrossEntropyLoss |  3.75079179\n","Step   7870: eval          Accuracy |  0.16666667\n","\n","Step   7880: Ran 10 train steps in 24.27 secs\n","Step   7880: train CrossEntropyLoss |  2.88955688\n","Step   7880: eval  CrossEntropyLoss |  3.17462015\n","Step   7880: eval          Accuracy |  0.06870229\n","\n","Step   7890: Ran 10 train steps in 24.98 secs\n","Step   7890: train CrossEntropyLoss |  2.89457560\n","Step   7890: eval  CrossEntropyLoss |  2.91202474\n","Step   7890: eval          Accuracy |  0.05263158\n","\n","Step   7900: Ran 10 train steps in 20.86 secs\n","Step   7900: train CrossEntropyLoss |  2.89457059\n","Step   7900: eval  CrossEntropyLoss |  2.85725260\n","Step   7900: eval          Accuracy |  0.08000000\n","\n","Step   7910: Ran 10 train steps in 23.79 secs\n","Step   7910: train CrossEntropyLoss |  2.71219778\n","Step   7910: eval  CrossEntropyLoss |  2.91182756\n","Step   7910: eval          Accuracy |  0.06666667\n","\n","Step   7920: Ran 10 train steps in 21.56 secs\n","Step   7920: train CrossEntropyLoss |  2.96048546\n","Step   7920: eval  CrossEntropyLoss |  2.73172283\n","Step   7920: eval          Accuracy |  0.08130081\n","\n","Step   7930: Ran 10 train steps in 24.45 secs\n","Step   7930: train CrossEntropyLoss |  2.74955750\n","Step   7930: eval  CrossEntropyLoss |  2.51982093\n","Step   7930: eval          Accuracy |  0.09615385\n","\n","Step   7940: Ran 10 train steps in 22.18 secs\n","Step   7940: train CrossEntropyLoss |  2.77964258\n","Step   7940: eval  CrossEntropyLoss |  3.04499435\n","Step   7940: eval          Accuracy |  0.05084746\n","\n","Step   7950: Ran 10 train steps in 22.81 secs\n","Step   7950: train CrossEntropyLoss |  3.02889013\n","Step   7950: eval  CrossEntropyLoss |  3.60672235\n","Step   7950: eval          Accuracy |  0.06349207\n","\n","Step   7960: Ran 10 train steps in 24.44 secs\n","Step   7960: train CrossEntropyLoss |  2.67273188\n","Step   7960: eval  CrossEntropyLoss |  2.96940064\n","Step   7960: eval          Accuracy |  0.07317073\n","\n","Step   7970: Ran 10 train steps in 25.04 secs\n","Step   7970: train CrossEntropyLoss |  2.89053798\n","Step   7970: eval  CrossEntropyLoss |  2.52092957\n","Step   7970: eval          Accuracy |  0.10588235\n","\n","Step   7980: Ran 10 train steps in 25.34 secs\n","Step   7980: train CrossEntropyLoss |  2.69494128\n","Step   7980: eval  CrossEntropyLoss |  2.98106885\n","Step   7980: eval          Accuracy |  0.10218978\n","\n","Step   7990: Ran 10 train steps in 21.62 secs\n","Step   7990: train CrossEntropyLoss |  2.68214011\n","Step   7990: eval  CrossEntropyLoss |  2.34336233\n","Step   7990: eval          Accuracy |  0.07500000\n","\n","Step   8000: Ran 10 train steps in 22.25 secs\n","Step   8000: train CrossEntropyLoss |  2.83946276\n","Step   8000: eval  CrossEntropyLoss |  3.10132146\n","Step   8000: eval          Accuracy |  0.10526316\n","\n","Step   8010: Ran 10 train steps in 23.23 secs\n","Step   8010: train CrossEntropyLoss |  2.79397488\n","Step   8010: eval  CrossEntropyLoss |  3.05102348\n","Step   8010: eval          Accuracy |  0.07352941\n"]}],"source":["# Model Training\n","additional_steps = 2000\n","\n","loop.run(additional_steps)"]},{"cell_type":"code","source":["# Saving\n","# Zip and Download\n","zip_n_dl(model_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"lzUUSBKLJdam","outputId":"83050ff9-a418-4d71-fa4b-00d1dd40af1e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_c84080cc-f5be-40f4-b109-dd5be2fcf905\", \"summary_transformer_model_v1.zip\", 3092536)"]},"metadata":{}}]},{"cell_type":"code","source":["# Additional Training:\n","additional_steps = 2000\n","loop.run(additional_steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEjmK1LtG07T","outputId":"633ba3c8-9820-447f-f8be-4d0d7396d714"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Step   8020: Ran 10 train steps in 22.86 secs\n","Step   8020: train CrossEntropyLoss |  2.84446287\n","Step   8020: eval  CrossEntropyLoss |  2.84076333\n","Step   8020: eval          Accuracy |  0.07563026\n","\n","Step   8030: Ran 10 train steps in 26.98 secs\n","Step   8030: train CrossEntropyLoss |  2.89877772\n","Step   8030: eval  CrossEntropyLoss |  2.84172583\n","Step   8030: eval          Accuracy |  0.08888889\n","\n","Step   8040: Ran 10 train steps in 25.69 secs\n","Step   8040: train CrossEntropyLoss |  2.85345316\n","Step   8040: eval  CrossEntropyLoss |  3.31788206\n","Step   8040: eval          Accuracy |  0.10714286\n","\n","Step   8050: Ran 10 train steps in 25.15 secs\n","Step   8050: train CrossEntropyLoss |  2.72076464\n","Step   8050: eval  CrossEntropyLoss |  3.16182280\n","Step   8050: eval          Accuracy |  0.10714286\n","\n","Step   8060: Ran 10 train steps in 22.76 secs\n","Step   8060: train CrossEntropyLoss |  2.78029776\n","Step   8060: eval  CrossEntropyLoss |  2.78838396\n","Step   8060: eval          Accuracy |  0.08965518\n","\n","Step   8070: Ran 10 train steps in 18.42 secs\n","Step   8070: train CrossEntropyLoss |  2.85353494\n","Step   8070: eval  CrossEntropyLoss |  2.50979495\n","Step   8070: eval          Accuracy |  0.13782051\n","\n","Step   8080: Ran 10 train steps in 23.03 secs\n","Step   8080: train CrossEntropyLoss |  2.90435719\n","Step   8080: eval  CrossEntropyLoss |  2.70721579\n","Step   8080: eval          Accuracy |  0.14880952\n","\n","Step   8090: Ran 10 train steps in 22.58 secs\n","Step   8090: train CrossEntropyLoss |  2.74183583\n","Step   8090: eval  CrossEntropyLoss |  2.49987864\n","Step   8090: eval          Accuracy |  0.08125000\n","\n","Step   8100: Ran 10 train steps in 22.13 secs\n","Step   8100: train CrossEntropyLoss |  2.99557185\n","Step   8100: eval  CrossEntropyLoss |  3.23770308\n","Step   8100: eval          Accuracy |  0.15540540\n","\n","Step   8110: Ran 10 train steps in 24.98 secs\n","Step   8110: train CrossEntropyLoss |  2.77364993\n","Step   8110: eval  CrossEntropyLoss |  2.84494734\n","Step   8110: eval          Accuracy |  0.13375796\n","\n","Step   8120: Ran 10 train steps in 25.07 secs\n","Step   8120: train CrossEntropyLoss |  2.88534784\n","Step   8120: eval  CrossEntropyLoss |  2.83131552\n","Step   8120: eval          Accuracy |  0.11290322\n","\n","Step   8130: Ran 10 train steps in 20.72 secs\n","Step   8130: train CrossEntropyLoss |  3.01813769\n","Step   8130: eval  CrossEntropyLoss |  2.46915817\n","Step   8130: eval          Accuracy |  0.05405406\n","\n","Step   8140: Ran 10 train steps in 24.04 secs\n","Step   8140: train CrossEntropyLoss |  2.79013562\n","Step   8140: eval  CrossEntropyLoss |  2.55629063\n","Step   8140: eval          Accuracy |  0.12605043\n","\n","Step   8150: Ran 10 train steps in 23.91 secs\n","Step   8150: train CrossEntropyLoss |  2.85679674\n","Step   8150: eval  CrossEntropyLoss |  2.64865279\n","Step   8150: eval          Accuracy |  0.09473684\n","\n","Step   8160: Ran 10 train steps in 24.41 secs\n","Step   8160: train CrossEntropyLoss |  2.75635386\n","Step   8160: eval  CrossEntropyLoss |  2.27432203\n","Step   8160: eval          Accuracy |  0.19148937\n","\n","Step   8170: Ran 10 train steps in 26.10 secs\n","Step   8170: train CrossEntropyLoss |  2.92847776\n","Step   8170: eval  CrossEntropyLoss |  2.79513884\n","Step   8170: eval          Accuracy |  0.07812500\n","\n","Step   8180: Ran 10 train steps in 23.94 secs\n","Step   8180: train CrossEntropyLoss |  2.75259423\n","Step   8180: eval  CrossEntropyLoss |  3.06299114\n","Step   8180: eval          Accuracy |  0.12380952\n","\n","Step   8190: Ran 10 train steps in 24.62 secs\n","Step   8190: train CrossEntropyLoss |  2.92192507\n","Step   8190: eval  CrossEntropyLoss |  2.98412657\n","Step   8190: eval          Accuracy |  0.07894737\n","\n","Step   8200: Ran 10 train steps in 21.27 secs\n","Step   8200: train CrossEntropyLoss |  2.85342288\n","Step   8200: eval  CrossEntropyLoss |  2.52582169\n","Step   8200: eval          Accuracy |  0.10328639\n","\n","Step   8210: Ran 10 train steps in 23.84 secs\n","Step   8210: train CrossEntropyLoss |  2.79827309\n","Step   8210: eval  CrossEntropyLoss |  2.06613898\n","Step   8210: eval          Accuracy |  0.14516129\n","\n","Step   8220: Ran 10 train steps in 25.09 secs\n","Step   8220: train CrossEntropyLoss |  2.88891482\n","Step   8220: eval  CrossEntropyLoss |  3.43632507\n","Step   8220: eval          Accuracy |  0.04729730\n","\n","Step   8230: Ran 10 train steps in 21.57 secs\n","Step   8230: train CrossEntropyLoss |  2.82025552\n","Step   8230: eval  CrossEntropyLoss |  2.69870186\n","Step   8230: eval          Accuracy |  0.08287293\n","\n","Step   8240: Ran 10 train steps in 24.42 secs\n","Step   8240: train CrossEntropyLoss |  2.81576467\n","Step   8240: eval  CrossEntropyLoss |  2.49214792\n","Step   8240: eval          Accuracy |  0.11111111\n","\n","Step   8250: Ran 10 train steps in 22.89 secs\n","Step   8250: train CrossEntropyLoss |  2.94059873\n","Step   8250: eval  CrossEntropyLoss |  2.30853796\n","Step   8250: eval          Accuracy |  0.07446808\n","\n","Step   8260: Ran 10 train steps in 21.89 secs\n","Step   8260: train CrossEntropyLoss |  2.75149870\n","Step   8260: eval  CrossEntropyLoss |  3.18385196\n","Step   8260: eval          Accuracy |  0.07142857\n","\n","Step   8270: Ran 10 train steps in 22.97 secs\n","Step   8270: train CrossEntropyLoss |  2.78648281\n","Step   8270: eval  CrossEntropyLoss |  2.50825238\n","Step   8270: eval          Accuracy |  0.08496732\n","\n","Step   8280: Ran 10 train steps in 22.39 secs\n","Step   8280: train CrossEntropyLoss |  3.01122332\n","Step   8280: eval  CrossEntropyLoss |  3.23241830\n","Step   8280: eval          Accuracy |  0.12352941\n","\n","Step   8290: Ran 10 train steps in 22.03 secs\n","Step   8290: train CrossEntropyLoss |  2.72868371\n","Step   8290: eval  CrossEntropyLoss |  3.29975057\n","Step   8290: eval          Accuracy |  0.11363637\n","\n","Step   8300: Ran 10 train steps in 19.29 secs\n","Step   8300: train CrossEntropyLoss |  2.67689085\n","Step   8300: eval  CrossEntropyLoss |  3.41264296\n","Step   8300: eval          Accuracy |  0.06024097\n","\n","Step   8310: Ran 10 train steps in 22.80 secs\n","Step   8310: train CrossEntropyLoss |  3.10077500\n","Step   8310: eval  CrossEntropyLoss |  2.70567131\n","Step   8310: eval          Accuracy |  0.09917355\n","\n","Step   8320: Ran 10 train steps in 23.40 secs\n","Step   8320: train CrossEntropyLoss |  2.83561540\n","Step   8320: eval  CrossEntropyLoss |  2.47958875\n","Step   8320: eval          Accuracy |  0.08387097\n","\n","Step   8330: Ran 10 train steps in 21.76 secs\n","Step   8330: train CrossEntropyLoss |  2.84546995\n","Step   8330: eval  CrossEntropyLoss |  2.68129444\n","Step   8330: eval          Accuracy |  0.11347517\n","\n","Step   8340: Ran 10 train steps in 26.38 secs\n","Step   8340: train CrossEntropyLoss |  2.77909231\n","Step   8340: eval  CrossEntropyLoss |  2.46972847\n","Step   8340: eval          Accuracy |  0.10303030\n","\n","Step   8350: Ran 10 train steps in 23.30 secs\n","Step   8350: train CrossEntropyLoss |  2.95583344\n","Step   8350: eval  CrossEntropyLoss |  2.68880630\n","Step   8350: eval          Accuracy |  0.08904110\n","\n","Step   8360: Ran 10 train steps in 25.65 secs\n","Step   8360: train CrossEntropyLoss |  2.70104647\n","Step   8360: eval  CrossEntropyLoss |  2.94205213\n","Step   8360: eval          Accuracy |  0.12903225\n","\n","Step   8370: Ran 10 train steps in 24.55 secs\n","Step   8370: train CrossEntropyLoss |  2.90304041\n","Step   8370: eval  CrossEntropyLoss |  2.40161395\n","Step   8370: eval          Accuracy |  0.07500000\n","\n","Step   8380: Ran 10 train steps in 23.90 secs\n","Step   8380: train CrossEntropyLoss |  2.69618845\n","Step   8380: eval  CrossEntropyLoss |  2.66768599\n","Step   8380: eval          Accuracy |  0.09090909\n","\n","Step   8390: Ran 10 train steps in 22.24 secs\n","Step   8390: train CrossEntropyLoss |  2.83740067\n","Step   8390: eval  CrossEntropyLoss |  2.77980518\n","Step   8390: eval          Accuracy |  0.11111111\n","\n","Step   8400: Ran 10 train steps in 23.29 secs\n","Step   8400: train CrossEntropyLoss |  2.75039911\n","Step   8400: eval  CrossEntropyLoss |  2.64126420\n","Step   8400: eval          Accuracy |  0.09210526\n","\n","Step   8410: Ran 10 train steps in 22.69 secs\n","Step   8410: train CrossEntropyLoss |  2.62435818\n","Step   8410: eval  CrossEntropyLoss |  2.91741705\n","Step   8410: eval          Accuracy |  0.11560693\n","\n","Step   8420: Ran 10 train steps in 23.29 secs\n","Step   8420: train CrossEntropyLoss |  2.85565996\n","Step   8420: eval  CrossEntropyLoss |  2.99774647\n","Step   8420: eval          Accuracy |  0.05309734\n","\n","Step   8430: Ran 10 train steps in 20.73 secs\n","Step   8430: train CrossEntropyLoss |  2.87774777\n","Step   8430: eval  CrossEntropyLoss |  2.83085918\n","Step   8430: eval          Accuracy |  0.10810811\n","\n","Step   8440: Ran 10 train steps in 22.47 secs\n","Step   8440: train CrossEntropyLoss |  2.70053458\n","Step   8440: eval  CrossEntropyLoss |  2.91298723\n","Step   8440: eval          Accuracy |  0.08648649\n","\n","Step   8450: Ran 10 train steps in 24.46 secs\n","Step   8450: train CrossEntropyLoss |  2.84073782\n","Step   8450: eval  CrossEntropyLoss |  2.69636488\n","Step   8450: eval          Accuracy |  0.08235294\n","\n","Step   8460: Ran 10 train steps in 24.43 secs\n","Step   8460: train CrossEntropyLoss |  2.89544582\n","Step   8460: eval  CrossEntropyLoss |  3.14097142\n","Step   8460: eval          Accuracy |  0.04347826\n","\n","Step   8470: Ran 10 train steps in 22.74 secs\n","Step   8470: train CrossEntropyLoss |  2.77016020\n","Step   8470: eval  CrossEntropyLoss |  2.30309963\n","Step   8470: eval          Accuracy |  0.09917355\n","\n","Step   8480: Ran 10 train steps in 22.70 secs\n","Step   8480: train CrossEntropyLoss |  2.74432516\n","Step   8480: eval  CrossEntropyLoss |  2.69083309\n","Step   8480: eval          Accuracy |  0.04166667\n","\n","Step   8490: Ran 10 train steps in 21.55 secs\n","Step   8490: train CrossEntropyLoss |  2.81429935\n","Step   8490: eval  CrossEntropyLoss |  2.66028404\n","Step   8490: eval          Accuracy |  0.10404624\n","\n","Step   8500: Ran 10 train steps in 23.90 secs\n","Step   8500: train CrossEntropyLoss |  2.78703380\n","Step   8500: eval  CrossEntropyLoss |  2.63913441\n","Step   8500: eval          Accuracy |  0.11702128\n","\n","Step   8510: Ran 10 train steps in 22.03 secs\n","Step   8510: train CrossEntropyLoss |  2.88251543\n","Step   8510: eval  CrossEntropyLoss |  3.10736752\n","Step   8510: eval          Accuracy |  0.12230216\n","\n","Step   8520: Ran 10 train steps in 26.27 secs\n","Step   8520: train CrossEntropyLoss |  2.91202259\n","Step   8520: eval  CrossEntropyLoss |  2.94222188\n","Step   8520: eval          Accuracy |  0.08225108\n","\n","Step   8530: Ran 10 train steps in 25.03 secs\n","Step   8530: train CrossEntropyLoss |  2.80512571\n","Step   8530: eval  CrossEntropyLoss |  2.77414107\n","Step   8530: eval          Accuracy |  0.11242604\n","\n","Step   8540: Ran 10 train steps in 21.93 secs\n","Step   8540: train CrossEntropyLoss |  2.85252047\n","Step   8540: eval  CrossEntropyLoss |  3.08645225\n","Step   8540: eval          Accuracy |  0.08391608\n","\n","Step   8550: Ran 10 train steps in 22.77 secs\n","Step   8550: train CrossEntropyLoss |  2.85017157\n","Step   8550: eval  CrossEntropyLoss |  3.02842736\n","Step   8550: eval          Accuracy |  0.14173229\n","\n","Step   8560: Ran 10 train steps in 25.00 secs\n","Step   8560: train CrossEntropyLoss |  2.61343527\n","Step   8560: eval  CrossEntropyLoss |  2.70153022\n","Step   8560: eval          Accuracy |  0.09392265\n","\n","Step   8570: Ran 10 train steps in 21.91 secs\n","Step   8570: train CrossEntropyLoss |  2.75695992\n","Step   8570: eval  CrossEntropyLoss |  2.56193447\n","Step   8570: eval          Accuracy |  0.07812500\n","\n","Step   8580: Ran 10 train steps in 22.72 secs\n","Step   8580: train CrossEntropyLoss |  2.76717615\n","Step   8580: eval  CrossEntropyLoss |  2.29350877\n","Step   8580: eval          Accuracy |  0.09459460\n","\n","Step   8590: Ran 10 train steps in 21.98 secs\n","Step   8590: train CrossEntropyLoss |  2.87048483\n","Step   8590: eval  CrossEntropyLoss |  2.38687015\n","Step   8590: eval          Accuracy |  0.08433735\n","\n","Step   8600: Ran 10 train steps in 21.59 secs\n","Step   8600: train CrossEntropyLoss |  2.77951360\n","Step   8600: eval  CrossEntropyLoss |  3.13660932\n","Step   8600: eval          Accuracy |  0.08441558\n","\n","Step   8610: Ran 10 train steps in 25.08 secs\n","Step   8610: train CrossEntropyLoss |  2.91988254\n","Step   8610: eval  CrossEntropyLoss |  3.43883729\n","Step   8610: eval          Accuracy |  0.09420290\n","\n","Step   8620: Ran 10 train steps in 23.79 secs\n","Step   8620: train CrossEntropyLoss |  2.91225863\n","Step   8620: eval  CrossEntropyLoss |  2.29081988\n","Step   8620: eval          Accuracy |  0.10112359\n","\n","Step   8630: Ran 10 train steps in 23.25 secs\n","Step   8630: train CrossEntropyLoss |  2.74637699\n","Step   8630: eval  CrossEntropyLoss |  2.48472452\n","Step   8630: eval          Accuracy |  0.11111111\n","\n","Step   8640: Ran 10 train steps in 26.29 secs\n","Step   8640: train CrossEntropyLoss |  2.69865298\n","Step   8640: eval  CrossEntropyLoss |  2.77089357\n","Step   8640: eval          Accuracy |  0.08641975\n","\n","Step   8650: Ran 10 train steps in 21.60 secs\n","Step   8650: train CrossEntropyLoss |  2.98496795\n","Step   8650: eval  CrossEntropyLoss |  2.44972396\n","Step   8650: eval          Accuracy |  0.08510638\n","\n","Step   8660: Ran 10 train steps in 23.50 secs\n","Step   8660: train CrossEntropyLoss |  2.59311461\n","Step   8660: eval  CrossEntropyLoss |  2.20518255\n","Step   8660: eval          Accuracy |  0.08510638\n","\n","Step   8670: Ran 10 train steps in 24.40 secs\n","Step   8670: train CrossEntropyLoss |  2.77572584\n","Step   8670: eval  CrossEntropyLoss |  2.91455579\n","Step   8670: eval          Accuracy |  0.13793103\n","\n","Step   8680: Ran 10 train steps in 28.25 secs\n","Step   8680: train CrossEntropyLoss |  2.79557228\n","Step   8680: eval  CrossEntropyLoss |  3.15442944\n","Step   8680: eval          Accuracy |  0.07812500\n","\n","Step   8690: Ran 10 train steps in 25.06 secs\n","Step   8690: train CrossEntropyLoss |  2.86321497\n","Step   8690: eval  CrossEntropyLoss |  2.89251518\n","Step   8690: eval          Accuracy |  0.13157895\n","\n","Step   8700: Ran 10 train steps in 25.02 secs\n","Step   8700: train CrossEntropyLoss |  2.63048172\n","Step   8700: eval  CrossEntropyLoss |  2.84759331\n","Step   8700: eval          Accuracy |  0.10891089\n","\n","Step   8710: Ran 10 train steps in 20.85 secs\n","Step   8710: train CrossEntropyLoss |  2.84182405\n","Step   8710: eval  CrossEntropyLoss |  2.75175428\n","Step   8710: eval          Accuracy |  0.10759494\n","\n","Step   8720: Ran 10 train steps in 24.99 secs\n","Step   8720: train CrossEntropyLoss |  2.73060942\n","Step   8720: eval  CrossEntropyLoss |  2.69587278\n","Step   8720: eval          Accuracy |  0.08241758\n","\n","Step   8730: Ran 10 train steps in 23.88 secs\n","Step   8730: train CrossEntropyLoss |  2.80350280\n","Step   8730: eval  CrossEntropyLoss |  2.73862839\n","Step   8730: eval          Accuracy |  0.13333334\n","\n","Step   8740: Ran 10 train steps in 23.47 secs\n","Step   8740: train CrossEntropyLoss |  2.89431572\n","Step   8740: eval  CrossEntropyLoss |  3.22134757\n","Step   8740: eval          Accuracy |  0.12500000\n","\n","Step   8750: Ran 10 train steps in 21.65 secs\n","Step   8750: train CrossEntropyLoss |  2.48884988\n","Step   8750: eval  CrossEntropyLoss |  2.56169987\n","Step   8750: eval          Accuracy |  0.06779661\n","\n","Step   8760: Ran 10 train steps in 27.90 secs\n","Step   8760: train CrossEntropyLoss |  2.84225488\n","Step   8760: eval  CrossEntropyLoss |  2.54292464\n","Step   8760: eval          Accuracy |  0.09016393\n","\n","Step   8770: Ran 10 train steps in 20.15 secs\n","Step   8770: train CrossEntropyLoss |  2.84145451\n","Step   8770: eval  CrossEntropyLoss |  3.21575141\n","Step   8770: eval          Accuracy |  0.07801419\n","\n","Step   8780: Ran 10 train steps in 24.10 secs\n","Step   8780: train CrossEntropyLoss |  2.73852658\n","Step   8780: eval  CrossEntropyLoss |  2.47591519\n","Step   8780: eval          Accuracy |  0.14285715\n","\n","Step   8790: Ran 10 train steps in 22.92 secs\n","Step   8790: train CrossEntropyLoss |  2.95488477\n","Step   8790: eval  CrossEntropyLoss |  3.00972652\n","Step   8790: eval          Accuracy |  0.13333334\n","\n","Step   8800: Ran 10 train steps in 23.84 secs\n","Step   8800: train CrossEntropyLoss |  2.94574022\n","Step   8800: eval  CrossEntropyLoss |  2.23721552\n","Step   8800: eval          Accuracy |  0.08510638\n","\n","Step   8810: Ran 10 train steps in 26.33 secs\n","Step   8810: train CrossEntropyLoss |  2.78208661\n","Step   8810: eval  CrossEntropyLoss |  2.57794809\n","Step   8810: eval          Accuracy |  0.10679612\n","\n","Step   8820: Ran 10 train steps in 20.78 secs\n","Step   8820: train CrossEntropyLoss |  2.76674390\n","Step   8820: eval  CrossEntropyLoss |  2.34936261\n","Step   8820: eval          Accuracy |  0.06593407\n","\n","Step   8830: Ran 10 train steps in 23.81 secs\n","Step   8830: train CrossEntropyLoss |  2.76970100\n","Step   8830: eval  CrossEntropyLoss |  2.63200569\n","Step   8830: eval          Accuracy |  0.08695652\n","\n","Step   8840: Ran 10 train steps in 20.89 secs\n","Step   8840: train CrossEntropyLoss |  2.88556862\n","Step   8840: eval  CrossEntropyLoss |  3.46008015\n","Step   8840: eval          Accuracy |  0.08648649\n","\n","Step   8850: Ran 10 train steps in 24.02 secs\n","Step   8850: train CrossEntropyLoss |  2.67811966\n","Step   8850: eval  CrossEntropyLoss |  3.32873940\n","Step   8850: eval          Accuracy |  0.13333334\n","\n","Step   8860: Ran 10 train steps in 20.98 secs\n","Step   8860: train CrossEntropyLoss |  2.76312852\n","Step   8860: eval  CrossEntropyLoss |  2.80996561\n","Step   8860: eval          Accuracy |  0.10625000\n","\n","Step   8870: Ran 10 train steps in 23.83 secs\n","Step   8870: train CrossEntropyLoss |  2.71203136\n","Step   8870: eval  CrossEntropyLoss |  2.46132541\n","Step   8870: eval          Accuracy |  0.13157895\n","\n","Step   8880: Ran 10 train steps in 22.08 secs\n","Step   8880: train CrossEntropyLoss |  2.78372097\n","Step   8880: eval  CrossEntropyLoss |  2.51704383\n","Step   8880: eval          Accuracy |  0.10204082\n","\n","Step   8890: Ran 10 train steps in 24.04 secs\n","Step   8890: train CrossEntropyLoss |  2.77432060\n","Step   8890: eval  CrossEntropyLoss |  2.80995846\n","Step   8890: eval          Accuracy |  0.08641975\n","\n","Step   8900: Ran 10 train steps in 20.95 secs\n","Step   8900: train CrossEntropyLoss |  2.84056520\n","Step   8900: eval  CrossEntropyLoss |  2.63815904\n","Step   8900: eval          Accuracy |  0.08029197\n","\n","Step   8910: Ran 10 train steps in 23.10 secs\n","Step   8910: train CrossEntropyLoss |  2.83802915\n","Step   8910: eval  CrossEntropyLoss |  3.13497901\n","Step   8910: eval          Accuracy |  0.12598425\n","\n","Step   8920: Ran 10 train steps in 26.28 secs\n","Step   8920: train CrossEntropyLoss |  2.78292513\n","Step   8920: eval  CrossEntropyLoss |  3.40354967\n","Step   8920: eval          Accuracy |  0.05000000\n","\n","Step   8930: Ran 10 train steps in 26.16 secs\n","Step   8930: train CrossEntropyLoss |  2.74006557\n","Step   8930: eval  CrossEntropyLoss |  2.55507183\n","Step   8930: eval          Accuracy |  0.11290322\n","\n","Step   8940: Ran 10 train steps in 21.49 secs\n","Step   8940: train CrossEntropyLoss |  2.87605476\n","Step   8940: eval  CrossEntropyLoss |  2.55623984\n","Step   8940: eval          Accuracy |  0.13147411\n","\n","Step   8950: Ran 10 train steps in 19.07 secs\n","Step   8950: train CrossEntropyLoss |  2.79833555\n","Step   8950: eval  CrossEntropyLoss |  3.31437778\n","Step   8950: eval          Accuracy |  0.07142857\n","\n","Step   8960: Ran 10 train steps in 22.81 secs\n","Step   8960: train CrossEntropyLoss |  2.72341299\n","Step   8960: eval  CrossEntropyLoss |  3.11291051\n","Step   8960: eval          Accuracy |  0.09756097\n","\n","Step   8970: Ran 10 train steps in 26.62 secs\n","Step   8970: train CrossEntropyLoss |  2.80264592\n","Step   8970: eval  CrossEntropyLoss |  2.94830465\n","Step   8970: eval          Accuracy |  0.11940298\n","\n","Step   8980: Ran 10 train steps in 23.49 secs\n","Step   8980: train CrossEntropyLoss |  2.56443667\n","Step   8980: eval  CrossEntropyLoss |  2.79079032\n","Step   8980: eval          Accuracy |  0.14473684\n","\n","Step   8990: Ran 10 train steps in 24.02 secs\n","Step   8990: train CrossEntropyLoss |  2.68897843\n","Step   8990: eval  CrossEntropyLoss |  2.07236147\n","Step   8990: eval          Accuracy |  0.17021276\n","\n","Step   9000: Ran 10 train steps in 23.32 secs\n","Step   9000: train CrossEntropyLoss |  2.84996748\n","Step   9000: eval  CrossEntropyLoss |  2.48206234\n","Step   9000: eval          Accuracy |  0.07812500\n","\n","Step   9010: Ran 10 train steps in 26.97 secs\n","Step   9010: train CrossEntropyLoss |  2.70725870\n","Step   9010: eval  CrossEntropyLoss |  2.83078456\n","Step   9010: eval          Accuracy |  0.10526316\n","\n","Step   9020: Ran 10 train steps in 23.27 secs\n","Step   9020: train CrossEntropyLoss |  3.00564146\n","Step   9020: eval  CrossEntropyLoss |  2.45091915\n","Step   9020: eval          Accuracy |  0.06756756\n","\n","Step   9030: Ran 10 train steps in 23.33 secs\n","Step   9030: train CrossEntropyLoss |  2.67454147\n","Step   9030: eval  CrossEntropyLoss |  2.90914559\n","Step   9030: eval          Accuracy |  0.08235294\n","\n","Step   9040: Ran 10 train steps in 22.99 secs\n","Step   9040: train CrossEntropyLoss |  2.88281155\n","Step   9040: eval  CrossEntropyLoss |  2.38701272\n","Step   9040: eval          Accuracy |  0.08176101\n","\n","Step   9050: Ran 10 train steps in 23.38 secs\n","Step   9050: train CrossEntropyLoss |  2.60247087\n","Step   9050: eval  CrossEntropyLoss |  2.21140456\n","Step   9050: eval          Accuracy |  0.09302326\n","\n","Step   9060: Ran 10 train steps in 26.18 secs\n","Step   9060: train CrossEntropyLoss |  2.90494633\n","Step   9060: eval  CrossEntropyLoss |  2.89033604\n","Step   9060: eval          Accuracy |  0.08571429\n","\n","Step   9070: Ran 10 train steps in 21.84 secs\n","Step   9070: train CrossEntropyLoss |  2.81404257\n","Step   9070: eval  CrossEntropyLoss |  2.89792991\n","Step   9070: eval          Accuracy |  0.04347826\n","\n","Step   9080: Ran 10 train steps in 24.39 secs\n","Step   9080: train CrossEntropyLoss |  2.87417603\n","Step   9080: eval  CrossEntropyLoss |  2.35150099\n","Step   9080: eval          Accuracy |  0.10526316\n","\n","Step   9090: Ran 10 train steps in 22.33 secs\n","Step   9090: train CrossEntropyLoss |  2.76651335\n","Step   9090: eval  CrossEntropyLoss |  3.08900380\n","Step   9090: eval          Accuracy |  0.10416666\n","\n","Step   9100: Ran 10 train steps in 21.78 secs\n","Step   9100: train CrossEntropyLoss |  2.80576396\n","Step   9100: eval  CrossEntropyLoss |  2.65821314\n","Step   9100: eval          Accuracy |  0.08783784\n","\n","Step   9110: Ran 10 train steps in 22.79 secs\n","Step   9110: train CrossEntropyLoss |  2.95286679\n","Step   9110: eval  CrossEntropyLoss |  2.84160233\n","Step   9110: eval          Accuracy |  0.10240964\n","\n","Step   9120: Ran 10 train steps in 25.38 secs\n","Step   9120: train CrossEntropyLoss |  2.75558352\n","Step   9120: eval  CrossEntropyLoss |  2.39127946\n","Step   9120: eval          Accuracy |  0.06250000\n","\n","Step   9130: Ran 10 train steps in 22.28 secs\n","Step   9130: train CrossEntropyLoss |  2.71865606\n","Step   9130: eval  CrossEntropyLoss |  2.70172977\n","Step   9130: eval          Accuracy |  0.18518518\n","\n","Step   9140: Ran 10 train steps in 22.86 secs\n","Step   9140: train CrossEntropyLoss |  2.88619471\n","Step   9140: eval  CrossEntropyLoss |  2.86644840\n","Step   9140: eval          Accuracy |  0.04615385\n","\n","Step   9150: Ran 10 train steps in 23.79 secs\n","Step   9150: train CrossEntropyLoss |  2.77473116\n","Step   9150: eval  CrossEntropyLoss |  2.55735064\n","Step   9150: eval          Accuracy |  0.11206897\n","\n","Step   9160: Ran 10 train steps in 22.13 secs\n","Step   9160: train CrossEntropyLoss |  2.72270536\n","Step   9160: eval  CrossEntropyLoss |  2.19181752\n","Step   9160: eval          Accuracy |  0.07894737\n","\n","Step   9170: Ran 10 train steps in 24.40 secs\n","Step   9170: train CrossEntropyLoss |  2.84864855\n","Step   9170: eval  CrossEntropyLoss |  2.47131467\n","Step   9170: eval          Accuracy |  0.08264463\n","\n","Step   9180: Ran 10 train steps in 22.85 secs\n","Step   9180: train CrossEntropyLoss |  3.01451683\n","Step   9180: eval  CrossEntropyLoss |  2.65043902\n","Step   9180: eval          Accuracy |  0.10734463\n","\n","Step   9190: Ran 10 train steps in 20.91 secs\n","Step   9190: train CrossEntropyLoss |  2.81435084\n","Step   9190: eval  CrossEntropyLoss |  2.90382481\n","Step   9190: eval          Accuracy |  0.08695652\n","\n","Step   9200: Ran 10 train steps in 23.84 secs\n","Step   9200: train CrossEntropyLoss |  2.75986719\n","Step   9200: eval  CrossEntropyLoss |  2.93845510\n","Step   9200: eval          Accuracy |  0.08196721\n","\n","Step   9210: Ran 10 train steps in 22.81 secs\n","Step   9210: train CrossEntropyLoss |  2.94749022\n","Step   9210: eval  CrossEntropyLoss |  2.62050939\n","Step   9210: eval          Accuracy |  0.10465116\n","\n","Step   9220: Ran 10 train steps in 21.42 secs\n","Step   9220: train CrossEntropyLoss |  2.67903924\n","Step   9220: eval  CrossEntropyLoss |  2.56301880\n","Step   9220: eval          Accuracy |  0.10951009\n","\n","Step   9230: Ran 10 train steps in 25.76 secs\n","Step   9230: train CrossEntropyLoss |  2.75198984\n","Step   9230: eval  CrossEntropyLoss |  3.18938160\n","Step   9230: eval          Accuracy |  0.05000000\n","\n","Step   9240: Ran 10 train steps in 27.39 secs\n","Step   9240: train CrossEntropyLoss |  2.89161038\n","Step   9240: eval  CrossEntropyLoss |  3.27270031\n","Step   9240: eval          Accuracy |  0.09708738\n","\n","Step   9250: Ran 10 train steps in 23.36 secs\n","Step   9250: train CrossEntropyLoss |  2.75158215\n","Step   9250: eval  CrossEntropyLoss |  2.79126883\n","Step   9250: eval          Accuracy |  0.09933775\n","\n","Step   9260: Ran 10 train steps in 22.19 secs\n","Step   9260: train CrossEntropyLoss |  2.76005101\n","Step   9260: eval  CrossEntropyLoss |  2.77071524\n","Step   9260: eval          Accuracy |  0.12582782\n","\n","Step   9270: Ran 10 train steps in 26.31 secs\n","Step   9270: train CrossEntropyLoss |  2.70027256\n","Step   9270: eval  CrossEntropyLoss |  3.15699077\n","Step   9270: eval          Accuracy |  0.11042945\n","\n","Step   9280: Ran 10 train steps in 21.04 secs\n","Step   9280: train CrossEntropyLoss |  2.71049738\n","Step   9280: eval  CrossEntropyLoss |  2.52440238\n","Step   9280: eval          Accuracy |  0.15000001\n","\n","Step   9290: Ran 10 train steps in 22.27 secs\n","Step   9290: train CrossEntropyLoss |  2.98804379\n","Step   9290: eval  CrossEntropyLoss |  2.91673470\n","Step   9290: eval          Accuracy |  0.09375000\n","\n","Step   9300: Ran 10 train steps in 22.91 secs\n","Step   9300: train CrossEntropyLoss |  2.87704349\n","Step   9300: eval  CrossEntropyLoss |  3.01957202\n","Step   9300: eval          Accuracy |  0.15023474\n","\n","Step   9310: Ran 10 train steps in 23.89 secs\n","Step   9310: train CrossEntropyLoss |  2.61437559\n","Step   9310: eval  CrossEntropyLoss |  3.50702333\n","Step   9310: eval          Accuracy |  0.11965812\n","\n","Step   9320: Ran 10 train steps in 21.63 secs\n","Step   9320: train CrossEntropyLoss |  3.02649045\n","Step   9320: eval  CrossEntropyLoss |  2.63560605\n","Step   9320: eval          Accuracy |  0.12500000\n","\n","Step   9330: Ran 10 train steps in 25.69 secs\n","Step   9330: train CrossEntropyLoss |  2.77731657\n","Step   9330: eval  CrossEntropyLoss |  2.56425714\n","Step   9330: eval          Accuracy |  0.06578948\n","\n","Step   9340: Ran 10 train steps in 22.84 secs\n","Step   9340: train CrossEntropyLoss |  2.90448022\n","Step   9340: eval  CrossEntropyLoss |  2.49866152\n","Step   9340: eval          Accuracy |  0.08333334\n","\n","Step   9350: Ran 10 train steps in 24.68 secs\n","Step   9350: train CrossEntropyLoss |  2.68853259\n","Step   9350: eval  CrossEntropyLoss |  2.80530548\n","Step   9350: eval          Accuracy |  0.14341085\n","\n","Step   9360: Ran 10 train steps in 28.62 secs\n","Step   9360: train CrossEntropyLoss |  2.67172241\n","Step   9360: eval  CrossEntropyLoss |  3.07534051\n","Step   9360: eval          Accuracy |  0.13076924\n","\n","Step   9370: Ran 10 train steps in 22.31 secs\n","Step   9370: train CrossEntropyLoss |  2.88345146\n","Step   9370: eval  CrossEntropyLoss |  2.85769677\n","Step   9370: eval          Accuracy |  0.09142857\n","\n","Step   9380: Ran 10 train steps in 23.05 secs\n","Step   9380: train CrossEntropyLoss |  2.78743410\n","Step   9380: eval  CrossEntropyLoss |  3.45815349\n","Step   9380: eval          Accuracy |  0.09900990\n","\n","Step   9390: Ran 10 train steps in 21.82 secs\n","Step   9390: train CrossEntropyLoss |  2.91098166\n","Step   9390: eval  CrossEntropyLoss |  2.74943066\n","Step   9390: eval          Accuracy |  0.09285714\n","\n","Step   9400: Ran 10 train steps in 23.32 secs\n","Step   9400: train CrossEntropyLoss |  2.88296461\n","Step   9400: eval  CrossEntropyLoss |  2.46736503\n","Step   9400: eval          Accuracy |  0.09578544\n","\n","Step   9410: Ran 10 train steps in 23.45 secs\n","Step   9410: train CrossEntropyLoss |  2.90695238\n","Step   9410: eval  CrossEntropyLoss |  2.76936984\n","Step   9410: eval          Accuracy |  0.11458334\n","\n","Step   9420: Ran 10 train steps in 21.79 secs\n","Step   9420: train CrossEntropyLoss |  3.05248499\n","Step   9420: eval  CrossEntropyLoss |  2.71656775\n","Step   9420: eval          Accuracy |  0.09589041\n","\n","Step   9430: Ran 10 train steps in 23.41 secs\n","Step   9430: train CrossEntropyLoss |  3.00478053\n","Step   9430: eval  CrossEntropyLoss |  3.22905707\n","Step   9430: eval          Accuracy |  0.10132159\n","\n","Step   9440: Ran 10 train steps in 22.30 secs\n","Step   9440: train CrossEntropyLoss |  2.69894457\n","Step   9440: eval  CrossEntropyLoss |  3.07810831\n","Step   9440: eval          Accuracy |  0.12621360\n","\n","Step   9450: Ran 10 train steps in 23.91 secs\n","Step   9450: train CrossEntropyLoss |  2.75308490\n","Step   9450: eval  CrossEntropyLoss |  2.83615494\n","Step   9450: eval          Accuracy |  0.11340206\n","\n","Step   9460: Ran 10 train steps in 21.94 secs\n","Step   9460: train CrossEntropyLoss |  2.68513536\n","Step   9460: eval  CrossEntropyLoss |  2.85544705\n","Step   9460: eval          Accuracy |  0.06521739\n","\n","Step   9470: Ran 10 train steps in 27.37 secs\n","Step   9470: train CrossEntropyLoss |  2.82766819\n","Step   9470: eval  CrossEntropyLoss |  3.15752625\n","Step   9470: eval          Accuracy |  0.09569378\n","\n","Step   9480: Ran 10 train steps in 21.11 secs\n","Step   9480: train CrossEntropyLoss |  2.71114755\n","Step   9480: eval  CrossEntropyLoss |  2.26196003\n","Step   9480: eval          Accuracy |  0.07272727\n","\n","Step   9490: Ran 10 train steps in 22.82 secs\n","Step   9490: train CrossEntropyLoss |  2.72317195\n","Step   9490: eval  CrossEntropyLoss |  2.47134590\n","Step   9490: eval          Accuracy |  0.08888889\n","\n","Step   9500: Ran 10 train steps in 20.61 secs\n","Step   9500: train CrossEntropyLoss |  2.74377751\n","Step   9500: eval  CrossEntropyLoss |  3.08152461\n","Step   9500: eval          Accuracy |  0.10687023\n","\n","Step   9510: Ran 10 train steps in 25.20 secs\n","Step   9510: train CrossEntropyLoss |  3.03363204\n","Step   9510: eval  CrossEntropyLoss |  2.81472850\n","Step   9510: eval          Accuracy |  0.04255319\n","\n","Step   9520: Ran 10 train steps in 25.81 secs\n","Step   9520: train CrossEntropyLoss |  2.91007853\n","Step   9520: eval  CrossEntropyLoss |  2.96965861\n","Step   9520: eval          Accuracy |  0.11612903\n","\n","Step   9530: Ran 10 train steps in 25.35 secs\n","Step   9530: train CrossEntropyLoss |  2.90730858\n","Step   9530: eval  CrossEntropyLoss |  2.29096937\n","Step   9530: eval          Accuracy |  0.08064516\n","\n","Step   9540: Ran 10 train steps in 24.56 secs\n","Step   9540: train CrossEntropyLoss |  2.64943075\n","Step   9540: eval  CrossEntropyLoss |  3.24631739\n","Step   9540: eval          Accuracy |  0.10526316\n","\n","Step   9550: Ran 10 train steps in 25.18 secs\n","Step   9550: train CrossEntropyLoss |  2.87081480\n","Step   9550: eval  CrossEntropyLoss |  2.58152652\n","Step   9550: eval          Accuracy |  0.08450704\n","\n","Step   9560: Ran 10 train steps in 24.76 secs\n","Step   9560: train CrossEntropyLoss |  2.74991465\n","Step   9560: eval  CrossEntropyLoss |  2.81229663\n","Step   9560: eval          Accuracy |  0.10526316\n","\n","Step   9570: Ran 10 train steps in 19.03 secs\n","Step   9570: train CrossEntropyLoss |  2.83638620\n","Step   9570: eval  CrossEntropyLoss |  2.44000554\n","Step   9570: eval          Accuracy |  0.11475410\n","\n","Step   9580: Ran 10 train steps in 26.88 secs\n","Step   9580: train CrossEntropyLoss |  2.77846098\n","Step   9580: eval  CrossEntropyLoss |  2.68085647\n","Step   9580: eval          Accuracy |  0.12820514\n","\n","Step   9590: Ran 10 train steps in 17.94 secs\n","Step   9590: train CrossEntropyLoss |  2.71804523\n","Step   9590: eval  CrossEntropyLoss |  3.18599486\n","Step   9590: eval          Accuracy |  0.08403362\n","\n","Step   9600: Ran 10 train steps in 22.81 secs\n","Step   9600: train CrossEntropyLoss |  2.91466951\n","Step   9600: eval  CrossEntropyLoss |  2.60644627\n","Step   9600: eval          Accuracy |  0.05263158\n","\n","Step   9610: Ran 10 train steps in 21.66 secs\n","Step   9610: train CrossEntropyLoss |  2.77022409\n","Step   9610: eval  CrossEntropyLoss |  2.93607712\n","Step   9610: eval          Accuracy |  0.09039548\n","\n","Step   9620: Ran 10 train steps in 25.11 secs\n","Step   9620: train CrossEntropyLoss |  2.72591090\n","Step   9620: eval  CrossEntropyLoss |  2.61330342\n","Step   9620: eval          Accuracy |  0.11250000\n","\n","Step   9630: Ran 10 train steps in 25.12 secs\n","Step   9630: train CrossEntropyLoss |  2.65245533\n","Step   9630: eval  CrossEntropyLoss |  2.96102309\n","Step   9630: eval          Accuracy |  0.08256881\n","\n","Step   9640: Ran 10 train steps in 21.62 secs\n","Step   9640: train CrossEntropyLoss |  2.86233449\n","Step   9640: eval  CrossEntropyLoss |  3.24975657\n","Step   9640: eval          Accuracy |  0.06250000\n","\n","Step   9650: Ran 10 train steps in 22.21 secs\n","Step   9650: train CrossEntropyLoss |  2.95578742\n","Step   9650: eval  CrossEntropyLoss |  3.19464779\n","Step   9650: eval          Accuracy |  0.12280702\n","\n","Step   9660: Ran 10 train steps in 25.19 secs\n","Step   9660: train CrossEntropyLoss |  2.88452768\n","Step   9660: eval  CrossEntropyLoss |  2.78891945\n","Step   9660: eval          Accuracy |  0.10738255\n","\n","Step   9670: Ran 10 train steps in 19.78 secs\n","Step   9670: train CrossEntropyLoss |  2.99730372\n","Step   9670: eval  CrossEntropyLoss |  2.68279457\n","Step   9670: eval          Accuracy |  0.11023622\n","\n","Step   9680: Ran 10 train steps in 25.20 secs\n","Step   9680: train CrossEntropyLoss |  2.94090557\n","Step   9680: eval  CrossEntropyLoss |  2.49388289\n","Step   9680: eval          Accuracy |  0.07692308\n","\n","Step   9690: Ran 10 train steps in 22.86 secs\n","Step   9690: train CrossEntropyLoss |  2.75467300\n","Step   9690: eval  CrossEntropyLoss |  2.54753232\n","Step   9690: eval          Accuracy |  0.11464968\n","\n","Step   9700: Ran 10 train steps in 21.74 secs\n","Step   9700: train CrossEntropyLoss |  2.69052362\n","Step   9700: eval  CrossEntropyLoss |  2.79443622\n","Step   9700: eval          Accuracy |  0.12000000\n","\n","Step   9710: Ran 10 train steps in 23.36 secs\n","Step   9710: train CrossEntropyLoss |  2.87305403\n","Step   9710: eval  CrossEntropyLoss |  3.57844496\n","Step   9710: eval          Accuracy |  0.07894737\n","\n","Step   9720: Ran 10 train steps in 28.49 secs\n","Step   9720: train CrossEntropyLoss |  2.92346954\n","Step   9720: eval  CrossEntropyLoss |  2.60370874\n","Step   9720: eval          Accuracy |  0.10191083\n","\n","Step   9730: Ran 10 train steps in 24.19 secs\n","Step   9730: train CrossEntropyLoss |  2.75998068\n","Step   9730: eval  CrossEntropyLoss |  3.20480847\n","Step   9730: eval          Accuracy |  0.12258065\n","\n","Step   9740: Ran 10 train steps in 20.73 secs\n","Step   9740: train CrossEntropyLoss |  2.64152813\n","Step   9740: eval  CrossEntropyLoss |  1.86163008\n","Step   9740: eval          Accuracy |  0.10169491\n","\n","Step   9750: Ran 10 train steps in 23.19 secs\n","Step   9750: train CrossEntropyLoss |  2.87498188\n","Step   9750: eval  CrossEntropyLoss |  2.49561548\n","Step   9750: eval          Accuracy |  0.14146341\n","\n","Step   9760: Ran 10 train steps in 26.43 secs\n","Step   9760: train CrossEntropyLoss |  2.92391205\n","Step   9760: eval  CrossEntropyLoss |  2.73291731\n","Step   9760: eval          Accuracy |  0.11250000\n","\n","Step   9770: Ran 10 train steps in 25.24 secs\n","Step   9770: train CrossEntropyLoss |  2.64879847\n","Step   9770: eval  CrossEntropyLoss |  2.87717223\n","Step   9770: eval          Accuracy |  0.08982036\n","\n","Step   9780: Ran 10 train steps in 22.39 secs\n","Step   9780: train CrossEntropyLoss |  2.60982370\n","Step   9780: eval  CrossEntropyLoss |  2.41957808\n","Step   9780: eval          Accuracy |  0.09722222\n","\n","Step   9790: Ran 10 train steps in 24.04 secs\n","Step   9790: train CrossEntropyLoss |  2.77692842\n","Step   9790: eval  CrossEntropyLoss |  2.78432465\n","Step   9790: eval          Accuracy |  0.08461539\n","\n","Step   9800: Ran 10 train steps in 22.93 secs\n","Step   9800: train CrossEntropyLoss |  2.99690604\n","Step   9800: eval  CrossEntropyLoss |  2.37847090\n","Step   9800: eval          Accuracy |  0.12109375\n","\n","Step   9810: Ran 10 train steps in 24.50 secs\n","Step   9810: train CrossEntropyLoss |  2.95096064\n","Step   9810: eval  CrossEntropyLoss |  2.64592695\n","Step   9810: eval          Accuracy |  0.08148148\n","\n","Step   9820: Ran 10 train steps in 21.75 secs\n","Step   9820: train CrossEntropyLoss |  2.64502668\n","Step   9820: eval  CrossEntropyLoss |  2.29647088\n","Step   9820: eval          Accuracy |  0.10714286\n","\n","Step   9830: Ran 10 train steps in 22.56 secs\n","Step   9830: train CrossEntropyLoss |  2.63856435\n","Step   9830: eval  CrossEntropyLoss |  3.28468394\n","Step   9830: eval          Accuracy |  0.10924370\n","\n","Step   9840: Ran 10 train steps in 24.92 secs\n","Step   9840: train CrossEntropyLoss |  2.80887270\n","Step   9840: eval  CrossEntropyLoss |  3.31857634\n","Step   9840: eval          Accuracy |  0.10810811\n","\n","Step   9850: Ran 10 train steps in 26.85 secs\n","Step   9850: train CrossEntropyLoss |  2.94404578\n","Step   9850: eval  CrossEntropyLoss |  2.77479291\n","Step   9850: eval          Accuracy |  0.07111111\n","\n","Step   9860: Ran 10 train steps in 25.24 secs\n","Step   9860: train CrossEntropyLoss |  2.91548347\n","Step   9860: eval  CrossEntropyLoss |  2.40130234\n","Step   9860: eval          Accuracy |  0.06756756\n","\n","Step   9870: Ran 10 train steps in 25.29 secs\n","Step   9870: train CrossEntropyLoss |  2.75188255\n","Step   9870: eval  CrossEntropyLoss |  2.41173649\n","Step   9870: eval          Accuracy |  0.11602210\n","\n","Step   9880: Ran 10 train steps in 24.61 secs\n","Step   9880: train CrossEntropyLoss |  2.76243258\n","Step   9880: eval  CrossEntropyLoss |  2.67411137\n","Step   9880: eval          Accuracy |  0.11363637\n","\n","Step   9890: Ran 10 train steps in 25.18 secs\n","Step   9890: train CrossEntropyLoss |  2.73123813\n","Step   9890: eval  CrossEntropyLoss |  2.49708724\n","Step   9890: eval          Accuracy |  0.09933775\n","\n","Step   9900: Ran 10 train steps in 21.07 secs\n","Step   9900: train CrossEntropyLoss |  2.72516584\n","Step   9900: eval  CrossEntropyLoss |  2.67327642\n","Step   9900: eval          Accuracy |  0.08474576\n","\n","Step   9910: Ran 10 train steps in 25.31 secs\n","Step   9910: train CrossEntropyLoss |  2.83755922\n","Step   9910: eval  CrossEntropyLoss |  2.94776082\n","Step   9910: eval          Accuracy |  0.11146497\n","\n","Step   9920: Ran 10 train steps in 25.28 secs\n","Step   9920: train CrossEntropyLoss |  2.84124112\n","Step   9920: eval  CrossEntropyLoss |  2.36570740\n","Step   9920: eval          Accuracy |  0.05617978\n","\n","Step   9930: Ran 10 train steps in 21.05 secs\n","Step   9930: train CrossEntropyLoss |  2.66946387\n","Step   9930: eval  CrossEntropyLoss |  2.99774361\n","Step   9930: eval          Accuracy |  0.07751938\n","\n","Step   9940: Ran 10 train steps in 24.61 secs\n","Step   9940: train CrossEntropyLoss |  2.79920220\n","Step   9940: eval  CrossEntropyLoss |  2.51105165\n","Step   9940: eval          Accuracy |  0.07594936\n","\n","Step   9950: Ran 10 train steps in 25.78 secs\n","Step   9950: train CrossEntropyLoss |  2.82046866\n","Step   9950: eval  CrossEntropyLoss |  2.50156498\n","Step   9950: eval          Accuracy |  0.07518797\n","\n","Step   9960: Ran 10 train steps in 20.23 secs\n","Step   9960: train CrossEntropyLoss |  2.82907820\n","Step   9960: eval  CrossEntropyLoss |  2.79301524\n","Step   9960: eval          Accuracy |  0.11111111\n","\n","Step   9970: Ran 10 train steps in 20.09 secs\n","Step   9970: train CrossEntropyLoss |  2.84800005\n","Step   9970: eval  CrossEntropyLoss |  2.54735923\n","Step   9970: eval          Accuracy |  0.12041885\n","\n","Step   9980: Ran 10 train steps in 24.15 secs\n","Step   9980: train CrossEntropyLoss |  2.74635172\n","Step   9980: eval  CrossEntropyLoss |  3.57544494\n","Step   9980: eval          Accuracy |  0.11111111\n","\n","Step   9990: Ran 10 train steps in 22.94 secs\n","Step   9990: train CrossEntropyLoss |  2.83261538\n","Step   9990: eval  CrossEntropyLoss |  2.67454123\n","Step   9990: eval          Accuracy |  0.11594203\n","\n","Step  10000: Ran 10 train steps in 29.15 secs\n","Step  10000: train CrossEntropyLoss |  2.75298619\n","Step  10000: eval  CrossEntropyLoss |  2.43224692\n","Step  10000: eval          Accuracy |  0.10552764\n","\n","Step  10010: Ran 10 train steps in 24.03 secs\n","Step  10010: train CrossEntropyLoss |  2.54764533\n","Step  10010: eval  CrossEntropyLoss |  2.31945753\n","Step  10010: eval          Accuracy |  0.03947368\n"]}]},{"cell_type":"code","source":["# Saving\n","# Zip and Download\n","zip_n_dl(model_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"qhdSvuTQUdC7","outputId":"ada43e6f-7e22-4dc5-949e-0e00093c53a0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_297479e2-8b07-44bf-93de-a32bc28aa5f1\", \"summary_transformer_model_v1.zip\", 3128605)"]},"metadata":{}}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"6vAdls2FG1K1"}},{"cell_type":"markdown","metadata":{"id":"XKrEBjmskeWa"},"source":[" <a name='4'></a>\n"," # Part 4:  Evaluation\n","\n","<a name='4.1'></a>\n","### 4.1 Loading in a trained model\n","\n","Evaluating the model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cr2MpCrdToVu","outputId":"b5bc1f82-270f-489f-c517-0d6c93915332"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['eval_model',\n"," 'eval_tasks',\n"," 'history',\n"," 'is_chief',\n"," 'load_checkpoint',\n"," 'log_summary',\n"," 'model',\n"," 'n_devices',\n"," 'new_rng',\n"," 'output_dir',\n"," 'run',\n"," 'run_evals',\n"," 'save_checkpoint',\n"," 'step',\n"," 'tasks',\n"," 'update_weights_and_state']"]},"metadata":{},"execution_count":51}],"source":["[x for x in dir(loop) if not(str(x).startswith('_'))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ohTV9-P0Tt_B"},"outputs":[],"source":["# Fetching Model\n","\n","model = loop.model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWoSzR5tkoAx"},"outputs":[],"source":["# # Fetching Model\n","\n","# # Get the model architecture\n","# model = TransformerLM(mode='eval')\n","\n","# # Load the pre-trained weights\n","# model.init_from_file('/root/model/model.pkl.gz', weights_only=True)\n","# model"]},{"cell_type":"markdown","metadata":{"id":"ilM9C8P3RWRf"},"source":["<a name='5'></a>\n","# Part 5: Testing\n","\n","Testing the model.\n","\n","Implementing a greedy decoding code. It will consists of two parts.\n","\n","The first part will identify the next symbol. It does it by calculating the argmax of the output of the model and will returns the index.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rD_bXRCpRWRg"},"outputs":[],"source":["def next_symbol(output_tokens, model):\n","  \"\"\"Returns the next symbol for a given sentence.\n","\n","  Args:\n","      output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n","      model (trax.layers.combinators.Serial): The Transformer model.\n","\n","  Returns:\n","      int: tokenized symbol.\n","  \"\"\"\n","  # current output tokens length\n","  token_length = len(output_tokens)\n","  #print(f'padded length: {token_length}')\n","\n","  # calculating the minimum power of 2 big enough to store token_length\n","  padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n","  #print(f'padded length: {padded_length}')\n","\n","  # Padding output_tokens with 0's\n","  padded = output_tokens + [0] * (padded_length - token_length)\n","  padded_with_batch = np.array(padded)[None, :]\n","\n","  # model consumes a tuple containing two padded tensors with batch\n","  output, _ = model((padded_with_batch, padded_with_batch))\n","  # token_length in the second dim and all of the entries for the last dim.\n","  log_probs = output[0, token_length, :]\n","\n","  return int(np.argmax(log_probs))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GA3iXCNooD1k","outputId":"d58ef0a4-a46f-414c-8df9-1875e6cfb1c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}],"source":["# Testing\n","sentence_test_nxt_symbl = \"Before the times of GPS and cell phones, maps were the main way to find one's way from point A to point B without getting lost. Nowadays, we have maps for everything from land elevation to animal location origins. Maps are fantastic things, just ask the cartographer at the local map store, he'll be sure to feel the same way. So if you're ready to take a look at 40 of the most interesting maps of America, keep reading!\"\n","detokenizer([next_symbol(tokenizer(sentence_test_nxt_symbl)+[0], model)])"]},{"cell_type":"markdown","metadata":{"id":"LAgTVb-WK9vs"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"2AwrQFglRWRj"},"source":["<a name='5.1'></a>\n","### 5.1 Greedy decoding\n","\n","Implementing the greedy_decode algorithm that will call the `next_symbol` function.\n","\n","Input:\n","* input_sentence\n","* the trained model\n","\n","Returns\n","* decoded sentence.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HwIdimiN0k2"},"outputs":[],"source":["# Decoding functions.\n","def greedy_decode(input_text, model, max_n = 100, show_progress = False):\n","  \"\"\"Greedy decode function.\n","  Args:\n","      input_text (string): a sentence or article.\n","      model (trax.layers.combinators.Serial): Transformer model.\n","      max_n (int): Max number of tokens limit\n","      show_progress (Bool): To show output after each word generation\n","\n","  Returns:\n","      string: summary of the input.\n","  \"\"\"\n","\n","  # Using tokenizer()\n","  processed_text = process_text(input_text)\n","  cur_output_tokens = tokenizer(processed_text) + [0]\n","  generated_output = []\n","  cur_output = 0\n","  EOS = 1\n","  i = 0\n","  l_t_len = 0\n","\n","  if show_progress:\n","    print('GENERATING SUMMARY:')\n","\n","  while cur_output != EOS and (i < max_n or max_n == 0):\n","    # Get next symbol\n","    cur_output = next_symbol(cur_output_tokens, model)\n","    # Appending next symbol to original sentence\n","    cur_output_tokens.append(cur_output)\n","    # Appending next symbol to generated sentence\n","    generated_output.append(cur_output)\n","    text_gen = detokenizer(generated_output)\n","    if show_progress:\n","      clear_text = \"\\b\" * l_t_len + \" \" * l_t_len + \"\\b\" * l_t_len\n","      print(clear_text, end=\"\", flush=True)\n","      print(f'{text_gen}....({i})',end=\"\", flush=True)\n","      l_t_len = len(text_gen) + len(f'....({i})')\n","    # Incrementing\n","    i += 1\n","\n","  print('\\n -------------- \\n')\n","  return text_gen"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kHuIDGW1sOr","outputId":"422302e2-2280-44ce-a111-93be1105903f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before the times of GPS and cell phones, maps were the main way to\n","find one's way from point A to point B without getting lost. Nowadays,\n","we have maps for everything from land elevation to animal location\n","origins. Maps are fantastic things, just ask the cartographer at the\n","local map store, he'll be sure to feel the same way. So if you're\n","ready to take a look at 40 of the most interesting maps of America,\n","keep reading! \n","\n","GENERATING SUMMARY:\n","The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-....(49)\n"," -------------- \n","\n","The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-\n"]}],"source":["# Test it out on a sentence!\n","test_sentence = \"Before the times of GPS and cell phones, maps were the main way to find one's way from point A to point B without getting lost. Nowadays, we have maps for everything from land elevation to animal location origins. Maps are fantastic things, just ask the cartographer at the local map store, he'll be sure to feel the same way. So if you're ready to take a look at 40 of the most interesting maps of America, keep reading!\"\n","print(wrapper.fill(test_sentence), '\\n')\n","print(greedy_decode(test_sentence, model, 50, show_progress=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYgX-mzjyUia","outputId":"c5f8e37a-d6f2-440b-a70c-27e1cde25df7"},"outputs":[{"output_type":"stream","name":"stdout","text":["It’s the posing craze sweeping the U.S. after being brought to fame by\n","skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n","Pujols - and even Republican politician Rick Perry. But now four\n","students at Riverhead High School on Long Island, New York, have been\n","suspended for dropping to a knee and taking up a prayer pose to mimic\n","Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n","Tyler Carroll and Connor Carroll were all suspended for one day\n","because the ‘Tebowing’ craze was blocking the hallway and presenting a\n","safety hazard to students. Scroll down for video. Banned: Jordan\n","Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n","left) were all suspended for one day by Riverhead High School on Long\n","Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n","Issue: Four of the pupils were suspended for one day because they\n","allegedly did not heed to warnings that the 'Tebowing' craze at the\n","school was blocking the hallway and presenting a safety hazard to\n","students. \n","\n","GENERATING SUMMARY:\n","The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-....(49)\n"," -------------- \n","\n","The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-\n"]}],"source":["# Test it out with a whole article!\n","article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n","print(wrapper.fill(article), '\\n')\n","print(greedy_decode(test_sentence, model, 50, show_progress=True))"]},{"cell_type":"code","source":["article='The success of modern farming and plant breeding relies on accurate and efficient collection of data. For a commercial organization that manages large amounts of crops, collecting accurate and consistent data is a bottleneck. Due to limited time and labor, accurately phenotyping crops to record color, head count, height, weight, etc. is severely limited. However, this information, combined with other genetic and environmental factors, is vital for developing new superior crop species that help feed the world’s growing population. Recent advances in machine learning, in particular deep learning, have shown promise in mitigating this bottleneck. In this paper, we propose a novel deep learning method for counting on-ear corn kernels in-field to aid in the gathering of real-time data and, ultimately, to improve decision making to maximize yield. We name this approach DeepCorn, and show that this framework is robust under various conditions. DeepCorn estimates the density of corn kernels in an image of corn ears and predicts the number of kernels based on the estimated density map.'\n","\n","print(wrapper.fill(article), '\\n')\n","print(greedy_decode(test_sentence, model, 50, show_progress=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haz4WvClYlMo","outputId":"07dabce0-316a-4843-e345-02b8c84c49fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The success of modern farming and plant breeding relies on accurate\n","and efficient collection of data. For a commercial organization that\n","manages large amounts of crops, collecting accurate and consistent\n","data is a bottleneck. Due to limited time and labor, accurately\n","phenotyping crops to record color, head count, height, weight, etc. is\n","severely limited. However, this information, combined with other\n","genetic and environmental factors, is vital for developing new\n","superior crop species that help feed the world’s growing population.\n","Recent advances in machine learning, in particular deep learning, have\n","shown promise in mitigating this bottleneck. In this paper, we propose\n","a novel deep learning method for counting on-ear corn kernels in-field\n","to aid in the gathering of real-time data and, ultimately, to improve\n","decision making to maximize yield. We name this approach DeepCorn, and\n","show that this framework is robust under various conditions. DeepCorn\n","estimates the density of corn kernels in an image of corn ears and\n","predicts the number of kernels based on the estimated density map. \n","\n","GENERATING SUMMARY:\n","The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-....(49)\n"," -------------- \n","\n","The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-\n"]}]},{"cell_type":"code","source":["article='A convicted sex offender this week fatally shot his wife and her three children in their Oklahoma home – as well as two teenage girls who there were for a sleepover – before killing himself, police said Wednesday, accounting for the bodies’ discovery days earlier. Authorities found the bodies Monday at a property in Henryetta, a city about 90 miles from Oklahoma City, where 39-year-old registered sex offender Jesse L. McFadden lived with his wife, 35-year-old Holly McFadden, and her children, who were McFadden’s stepchildren: Rylee Allen, 17; Michael Mayo, 15; and Tiffany Guess, 13. The two other teen girls who were killed – 14-year-old Ivy Webster and 16-year-old Brittany Brewer – had been reported missing and in danger on Monday morning. The girls were Tiffany’s friends and spent the night with her on Saturday, and they were reported missing when they didn’t return home Sunday as planned, Okmulgee Police Chief Joe Prentice said. 16-year-old Brittany Brewer, left, and 14-year-old Ivy Webster, right, were the subject of an endangered/missing advisory issued by the Oklahoma Highway Patrol, which said they were last seen early Monday morning in Henryetta. A convicted sex offender and 2 teenage girls believed among 7 bodies found at his Oklahoma home “I follow the evidence … and the evidence is that Jesse McFadden murdered six people and then killed himself,” Prentice said during a news conference Wednesday. “I don’t have any evidence to indicate what the actual motive was.” The bodies of Holly and two of her children – Michael and Tiffany – were found together outside on the property, which the McFaddens were renting, Prentice said. The bodies of Ivy, Brittany and Rylee were found separately, about 150 yards apart from each other, he added. The scene “appeared to be staged,” the chief said, adding he believes the victims’ bodies were moved after they were killed. “There are questions that will never get answered because the only people who know are no longer here,” Prentice said. “We will continue to document everything that we have found and anything that we discover in follow-up interviews moving forward, and generate a report. We will submit that report to the district attorney’s office for her review as a formality, because there is no prosecution to be had here.” The firearm used in the killings was a handgun that Holly bought in 2022, Prentice said. Holly’s mother, Janette Mayo, told CNN affiliate KJRH in an on-air interview that her daughter was married to McFadden. She identified Holly and Holly’s three children as four of the victims.'\n","\n","print(wrapper.fill(article), '\\n')\n","print(greedy_decode(test_sentence, model, 50, show_progress=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpSyswXzZPI-","outputId":"a827be3d-9b61-4851-b9ae-b42e8420f118"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A convicted sex offender this week fatally shot his wife and her three\n","children in their Oklahoma home – as well as two teenage girls who\n","there were for a sleepover – before killing himself, police said\n","Wednesday, accounting for the bodies’ discovery days earlier.\n","Authorities found the bodies Monday at a property in Henryetta, a city\n","about 90 miles from Oklahoma City, where 39-year-old registered sex\n","offender Jesse L. McFadden lived with his wife, 35-year-old Holly\n","McFadden, and her children, who were McFadden’s stepchildren: Rylee\n","Allen, 17; Michael Mayo, 15; and Tiffany Guess, 13. The two other teen\n","girls who were killed – 14-year-old Ivy Webster and 16-year-old\n","Brittany Brewer – had been reported missing and in danger on Monday\n","morning. The girls were Tiffany’s friends and spent the night with her\n","on Saturday, and they were reported missing when they didn’t return\n","home Sunday as planned, Okmulgee Police Chief Joe Prentice said.\n","16-year-old Brittany Brewer, left, and 14-year-old Ivy Webster, right,\n","were the subject of an endangered/missing advisory issued by the\n","Oklahoma Highway Patrol, which said they were last seen early Monday\n","morning in Henryetta. A convicted sex offender and 2 teenage girls\n","believed among 7 bodies found at his Oklahoma home “I follow the\n","evidence … and the evidence is that Jesse McFadden murdered six people\n","and then killed himself,” Prentice said during a news conference\n","Wednesday. “I don’t have any evidence to indicate what the actual\n","motive was.” The bodies of Holly and two of her children – Michael and\n","Tiffany – were found together outside on the property, which the\n","McFaddens were renting, Prentice said. The bodies of Ivy, Brittany and\n","Rylee were found separately, about 150 yards apart from each other, he\n","added. The scene “appeared to be staged,” the chief said, adding he\n","believes the victims’ bodies were moved after they were killed. “There\n","are questions that will never get answered because the only people who\n","know are no longer here,” Prentice said. “We will continue to document\n","everything that we have found and anything that we discover in follow-\n","up interviews moving forward, and generate a report. We will submit\n","that report to the district attorney’s office for her review as a\n","formality, because there is no prosecution to be had here.” The\n","firearm used in the killings was a handgun that Holly bought in 2022,\n","Prentice said. Holly’s mother, Janette Mayo, told CNN affiliate KJRH\n","in an on-air interview that her daughter was married to McFadden. She\n","identified Holly and Holly’s three children as four of the victims. \n","\n","GENERATING SUMMARY:\n","The....(0)\b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\bThe U....(1)\b\b\b\b\b\b\b\b\b\b\b\b            \b\b\b\b\b\b\b\b\b\b\b\bThe U.....(2)\b\b\b\b\b\b\b\b\b\b\b\b\b             \b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S....(3)\b\b\b\b\b\b\b\b\b\b\b\b\b\b              \b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S.....(4)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The....(5)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U....(6)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.....(7)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S....(8)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S.....(9)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The....(10)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U....(11)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.....(12)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S....(13)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S.....(14)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says....(15)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the....(16)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same....(17)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-....(18)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year....(19)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-....(20)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year....(21)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-....(22)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year....(23)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-....(24)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year....(25)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-....(26)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year....(27)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-....(28)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-year....(29)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-....(30)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old....(31)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was....(32)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found....(33)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to....(34)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be....(35)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a....(36)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-....(37)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                      \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year....(38)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-....(39)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year....(40)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-....(41)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year....(42)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-....(43)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year....(44)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                         \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-....(45)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year....(46)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-....(47)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year....(48)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                                                                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bThe U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-....(49)\n"," -------------- \n","\n","The U.S. The U.S. The U.S. says the same-year-year-year-year-year-\n","year-old was found to be a-year-year-year-year-year-year-\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"47XrJFrl2ic7"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"Jw5R7V3X2jpV"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"coursera":{"schema_names":["NLPC4-2"]},"gpuClass":"standard","jupytext":{"encoding":"# -*- coding: utf-8 -*-","formats":"ipynb,py:percent"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}